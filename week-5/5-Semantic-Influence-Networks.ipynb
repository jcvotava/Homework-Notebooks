{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Semantic & Influence Networks\n",
    "\n",
    "This week, we explore the representation and analysis of semantic and influence networks. We begin by introducing you to some straightforward approaches to network analysis. We then begin to illustrate them with word networks that we can analyze to understand the structure of how words connect with one another, and the dynamics of how their meanings flow through a discursive system. For example, we can define links between words as a function of their co-presence within a document, chapter, paragraph, sentence, noun phrase or continuous bag of words. We can also define links as a function of words that rely on one another within a directed dependency parse, or links between extracted Subjects, Verbs and Objects, or nouns and the adjectives that modify them (or verbs and the adverbs that modify them). Rendering words linked as a network or discrete topology allows us to take advantage of the network analysis metrics like centrality and community.\n",
    "\n",
    "Texts also represent moves in a social game, and we can analyze the interchange of words and phrases within them in order to understand more about the relationships between people, groups and organizations that interact with one another. For example, who influences whom? And in a singular conversation--like a discussion of tweets on comments in Facebook--or over vast stretches of time (e.g., How much was Kant influenced by Aristotle? by Plato? Was _Chinatown_ director Robert Towne influenced by Sophocles?) We will do this by extracting conversational moves from text. But note that the similarity measures from last week's homework can also be used to represent a matrix of similarities--and potential influences--from one document to another. Rendering social actors or documentary events (e.g., a book) linked as a network or discrete topology allows us to take advantage of the wide range of metrics and models developed for network analysis. These include measurement of network centrality, density and modularity, \"block modeling\" structurally equivalent relationships, and sophisticated graphical renderings of networks or network partitions that allow us to visually interrogate their structure and complexity of social relations. Moreover, we can link explicit social interactions (e.g., \"friendship\" on Facebook, coupling in a romantic relationship, etc.) with semantic networks to better make sense of how pepole (inter)act in the world. \n",
    "\n",
    "For this notebook we will use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "\n",
    "#This will be doing most of the work\n",
    "import networkx as nx\n",
    "\n",
    "import sklearn #For generating some matrices\n",
    "import pandas #For DataFrames\n",
    "import numpy as np #For arrays\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "import seaborn #Makes the plots look nice\n",
    "import scipy #Some stats\n",
    "import nltk #a little language code\n",
    "from IPython.display import Image #for pics\n",
    "\n",
    "import pickle #if you want to save layouts\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to networks in *networkx*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will primarily be dealing with graphs in this notebook, so lets first go over how to use them.\n",
    "\n",
    "To start with, let's create an undirected graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x21d546dd648>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = nx.Graph()\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add nodes. These are all named, like entries in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_node(1)\n",
    "g.add_node(2)\n",
    "g.add_node(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 3 vertices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we want to get more information about the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 3\n",
      "Number of edges: 0\n",
      "Average degree:   0.0000\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give nodes properties, like name or type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.nodes[1]['type'] = 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'NN'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.nodes[2]['name'] = 'dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'dog'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still pretty boring...\n",
    "\n",
    "Lets add a couple of edges. Notice that we use the ids, but not any of the properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 4\n",
      "Number of edges: 4\n",
      "Average degree:   2.0000\n"
     ]
    }
   ],
   "source": [
    "g.add_edges_from([(1, 2), (2, 3), (3, 1), (1,4)])\n",
    "print(nx.info(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the summary has changed. Moreover, there's one additional node, because we asked for an edge to 4.\n",
    "\n",
    "We can also give the edges properties like weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edges[1, 2]['weight'] = 2\n",
    "g.edges[1, 4]['weight'] = 2\n",
    "g.edges[1, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our graph now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw_networkx(g)\n",
    "plt.savefig(\"test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very exciting :-).\n",
    "\n",
    "There are many things to do with the graph once we have created it, some of which we will explore here with a word semantic network.\n",
    "\n",
    "First let's load some data: the Grimmer Senate press release corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/senReleasesTraining.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c07b4f028b65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msenReleasesDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/senReleasesTraining.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msenReleasesDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1043\u001b[0m             )\n\u001b[0;32m   1044\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1045\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1863\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1864\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1361\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1364\u001b[0m         )\n\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    642\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m             )\n\u001b[0;32m    646\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/senReleasesTraining.csv'"
     ]
    }
   ],
   "source": [
    "senReleasesDF = pandas.read_csv('../data/senReleasesTraining.csv', index_col = 0)\n",
    "senReleasesDF[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be extracting sentences, as well as tokenizing and stemming. (You should be able to do this in your sleep now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senReleasesDF['tokenized_sents'] = senReleasesDF['text'].apply(lambda x: [lucem_illud_2020.word_tokenize(s) for s in lucem_illud_2020.sent_tokenize(x)])\n",
    "senReleasesDF['normalized_sents'] = senReleasesDF['tokenized_sents'].apply(lambda x: [lucem_illud_2020.normalizeTokens(s) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by looking at words that co-occur in the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordCooccurrence(sentences, makeMatrix = False):\n",
    "    words = set()\n",
    "    for sent in sentences:\n",
    "        words |= set(sent)\n",
    "    wordLst = list(words)\n",
    "    wordIndices = {w: i for i, w in enumerate(wordLst)}\n",
    "    wordCoCounts = {}\n",
    "    #consider a sparse matrix if memory becomes an issue\n",
    "    coOcMat = np.zeros((len(wordIndices), len(wordIndices)))\n",
    "    for sent in sentences:\n",
    "        for i, word1 in enumerate(sent):\n",
    "            word1Index = wordIndices[word1]\n",
    "            for word2 in sent[i + 1:]:\n",
    "                coOcMat[word1Index][wordIndices[word2]] += 1\n",
    "    if makeMatrix:\n",
    "        return coOcMat, wordLst\n",
    "    else:\n",
    "        coOcMat = coOcMat.T + coOcMat\n",
    "        g = nx.convert_matrix.from_numpy_matrix(coOcMat)\n",
    "        g = nx.relabel_nodes(g, {i : w for i, w in enumerate(wordLst)})\n",
    "        return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to explain the function defined above line by line. But before we look at the function, let's first look at the structure of the data that we use here, senReleasesDF['normalized_sents']. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesDF['normalized_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, senReleasesDF['normalized_sents'] is a pandas series, i.e., one-dimensional labeled array that contains any type of data. Each array is comprised of a list of lists. In particular, each array is a list of sentences, each of which is also a list of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesDF['normalized_sents'][0] #each array is a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesDF['normalized_sents'][0][4] #each sentence is a list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do we want to do with this data? First, we want a list of words that occurred at least once in this corpus. Then, second, we want a word-word co-occurrence matrix. This is a square matrix (i.e. matrix with the name number of rows and columns), the rows and columns of which are words, and the entries of which refer to how many times these two words appeared together in sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the first task: getting a complete list of words in the corpus. \n",
    "\n",
    "(1) a complete list of unique words that appeared in this corpus\n",
    "\n",
    "Since the data is a nested list (words are elements of sentences (which are lists) that are elements of lists (documents), we need to get each sentence out of each document and then get each word out of each sentence. So we do \"for sent in sentences\" to get each sentence out of each document, one by one, to extract word(s) that appeared in each sentence. But, in some sentences, same word appears more than once. So we use set() function, which converts a list (which, here, is each sentence) into a set. What is a set? It's the same old set that we know in mathematics: an unordered collection of items. Each element in a set is unique. Thus, using set(), we can get unique elements (that are, here, words) of each sentence. \n",
    "\n",
    "What is the \"|=\" here? This works as union operation when used with sets (just like +=). Then, we use list() to convert this set of words into a list. \n",
    "\n",
    "Then we assign index to each word. You'll see why we do it in the next step--to make the word-word co-occurrence matrix. So we do \"wordIndices = {w: i for i, w in enumerate(wordLst)}\", which will give us a dictionary whose keys are words and values are indices that we assigned to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the complete list of words that appeared at least once in the corpus (with index assigned to each word). Using this, we want to make word-word co-occurrence matrix.\n",
    "\n",
    "We first make an empty matrix, the size of which is n x n, where n refers to the number of unique words in the corpus. coOcMat is the empty matrix that we made by \"coOcMat = np.zeros((len(wordIndices), len(wordIndices)))\"\n",
    "\n",
    "Then we'll count how many times each pair of words co-occurred in the corpus. So, we do another loop. Each sentence (\"sent\") has word(s). We want to select each word in a sentence, and find which words co-occurred with this word in each sentence, and then we move on to the next word in a sentence, do the same thing, until we get to the last word in this list (which is, here, a sentence); and, we iterate the whole process for every sentence. \n",
    "\n",
    "How did we do it? Let's look at the second loop. By doing \"for i, word1 in enumerate(sent):\", we select each word in each sentence (\"sent\"). Then we get the index of this word that we assigned above in the word list (here, be careful, we have two indices here, one refers to the index that we assigned in \"wordIndices\", and the other to the temporary index in each sentence that we got from \"enumerate(sent)\"--here, the \"index\" refers to the former, the one we can find in the wordIndices), by doing: \"word1Index = wordIndices[word1]\". Then, we count how many times the words next to this word in this list (\"sent[i +1:]\") co-occurred with this word, and update it to the coOcMat matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, why do we do \"coOcMat = coOcMat.T + coOcMat\"?\n",
    "\n",
    "If you go back and see what the loop does, and think about the characteristics of word-word co-occurrence matrix, it would make sense. Think about the structural characteristics of word-word co-occurrence matrix: it is a square matrix; it's a symmetric matrix (i.e. a square matrix that is equal to its transpose); and, it's main diagonal is zero. That said, in this matrix, let's say A, Aij and Aji are identical. This makes sense since the co-occurrence of i and j and the same as the co-occurrence of j and i. \n",
    "\n",
    "Let's go back to the loop we did above, especially how we counted the co-occurrence of each pair of words. We got the index of a word, let's say i, with another words, let's say j, and then we added 1 to Aij, rather than Aji. So, to make this matrix, coOcMat, a word-word co-occurrence matrix, we should add coOcMat and the transpose of this matrix (coOcMat.T)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, build a graph based on word cooccurences in the first 100 press releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = wordCooccurrence(senReleasesDF['normalized_sents'][:100].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we do .sum() here? This is because senReleasesDF['normalized_sents'] is, as I mentioned above, a pandas series, and the wordCooccurrence function that we defined above takesa list rather than pandas series. So, by doing senReleasesDF['normalized_sents'].sum(), we combine all the arrays into a list (of course, of lists)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of vertices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A part of the adjacency matrix of cleaned word by press releases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.to_numpy_matrix(g)[:10, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the graph and read it later, although this is slow if there are many edges or nodes, so we will filter first, as we will demonstrate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nx.write_graphml(g, '../data/Obama_words.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can build graphs starting with a two-mode network. Let's use the document-word frequency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenlist = lucem_illud_2020.word_tokenize(text)\n",
    "    normalized = lucem_illud_2020.normalizeTokens(tokenlist)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction\n",
    "# in case we get a module not found error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senVectorizer = sklearn.feature_extraction.text.CountVectorizer(tokenizer = tokenize)\n",
    "senVects_incidence = senVectorizer.fit_transform(senReleasesDF['text'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senVects_incidence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to turn the incidence matrix into a network. Note the use of bipartite as an attribute--this is how you need to tell networkx the graph is bipartite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_2mode = nx.Graph()\n",
    "\n",
    "#define all the nodes\n",
    "g_2mode.add_nodes_from((senVectorizer.get_feature_names()[i] for i in range(senVects_incidence.shape[1])), bipartite = 'word')\n",
    "g_2mode.add_nodes_from(range(senVects_incidence.shape[0]), bipartite = 'doc')\n",
    "\n",
    "#add all the edges\n",
    "g_2mode.add_edges_from(((d, senVectorizer.get_feature_names()[w], {'weight' : senVects_incidence[d, w]}) for d, w in zip(*senVects_incidence.nonzero())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(g_2mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very popular layout algorithm for visualizing graphs is the Fruchterman-Reingold Algorithm (or spring layout), which uses a physical metaphor for lay-out. Nodes repel one another, and edges draw connected elements together like springs. The algorithm attempts to minimize the energy in such a system. For a large graph, however, the algorithm is computational demanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the bipartite network with a quick spring layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(g_2mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no filtering, this will not bring insight (see below). If we want even faster computation and tunable visualizations, check out [Pajek](http://mrvar.fdv.uni-lj.si/pajek/) or [gephi](https://gephi.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A two-mode network can be easily transformed into two one-mode network, enabling words to be connected to other words via the number of documents that share them, or documents to be connected to other documents via the words they share:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contractNetwork(g, targetType):\n",
    "    g_mono = nx.Graph()\n",
    "    g_mono.add_nodes_from(((n, d) for n, d in g_2mode.nodes(data = True) if d['bipartite'] == targetType))\n",
    "    \n",
    "    for n_outside in (n for n, d in g_2mode.nodes(data = True) if d['bipartite'] != targetType):\n",
    "        neighbors = list((n for n in g.neighbors(n_outside) if g.nodes[n]['bipartite'] == targetType))\n",
    "        for i, n1 in enumerate(neighbors):\n",
    "            for n2 in neighbors[i+1:]:\n",
    "                try:\n",
    "                    g_mono.edges[n1, n2]['weight'] += 1\n",
    "                except KeyError:\n",
    "                    g_mono.add_edge(n1, n2, weight = 1)\n",
    "    return g_mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gDoc = contractNetwork(g_2mode, 'doc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at the document-to-document network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(gDoc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a visualization. It is not surprising that almost every document is connected to every other. We can use edge weight to distinguish document distance (modeled as attraction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = nx.spring_layout(gDoc, k = 1/3, weight='weight', iterations= 50)\n",
    "nx.draw(gDoc, pos = layout, labels = {n:n for n in gDoc.nodes()}) #Getting labels is a bit annoying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets draw the graph with high and low weight edges distinguished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wMedian = np.median([d['weight'] for n1, n2, d in gDoc.edges(data = True)])\n",
    "edgesHigh = [(n1, n2) for n1, n2, d in gDoc.edges(data = True) if d['weight'] > wMedian]\n",
    "edgesLow = [(n1, n2) for n1, n2, d in gDoc.edges(data = True) if d['weight'] <= wMedian]\n",
    "nx.draw(gDoc, pos = layout, labels = {n:n for n in gDoc.nodes()}, edgelist = edgesLow, style='dotted', width=.5)\n",
    "nx.draw(gDoc, pos = layout, nodelist=None, edgelist = edgesHigh, width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see why 53 and 63 are on the outside, while 39 and 23 are at the center. We can look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(senReleasesDF.iloc[63]['text']),\"words:\",\"\\n\",senReleasesDF.iloc[63]['text'].strip().replace('  ', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, what's \"iloc\" here? iloc is used to select rows and columns in Pandas DataFrame. So,  senReleasesDF.iloc[63] gives the 63th row of senReleasesDF; and since we also specified the column (['text']), it gives us the text of the 63th row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(senReleasesDF.iloc[23]['text']),\"words:\",\"\\n\",senReleasesDF.iloc[23]['text'].strip().replace('  ', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah...those documents with the **most** words are unsprisingly the most central in this simple document network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn it around and look at the word-to-word network by documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gWord = contractNetwork(g_2mode, 'word')\n",
    "print(nx.info(gWord))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reduce the number of words to a manageable size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wMean = np.mean([d['weight'] for n1, n2, d in gWord.edges(data = True)])\n",
    "wMean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to return to the sentence cooccurence graph, as it suggests many more meaningful (more local) associations. But without filtering, it is too large. Let's first drop all the edges with weight below 25, then drop all the isolates. You are recommended to play with the weight here to see how the graph might look different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = wordCooccurrence(senReleasesDF['normalized_sents'][:100].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.remove_edges_from([(n1, n2) for n1, n2, d in g.edges(data = True) if d['weight'] <= 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.remove_nodes_from(list(nx.isolates(g))) #since we are changing the graph list() evaluates the isolates first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connected_component_subgraphs(G):\n",
    "    for c in nx.connected_components(G):\n",
    "        yield G.subgraph(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant = max(connected_component_subgraphs(g), key=len) # keep just the giant connected component\n",
    "print(nx.info(giant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit smaller now, Now, let's visualize it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = nx.spring_layout(giant, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=.2, \n",
    "        alpha = .9, \n",
    "        node_size = 100,\n",
    "        node_color = \"xkcd:light red\",\n",
    "        edge_color='xkcd:sky blue') #Getting labels is a bit annoying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "c = list(greedy_modularity_communities(giant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use this oppurtunity to introduce a concept of finding communities in graphs: a popular one is the modularity measure, introduced in: Clauset, A., Newman, M. E., & Moore, C. Finding community structure in very large networks. Physical Review E 70(6), 2004.\n",
    "\n",
    "networkx implements this through the above method. It yields sets of nodes, one for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove those words that show up in many documents...those with many connections (note that this has a similar effect to selecting those with a high tf.idf score). Note that for this case we get the same graph; this is just to demonstrate the different kinds of node dropping and graph changes we can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "giant = nx.Graph(giant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant.remove_nodes_from([n for n in giant.nodes if len(set(giant.neighbors(n))) >= 2000]) \n",
    "giant.remove_nodes_from(list(nx.isolates(giant)))\n",
    "print(nx.info(giant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = nx.spring_layout(giant, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=.2, \n",
    "        alpha = .9, \n",
    "        node_size = 100,\n",
    "        node_color = \"xkcd:light red\",\n",
    "        edge_color='xkcd:sky blue') #Getting labels is a bit annoying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue to trim globally to investigate the structure of words at alternative slices of network density. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can find cliques, or completely connected sets of nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(', '.join(max(nx.clique.find_cliques(giant), key = lambda x: len(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at a subgraph of the network, those nodes that are within 1 or 2 network steps of 'america'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "americanNeighbors = giant.neighbors('american')\n",
    "g_american = giant.subgraph(americanNeighbors)\n",
    "print(nx.info(g_american))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_amer = nx.spring_layout(g_american, weight='weight', iterations= 100, k = .3)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "maxWeight = max((d['weight'] for n1, n2, d in g_american.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in g_american.edges(data = True)))\n",
    "nx.draw(g_american, ax = ax, pos = layout_amer, labels = {n:n for n in g_american.nodes()},\n",
    "        width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in g_american.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:sky blue',\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "americanNeighbors = gWord.neighbors('american')\n",
    "americanNeighborsPlus1 = set(americanNeighbors)\n",
    "for n in americanNeighbors:\n",
    "    americanNeighborsPlus1 |= set(giant.neighbors(n))\n",
    "    \n",
    "#for x in americanNeighborsPlus1:\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_american2 = giant.subgraph(americanNeighborsPlus1)\n",
    "print(nx.info(g_american2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_amer = nx.spring_layout(g_american2, weight='weight', iterations= 100, k = .3)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "centralities_amer = nx.eigenvector_centrality(g_american2)\n",
    "maxC = max(centralities_amer.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in g_american2.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in g_american2.edges(data = True)))\n",
    "nx.draw(g_american2, ax = ax, pos = layout_amer, labels = {n:n for n in g_american2.nodes()},\n",
    "        width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in g_american2.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:sky blue',\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that render networks to meaningfully characterize the structure of words and documents (or subdocuments like chapters or paragraphs) from your corpus. What are useful filters and thresholds and what semantic structures do they reveal that give insight into the social world and social game inscribed in your corpus? Interrogate interesting subgraphs of your network and detail what they reveal about the semantic space involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Network Statistics\n",
    "We can often gain more insight into our semantic network through statistics that describe the positions of words within it.\n",
    "\n",
    "We begin with measures of centrality. The concept of centrality is that some nodes (words or documents) are more *central* to the network than others. The most straightforward is the notion of degree centrality: those nodes that have the highest number of connections are the most central. Here our measure normalizes the number of connections by those with the most connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcentralities = nx.degree_centrality(giant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcentralities['american']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dcentralities.items(), reverse = True, key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(dcentralities.items(), key = lambda x : x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, why do we have .items() here? This is because decentralities is a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can color and size the nodes by betweenness centrality, it's much faster to redraw since we aren't redoing the layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "maxC = max(dcentralities.items(), key = lambda x : x[1])[1]\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=.2, \n",
    "        alpha = .9, \n",
    "        node_size = 100,\n",
    "        node_color = [dcentralities[n] / maxC for n in giant.nodes],\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualize the graph involes the use of size to represent degree centrality and edge weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "maxC = max(dcentralities.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in gWord.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        node_color = [dcentralities[n] / maxC for n in giant.nodes],\n",
    "        node_size = [dcentralities[n] / maxC * 200 for n in giant.nodes],\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distrubution of degree centrality is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(dcentralities.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top and bottom ten words in terms of degree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dcentralities.items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dcentralities.items(), key = lambda x: x[1], reverse = True)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider another very different measure, which is *betweenness* centrality. Betweenness centrality distinguishes nodes that require the most shortest pathways between all other nodes in the network. Semantically, words with a high betweenness centrality may link distinctive domains, rather than being \"central\" to any one. In other words, high *betweenness centrality* nodes may not have the highest *degree centrality*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centralities = nx.betweenness.betweenness_centrality(giant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities['american']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(centralities.items(), key = lambda x : x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can color and size the nodes by betweenness centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "maxC = max(centralities.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in giant.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        node_color = [centralities[n] / maxC for n in giant.nodes],\n",
    "        node_size = [centralities[n] / maxC * 200 for n in giant.nodes],\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distrubution of betweenness centrality is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(centralities.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exponential distrubution, but you might need to add more nodes to see it clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top ten words in terms of betweenness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dcentralities.items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here it appears that \"health\"/\"care\", \"family\"/\"children\" are key concepts that connect others in the broader network. This is interesting in that they seem to be domain-specific rather than linking words like \"state\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are words lower down?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dcentralities.items(), key = lambda x: x[1], reverse = True)[50:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at closeness centrality, or the average Euclidean or path distance between a node and all others in the network. A node with the highest closeness centrality is most likely to send a signal with the most coverage to the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centralities = nx.closeness_centrality(giant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "maxC = max(centralities.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in giant.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        node_color = [centralities[n] / maxC for n in giant.nodes],\n",
    "        node_size = [centralities[n] / maxC * 200 for n in giant.nodes],\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top and bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(centralities.items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(centralities.items(), key = lambda x: x[1], reverse = True)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or eignvector centrality, an approach that weights degree by the centrality of those to whom one is tied (and the degree to whom they are tied, etc.) In short, its an $n$th order degree measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centralities = nx.eigenvector_centrality(giant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "maxC = max(centralities.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in giant.edges(data = True)))\n",
    "nx.draw(giant, ax = ax, pos = layout, labels = {n:n for n in giant.nodes()},\n",
    "        width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in giant.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        node_color = [centralities[n] / maxC for n in giant.nodes],\n",
    "        node_size = [centralities[n] / maxC * 200 for n in giant.nodes],\n",
    "        cmap = plt.get_cmap('plasma')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the colors reveal a much more graduate distribution here. Let's look at it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(centralities.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top and bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(centralities.items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(centralities.items(), key = lambda x: x[1], reverse = True)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now filter our network by a centrality measure. Let's define a function and experiment with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterWords(G, minWeight = 3, filter_ = \"betweenness\", rule = \"number\", value_of_rule = 200):\n",
    "    \"\"\"Function to filter network by degree centrality measures\"\"\"\n",
    "    G = G.copy()\n",
    "    try:\n",
    "        G.remove_edges_from([(n1,n2) for n1, n2, d in G.edges(data = True) if d['weight'] < minWeight])\n",
    "    except:\n",
    "        print(\"weight might be missing from one or more edges\")\n",
    "        raise\n",
    "    if filter_ ==\"betweenness\":\n",
    "        index = nx.betweenness_centrality(G) #betweeness centrality score\n",
    "    elif filter_ == \"closeness\":\n",
    "        index = nx.closeness_centrality(G) #closeness centrality score\n",
    "    elif filter_ == \"eigenvector\":\n",
    "        index = nx.eigenvector_centrality(G) #eigenvector centrality score\n",
    "    elif filter_ == \"degree\":\n",
    "        index = nx.degree_centrality(G) #degree centrality score\n",
    "    else:\n",
    "        raise ValueError(\"wrong filter paremeter, should be: betweenness/closeness/eigenvector\")    \n",
    "        \n",
    "    if rule=='number':# if filter by limiting the total number of nodes \n",
    "        \n",
    "        sorted_index = sorted(index.items(), key=lambda x:x[1], reverse=True)\n",
    "        value_of_rule = np.min([value_of_rule, len(G.nodes)])\n",
    "        \n",
    "        nodes_remain = {}\n",
    "        for word, centr in sorted_index[:value_of_rule]:\n",
    "            nodes_remain[word] = centr\n",
    "        G.remove_nodes_from([n for n in index if n not in nodes_remain])\n",
    "        print (\"Total number of nodes(after filtering) in the graph is %s\" % len(G))\n",
    "        return G\n",
    "    \n",
    "    if rule=='above':# if filter by limiting the min value of centrality\n",
    "        value_of_rule = np.max([float(value_of_rule),0])\n",
    "        G.remove_nodes_from([n for n in index if index[n] >=value_of_rule])\n",
    "        print (\"Total number of nodes(after filtering) in the graph is %s\" % len(G))\n",
    "        return G\n",
    "    \n",
    "    if rule=='below':# if filter by limiting the max value of centrality\n",
    "        value_of_rule = np.max([float(value_of_rule),0])\n",
    "        G.remove_nodes_from([n for n in index if index[n] <=value_of_rule])\n",
    "        print (\"Total number of nodes(after filtering) in the graph is %s\" % len(G))\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant_filtered30 = filterWords(giant, minWeight=3, filter_='betweenness', rule='number', value_of_rule=25)\n",
    "print(nx.info(giant_filtered30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_giant_filtered30 = nx.spring_layout(giant_filtered30, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "centralities_giant30 = nx.betweenness_centrality(giant_filtered30)\n",
    "maxC = max(centralities_giant30.items(), key = lambda x : x[1])[1]\n",
    "nx.draw(giant_filtered30, ax = ax, pos = layout_giant_filtered30, labels = {n: n for n in giant_filtered30.nodes()},\n",
    "        alpha = .9, \n",
    "        width = .5,\n",
    "        node_color = [centralities_giant30[n] / maxC for n in giant_filtered30.nodes],\n",
    "        node_size = [centralities_giant30[n] / maxC * 100 for n in giant_filtered30.nodes],\n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:medium blue',\n",
    "        cmap = plt.get_cmap('plasma'),\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at global statistics, like the density of a network, defined as the number of actual edges divided by the total number of possible edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(giant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the average degree per node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([v for w,v in nx.degree(giant)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diameter calculates the average distance between any two nodes in the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.diameter(giant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that calculate different kinds of centrality for distinct words or documents in a network composed from your corpus of interest. Which type of words tend to be most and least central? Can you identify how different centrality measures distinguish different kind of words in your corpus? What do these patterns suggest about the semantic content and structure of your documents? Finally, calculate global measure for your network(s) and discuss what they reveal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS based networks\n",
    "\n",
    "Now let's look at links between specific parts of speech within a network.\n",
    "\n",
    "\n",
    "\n",
    "Let's look at nouns co-occurring in sentences using the top 10 (by score) reddit posts on thread topics we have explored in prior sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditDF = pandas.read_csv('../data/reddit.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditTopScores = redditDF.sort_values('score')[-100:]\n",
    "redditTopScores['sentences'] = redditTopScores['text'].apply(lambda x: [lucem_illud_2020.word_tokenize(s) for s in lucem_illud_2020.sent_tokenize(x)])\n",
    "redditTopScores.index = range(len(redditTopScores) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "redditTopScores[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few things here, especially some things about Pandas that might come in handy later on.\n",
    "\n",
    "First, sort_values('score'): sort_values is a function used for Pandas DataFrame to literally sort the values. Here, it sorts the values from the lowest one to the highest one. So, by doing \"redditDF.sort_values('score')[-100:]\", it gets us 100 rows, from the 100th highest one (-99) to the highest one (0). Why not -100 and 1 rather than -99 and 0? That's just how Python works.\n",
    "\n",
    "Second, .index: it literally gives you the index of the Pandas DataFrame. You can also reindex the indices here. But what does \"range(len(redditTopScores) - 1, -1,-1)\" do? It generates a range from len(redditTopScores)-1 to -1 by increments of -1. So, a range from 99 to -1, by -1, i.e., 99, 98, 97...0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll normalize the tokens through stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['normalized_sents'] = redditTopScores['sentences'].apply(lambda x: [lucem_illud_2020.normalizeTokens(s) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will revisit spacy and use it's english language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posCooccurrence(sentences, *posType, makeMatrix = False):\n",
    "    words = set()\n",
    "    reducedSents = []\n",
    "    #Only using the first kind of POS for each word\n",
    "    wordsMap = {}\n",
    "    for sent in sentences:\n",
    "        s = [(w, t) for w, t in lucem_illud_2020.spacy_pos(sent) if t in posType]\n",
    "        for w, t in s:\n",
    "            if w not in wordsMap:\n",
    "                wordsMap[w] = t\n",
    "        reducedSent = [w for w, t in s]\n",
    "        words |= set(reducedSent)\n",
    "        reducedSents.append(reducedSent)\n",
    "    wordLst = list(words)\n",
    "    wordIndices = {w: i for i, w in enumerate(wordLst)}\n",
    "    wordCoCounts = {}\n",
    "    #consider a sparse matrix if memory becomes an issue\n",
    "    coOcMat = np.zeros((len(wordIndices), len(wordIndices)))\n",
    "    for sent in reducedSents:\n",
    "        for i, word1 in enumerate(sent):\n",
    "            word1Index = wordIndices[word1]\n",
    "            for word2 in sent[i + 1:]:\n",
    "                coOcMat[word1Index][wordIndices[word2]] += 1\n",
    "    if makeMatrix:\n",
    "        return coOcMat, wordLst\n",
    "    else:\n",
    "        coOcMat = coOcMat.T + coOcMat\n",
    "        g = nx.convert_matrix.from_numpy_matrix(coOcMat)\n",
    "        g = nx.relabel_nodes(g, {i : w for i, w in enumerate(wordLst)})\n",
    "        for w in g.nodes:\n",
    "            g.nodes[w]['bipartite'] = wordsMap[w]\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gNN = posCooccurrence(redditTopScores['normalized_sents'].sum(), 'NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(gNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a bit too large to effectively visualize, so let's remove the vertices with degree less than or equal to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gNN.remove_nodes_from([n for n in gNN.nodes if len(set(gNN.neighbors(n))) <= 100]) \n",
    "print(nx.info(gNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And low weight edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gNN.remove_edges_from([(n1, n2) for n1, n2, d in gNN.edges(data = True) if d['weight'] <= 2])\n",
    "print(nx.info(gNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_nn = nx.spring_layout(gNN, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "centralities_nn = nx.eigenvector_centrality(gNN)\n",
    "maxC = max(centralities_nn.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in gNN.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in gNN.edges(data = True)))\n",
    "nx.draw(gNN, ax = ax, pos = layout_nn, labels = {n: n for n in gNN.nodes()},\n",
    "        #width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in gNN.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        node_color = [centralities_nn[n] / maxC for n in gNN.nodes],\n",
    "        node_size = [centralities_nn[n] / maxC * 100 for n in gNN.nodes],\n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:medium blue',\n",
    "        cmap = plt.get_cmap('plasma'),\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense: people are talking about work, support, computers, time... it's a tech support forum, so these are expected connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to look at noun-verb pairs instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gNV = posCooccurrence(redditTopScores['normalized_sents'].sum(), 'NN', 'VB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gNV` has co-occurrences between nouns and nouns as well as between verbs and verbs. Let's remove these and make it purely about noun and verb combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(gNV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gNV.remove_edges_from([(n1,n2) for n1,n2,d in gNV.edges(data = True) if gNV.nodes[n1]['bipartite'] == gNV.nodes[n2]['bipartite']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(gNV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping low weight edges and low degree vertices gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gNV.remove_edges_from([(n1, n2) for n1, n2, d in gNV.edges(data = True) if d['weight'] <= 2])\n",
    "gNV.remove_nodes_from([n for n in gNV.nodes if len(set(gNV.neighbors(n))) <= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(gNV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_nn = nx.spring_layout(gNV, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "centralities_nv = nx.eigenvector_centrality(gNV)\n",
    "maxC = max(centralities_nv.items(), key = lambda x : x[1])[1]\n",
    "maxWeight = max((d['weight'] for n1, n2, d in gNV.edges(data = True)))\n",
    "minWeight = min((d['weight'] for n1, n2, d in gNV.edges(data = True)))\n",
    "nx.draw(gNV, ax = ax, pos = layout_nn, labels = {n: n for n in gNV.nodes()},\n",
    "        #width=[(d['weight'] - minWeight + .7) / maxWeight for n1, n2, d in gNN.edges(data = True)], \n",
    "        alpha = .9, \n",
    "        node_color = [centralities_nv[n] / maxC for n in gNV.nodes],\n",
    "        node_size = [centralities_nv[n] / maxC * 100 for n in gNV.nodes],\n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:medium blue',\n",
    "        cmap = plt.get_cmap('plasma'),\n",
    "       ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create an \"ego network\" surrounding a single (important) word, as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_i = gNV.subgraph(['work'] + list(gNV.neighbors('work')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(g_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(g_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of just nodes connected to a vertex, we can find all those connected to it within 2 hops, lets look at 'look' (+ points for being meta) for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storyNeighbors = gNV.neighbors('look')\n",
    "set(storyNeighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storyNeighbors = set(gNV.neighbors('look'))\n",
    "storyNeighborsPlus1 = set(storyNeighbors)\n",
    "for n in storyNeighbors:\n",
    "    storyNeighborsPlus1 |= set(gNV.neighbors(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gNV_story = gNV.subgraph(storyNeighborsPlus1)\n",
    "print(nx.info(gNV_story))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a mid-sized network, but we can interrogate it intelligently by computing some statistics. Degree centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(nx.degree_centrality(gNV_story).items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or eigenvector centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(nx.eigenvector_centrality(gNV_story).items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_story = nx.spring_layout(gNV_story, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "nx.draw(gNV_story, ax = ax, pos = layout_story, labels = {n: n for n in gNV_story.nodes()},\n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:medium blue',\n",
    "       ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a noun-adjective network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gNJ = posCooccurrence(redditTopScores['normalized_sents'][:100].sum(), 'NN', 'JJ')\n",
    "print(nx.info(gNJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By filtering by a centrality measure we can get a more 'central' set of nodes instead of just the most connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gNJ_filtered200 = filterWords(gNJ, minWeight=3, filter_='betweenness', rule='number', value_of_rule=25)\n",
    "print(nx.info(gNJ_filtered200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_NJ_filtered200 = nx.spring_layout(gNJ_filtered200, weight='weight', iterations= 100)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "centralities_nj200 = nx.betweenness_centrality(gNJ_filtered200)\n",
    "maxC = max(centralities_nj200.items(), key = lambda x : x[1])[1]\n",
    "nx.draw(gNJ_filtered200, ax = ax, pos = layout_NJ_filtered200, labels = {n: n for n in gNJ_filtered200.nodes()},\n",
    "        alpha = .9, \n",
    "        width = .5,\n",
    "        node_color = [centralities_nj200[n] / maxC for n in gNJ_filtered200.nodes],\n",
    "        node_size = [centralities_nj200[n] / maxC * 100 for n in gNJ_filtered200.nodes],\n",
    "        font_size = 16,\n",
    "        font_color = 'xkcd:dark grey',\n",
    "        edge_color = 'xkcd:medium blue',\n",
    "        cmap = plt.get_cmap('plasma'),\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that construct at least two different networks comprising different combinations of word types, linked by different syntactic structures, which illuminate your corpus and the dynamics you are interested to explore. Graph these networks or subnetworks within them. What are relationships that are meaningful? \n",
    "\n",
    "<span style=\"color:red\">***Stretch***: Graph some word-centered \"ego-networks\" with words one link away, two links away, and three links away (we only did up to two links away above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating networks of agents from corpora\n",
    "\n",
    "Now that we have an idea of how we can use networks in python (a useful skill to know!), let us see how we can now use our knowledge of networks on extracting actors from corpora and building relations from them. Let us use the soap opera corpus now. \n",
    "\n",
    "In the last notebook we wrote methods to load corpora - this has been added to lucem_illud_2020.\n",
    "The code returns a dictionary with each id mapping to the text associated with it. \n",
    "\n",
    "We would then use the source file to create the dataframe with the data - since this varies for each corpora, there is no built in function, but would follow a similar process.\n",
    "\n",
    "This function might take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora_address = \"/Users/bhargavvader/Downloads/Academics_Tech/corpora/SOAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soap_texts = lucem_illud_2020.loadDavies(corpora_address, num_files=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the source to see how the data is stored. Note that this is different from the movies corpus, and that we will need to use a different aggregating method to store the data. Each dataset would have a different approach, but they are all very similar, it depends on how the data is stored. Here multiple textids match multiple scripts, so our soap dataframe would be structured a little differently. \n",
    "\n",
    "You can see the first 20 lines of the source file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zfile = zipfile.ZipFile(corpora_address + \"/soap_sources.zip\")\n",
    "source = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in zfile.namelist():\n",
    "    with zfile.open(file) as f:\n",
    "        for line in f:\n",
    "            source.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soap_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for soap in source[3:]:\n",
    "    try:\n",
    "        textID, year, show, url = soap.decode(\"utf-8\").split(\"\\t\")\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    if show.strip() not in soap_dict:\n",
    "        soap_dict[show.strip()] = []\n",
    "    if show.strip() in soap_dict:\n",
    "        try:\n",
    "            soap_dict[show.strip()].append(soap_texts[textID.strip()])\n",
    "        except KeyError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soap_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soap_df = pd.DataFrame(columns=[\"Soap Name\", \"Tokenized Texts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for soap in soap_dict:\n",
    "    # since there were multiple lists\n",
    "    print(soap)\n",
    "    full_script = []\n",
    "    for part in soap_dict[soap]:\n",
    "        full_script = full_script + part\n",
    "    soap_df.loc[i] = [soap, full_script]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have each Soap, and each of the Tokenized texts. Let us see what kind of information we can get. These are American soap operas, and are likely to be cheesy and dramatic (an understatment). A fun start would be to make networks of each of the actors and actresses in these soaps. \n",
    "\n",
    "What would be a good way to create a network? Maybe everytime someone talks to someone we add one weight? But we wouldn't want to add weights whenever it's a different scene - or maybe we do? Let us look at the text and figure it out.\n",
    "\n",
    "Note that we didn't add the year here because it spans over multiple years. If we are doing different kinds of analysis we would want to a years column as well.\n",
    "\n",
    "In my dataframe, Days of Our Lives is the 9th corpora, and I conducted my basic analysis on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dool = soap_df['Tokenized Texts'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(dool[0:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... we can't do our normal text processing. But this provides us with an interesting oppurtunity: every '@!' is followed by some useeful information. Let us do a quick check of how many characters exist here, and how many times they speak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "characters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for token in dool:\n",
    "    if token[0] == '@':\n",
    "        # all characters or actions start with @, so we add that to character\n",
    "        if token[2:] not in characters:\n",
    "            characters[token[2:]] = 0\n",
    "        if token[2:] in characters:\n",
    "            characters[token[2:]] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a lot of characters: but we notice a '@!' between certain actions too, such as screaming and sobbing. Let us maybe only look for characters with a high number of appearances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in characters:\n",
    "    if characters[character] > 2000:\n",
    "        print(character, characters[character])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check these folks out on the interwebz...a image of search of the name + \"days of our lives\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../data/dool/dool_john.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../data/dool/dool_brady.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../data/dool/dool_hope.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_philip.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_marlena.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../data/dool/dool_kate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../data/dool/dool_bo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../data/dool/dool_chloe.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image(filename='../data/dool/dool_sami.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../data/dool/dool_shawn.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../data/dool/dool_belle.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../data/dool/dool_lucas.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../data/dool/dool_nicole.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are definitely big, long-time players in the dramatic Days narrative. It would make sense to create a graph where each character who appears over 2000 times is a node, and each time they talk to each other, we add one to their weight. We should also store all the things these chracters say: that's useful information.\n",
    "\n",
    "So we now iterate through the tokens in a manner where we can capture this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actor_network = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for character in characters:\n",
    "    if characters[character] > 2000:\n",
    "        actor_network.add_node(character, lines_spoken= characters[character], words=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(actor_network.nodes.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_network.nodes.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_network.nodes['Sami']['lines_spoken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code creates the graph. Please carefully go through the code: you should be able to understand (more or less) what is going on here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for token in dool:\n",
    "    i += 1\n",
    "    if i > len(dool):\n",
    "        break\n",
    "    if token[0] == \"@\":\n",
    "        if token[2:] in actor_network.nodes():\n",
    "            j = i\n",
    "            for token_ in dool[i:]:\n",
    "                if token_[0] == \"@\":\n",
    "                    # if both the characters exist in the graph, add a weight\n",
    "                    if token_[2:] != token[2:] and token_[2:] in actor_network.nodes():\n",
    "                        if (token[2:], token_[2:]) not in actor_network.edges():\n",
    "                            actor_network.add_edge(token[2:], token_[2:], weight=0)\n",
    "                        if (token[2:], token_[2:]) in actor_network.edges():\n",
    "                            actor_network.edges[(token[2:], token_[2:])]['weight'] += 1\n",
    "                    break\n",
    "                j += 1\n",
    "            # adding characters sentences\n",
    "            actor_network.nodes[token[2:]]['words'].append(dool[i:j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(actor_network, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for node in actor_network.nodes():\n",
    "    l = []\n",
    "    for node_ in actor_network.nodes():\n",
    "        if node == node_:\n",
    "            l.append(0)\n",
    "        else:\n",
    "            l.append(actor_network.edges[(node, node_)]['weight'])\n",
    "    L.append(l)\n",
    "M_ = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M_, columns = list(actor_network.nodes()), index = list(actor_network.nodes()))\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "c = list(greedy_modularity_communities(actor_network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding structure in networks\n",
    "\n",
    "We now have a lot of useful information: we have a graph of all the characters, with their relationships with other characters, as well as all the words they've said. We tried finding communities, but it seems like everyone is connected to everyone: each of them form their own 'community'. Seems like people talk to each other a bunch in soaps.\n",
    "\n",
    "This is however, not the best network to find any meaningful patterns, as we can see with everyone connected to everyone. But as we can see with our heatmap, not everyone talks to everyone an equal amount. How about we only keep our \"important\" ties, where people are talking to each other a lot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smaller_actor_network = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for actor_1 in actor_network.nodes():\n",
    "    smaller_actor_network.add_node(actor_1, lines_spoken= actor_network.nodes[actor_1]['lines_spoken'], words=actor_network.nodes[actor_1]['words'])\n",
    "    for actor_2 in actor_network.nodes():\n",
    "        if actor_2!=actor_1 and actor_network.edges[(actor_1, actor_2)]['weight'] > 250:\n",
    "            smaller_actor_network.add_edge(actor_1, actor_2, weight=actor_network.edges[(actor_1, actor_2)]['weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(smaller_actor_network, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot more interesting: while the sets of characters overlap, there is still two distinct communities if you look at characters who regularly talk to each other!\n",
    "\n",
    "Let us see what our centrality measures look like, as well as communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "c = list(greedy_modularity_communities(smaller_actor_network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcentralities = nx.degree_centrality(smaller_actor_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcentralities['John'], dcentralities['Philip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our two different communities show up as detected by the networkx algorithm, and when we look at centralities, we can see that John is a lot more central than Philip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go back to our original graph, and see if the weight or number of similar appearences matches the text... how do we do this? Well, we already have the graph, and we also have information of who spoke to who. So we have our framework!\n",
    "\n",
    "This means we can explore ideas contained in two of the papers you will be reading: . No country for old members: User lifecycle and linguistic change in online communities., and  Fitting In or Standing Out? The Tradeoffs of Structural and Cultural Embeddedness, both of which you can access on Canvas. \n",
    "\n",
    "Let us use a simplified version of the papers, and check if a higher number of conversations might lead to a higher similarity between the word distributions for two characters. We can use the same divergences we used in the last notebook. Do you think it will match with the number of times each character was associated with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_divergence(X, Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    D_kl = scipy.stats.entropy(p, q)\n",
    "    return D_kl\n",
    "\n",
    "def chi2_divergence(X,Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    return scipy.stats.chisquare(p, q).statistic\n",
    "\n",
    "def Divergence(corpus1, corpus2, difference=\"KL\"):\n",
    "    \"\"\"Difference parameter can equal KL, Chi2, or Wass\"\"\"\n",
    "    freqP = nltk.FreqDist(corpus1)\n",
    "    P = pandas.DataFrame(list(freqP.values()), columns = ['frequency'], index = list(freqP.keys()))\n",
    "    freqQ = nltk.FreqDist(corpus2)\n",
    "    Q = pandas.DataFrame(list(freqQ.values()), columns = ['frequency'], index = list(freqQ.keys()))\n",
    "    if difference == \"KL\":\n",
    "        return kl_divergence(P, Q)\n",
    "    elif difference == \"Chi2\":\n",
    "        return chi2_divergence(P, Q)\n",
    "    elif difference == \"KS\":\n",
    "        try:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency']).statistic\n",
    "        except:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency'])\n",
    "    elif difference == \"Wasserstein\":\n",
    "        try:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None).statistic\n",
    "        except:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora = []\n",
    "for character in actor_network.nodes():\n",
    "    character_words = []\n",
    "    for sentence in actor_network.nodes[character]['words']:\n",
    "        for word in sentence:\n",
    "            character_words.append(word)\n",
    "    corpora.append(lucem_illud_2020.normalizeTokens(character_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p in corpora:\n",
    "    l = []\n",
    "    for q in corpora:\n",
    "        l.append(Divergence(p,q, difference='KS'))\n",
    "    L.append(l)\n",
    "M = np.array(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = list(actor_network.nodes()), index = list(actor_network.nodes()))\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.corrcoef(M_, M)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our two heatplots, we can attempt to do some rudimentary analysis. We can see from our previous plot that Shawn and Belle talk to each other a lot, so do Hope and Bo, and Nicole and Brady, and Lucas and Sami. Do they also talk *like* each other?\n",
    "\n",
    "Kind of, actually: all four of these pairs have a lower distance between them. Now I don't know anything about this particular soap... are these four pairs related? Are they in a relationship, either married or dating, or are they just really good friends?\n",
    "\n",
    "This lays out the frameworks which you can now use to explore your own networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactional influence\n",
    "\n",
    "In a recent paper by Fangjian Guo, Charles Blundell, Hanna Wallach, and Katherine Heller entitled [\"The Bayesian Echo Chamber: Modeling Social Influence via Linguistic Accommodation\"](https://arxiv.org/pdf/1411.2674.pdf), the authors develop a method to estimate the influence of one speaker on another in order to estimate a kind of interpersonal influence network. Here we walk through this method, which relies on a kind of point process called a Hawkes process that estimate the influence of one point on another. Specifically, what they estimate is the degree to which one actor to an interpersonal interaction engaged in \"accomodation\" behaviors relative to the other, generating a directed edge from the one to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's look at the output of their analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_name = '12-angry-men'   #example datasets: \"12-angry-men\" or \"USpresident\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_path = '../data/Bayesian-echo/results/{}/'.format(example_name)\n",
    "if not os.path.isdir(result_path):\n",
    "    raise ValueError('Invalid example selected, only \"12-angry-men\" or \"USpresident\" are avaliable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_info = pandas.read_table(result_path + 'meta-info.txt',header=None)\n",
    "df_log_prob = pandas.read_csv(result_path + \"SAMPLE-log_prior_and_log_likelihood.txt\",delim_whitespace=True) #log_prob samples\n",
    "df_influence = pandas.read_csv(result_path + 'SAMPLE-influence.txt',delim_whitespace=True) # influence samples\n",
    "df_participants = pandas.read_csv(result_path + 'cast.txt', delim_whitespace=True)\n",
    "person_id = pandas.Series(df_participants['agent.num'].values-1,index=df_participants['agent.name']).to_dict()\n",
    "print()\n",
    "print ('Person : ID')\n",
    "person_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDensity(df):\n",
    "    data = df#_log_prob['log.prior']\n",
    "    density = scipy.stats.gaussian_kde(data)\n",
    "    width = np.max(data) - np.min(data)\n",
    "    xs = np.linspace(np.min(data)-width/5, np.max(data)+width/5,600)\n",
    "    density.covariance_factor = lambda : .25\n",
    "    density._compute_covariance()\n",
    "    return xs, density(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot MCMC (Markov Monte Carlo) trace and the density of log-likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,10])\n",
    "\n",
    "plt.subplot(4,2,1)\n",
    "plt.plot(df_log_prob['log.prior'])\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Trace of log.prior')\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "x,y = getDensity(df_log_prob['log.prior'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.prior')\n",
    "\n",
    "plt.subplot(4,2,3)\n",
    "plt.plot(df_log_prob['log.likelihood'])\n",
    "plt.title('Trace of log.likelihood')\n",
    "plt.xlabel('Iterations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(4,2,4)\n",
    "x,y = getDensity(df_log_prob['log.likelihood'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.likelihood')\n",
    "\n",
    "plt.subplot(4,2,5)\n",
    "plt.plot(df_log_prob['log.likelihood.test.set'])\n",
    "plt.title('Trace of log.likelihood.test.set')\n",
    "plt.xlabel('Iterations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(4,2,6)\n",
    "x,y = getDensity(df_log_prob['log.likelihood.test.set'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.likelihood.test.set')\n",
    "\n",
    "plt.subplot(4,2,7)\n",
    "plt.plot(df_log_prob['log.prior']+df_log_prob['log.likelihood'])\n",
    "plt.title('Trace of log.prob')\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "plt.subplot(4,2,8)\n",
    "x,y = getDensity(df_log_prob['log.prior']+df_log_prob['log.likelihood'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.prob')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the influence matrix between participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = int(np.sqrt(len(df_influence.columns))) #number of participants\n",
    "id_person = {}\n",
    "for p in person_id:\n",
    "    id_person[person_id[p]]=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getmatrix(stacked,A):\n",
    "    influence_matrix = [[0 for i in range(A)] for j in range(A)]\n",
    "    for row in stacked.iteritems():\n",
    "        from_ = int(row[0].split('.')[1])-1\n",
    "        to_ = int(row[0].split('.')[2])-1\n",
    "        value = float(row[1])\n",
    "        influence_matrix[from_][to_]=value\n",
    "    df_ = pandas.DataFrame(influence_matrix) \n",
    "    \n",
    "    df_ =df_.rename(index = id_person)\n",
    "    df_ =df_.rename(columns = id_person)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked = df_influence.mean(axis=0)\n",
    "df_mean = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.std(axis=0)\n",
    "df_std = getmatrix(stacked,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "seaborn.heatmap(df_mean, annot=True,  linewidths=.5, ax=ax,cmap=\"YlGnBu\")\n",
    "print('MEAN of influence matrix (row=from, col=to)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "seaborn.heatmap(df_std, annot=True,  linewidths=.5, ax=ax,cmap=\"YlGnBu\")\n",
    "print('SD of influence matrix (row=from, col=to)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot of total influences sent/received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sender_std = {} #sd of total influence sent\n",
    "reciever_std = {} #sd of total influence recieved\n",
    "for i in range(A):\n",
    "    reciever_std[id_person[i]] = df_influence[df_influence.columns[i::A]].sum(axis=1).std()\n",
    "    sender_std[id_person[i]] = df_influence[df_influence.columns[i*A:(i+1)*A:]].sum(axis=1).std()\n",
    "\n",
    "sent = df_mean.sum(axis=1) #mean of total influence sent\n",
    "recieved =df_mean.sum(axis=0) #mean of total influence recieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total influence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\t\\tTotal linguistic influence sent/received \")\n",
    "ax.fig = plt.figure(figsize=[np.min([A,20]),6])\n",
    "\n",
    "plt.grid()\n",
    "wd=0.45\n",
    "ii=0\n",
    "for p in sender_std:\n",
    "    plt.bar(person_id[p],sent.loc[p],width=wd,color='red',alpha=0.6,label = \"Sent\" if ii == 0 else \"\")\n",
    "    plt.plot([person_id[p]-wd/4,person_id[p]+wd/4],[sent.loc[p]+sender_std[p],sent.loc[p]+sender_std[p]],color='k')\n",
    "    plt.plot([person_id[p]-wd/4,person_id[p]+wd/4],[sent.loc[p]-sender_std[p],sent.loc[p]-sender_std[p]],color='k')\n",
    "    plt.plot([person_id[p],person_id[p]],[sent.loc[p]-sender_std[p],sent.loc[p]+sender_std[p]],color='k')\n",
    "    ii+=1\n",
    "ii=0\n",
    "for p in reciever_std:\n",
    "    plt.bar(person_id[p]+wd,recieved.loc[p],width=wd,color='blue',alpha=0.4,label = \"Received\" if ii == 0 else \"\")\n",
    "    plt.plot([person_id[p]+wd-wd/4,person_id[p]+wd+wd/4],[recieved.loc[p]+reciever_std[p],recieved.loc[p]+reciever_std[p]],color='k')\n",
    "    plt.plot([person_id[p]+wd-wd/4,person_id[p]+wd+wd/4],[recieved.loc[p]-reciever_std[p],recieved.loc[p]-reciever_std[p]],color='k')\n",
    "    plt.plot([person_id[p]+wd,person_id[p]+wd],[recieved.loc[p]-reciever_std[p],recieved.loc[p]+reciever_std[p]],color='k')\n",
    "    ii+=1\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.7))\n",
    "plt.xticks([i+0.25 for i in range(A)],list(zip(*sorted(id_person.items())))[1])\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('speaker',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Influence Network!\n",
    "\n",
    "You can visualize any of the influence matrices above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using networkx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drawNetwork(df,title):\n",
    "    fig = plt.figure(figsize=[8,8])\n",
    "    G = nx.DiGraph()\n",
    "    for from_ in df.index:\n",
    "        for to_ in df.columns:\n",
    "            G.add_edge(from_,to_,weight = df.loc[from_][to_])\n",
    "            \n",
    "    pos = nx.spring_layout(G,k=0.55,iterations=20)\n",
    "    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    weights = np.array(weights)\n",
    "    #weights = weights*weights\n",
    "    weights = 6*weights/np.max(weights)\n",
    "    print(title)\n",
    "    \n",
    "    edge_colors=20*(weights/np.max(weights))\n",
    "    edge_colors = edge_colors.astype(int)\n",
    "#     nx.draw_networkx_nodes(G,pos,node_size=1200,alpha=0.7,node_color='#99cef7')\n",
    "#     nx.draw_networkx_edges(G,pos,edge_color=edge_colors)\n",
    "#     nx.draw_networkx_labels(G,pos,font_weight='bold')\n",
    "    nx.draw(G,pos,with_labels=True, font_weight='bold',width=weights,\\\n",
    "            edge_color=255-edge_colors,node_color='#99cef7',node_size=1200,\\\n",
    "            alpha=0.75,arrows=True,arrowsize=20)\n",
    "    return edge_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get quantile influence matrices for 25%, 50%, 75% quantile\n",
    "stacked = df_influence.quantile(0.25)\n",
    "df_q25 = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.quantile(0.5)\n",
    "df_q50 = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.quantile(0.75)\n",
    "df_q75 = getmatrix(stacked,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_mean = drawNetwork(df_mean,'Mean Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_q25 = drawNetwork(df_q25,'25 Quantile Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_q75 = drawNetwork(df_q75,'75 Quantile Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lucem_illud_2020\n",
    "import pandas\n",
    "def fakeEnglish(length):\n",
    "    listd=['a','b','c','d','e','f','g','s','h','i','j','k','l']\n",
    "    return ''.join(np.random.choice(listd,length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your own dataset should contains 4 columns (with the same column names) as the artificial one below:\n",
    "\n",
    "- name: name of the participant\n",
    "- tokens: a list of tokens in one utterance\n",
    "- start: starting time of utterance (unit doesn't matter, can be 'seconds','minutes','hours'...)\n",
    "- end: ending time of utterance (same unit as start)\n",
    "\n",
    "There is no need to sort data for the moment.\n",
    "\n",
    "Below, we generate a fake collection of data from \"Obama\", \"Trump\", \"Clinton\"...and other recent presidents. You can either create your own simulation OR (better), add real interactional data from a online chat forum, comment chain, or transcribed from a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script= []\n",
    "language = 'eng' #parameter, no need to tune if using English, accept:{'eng','chinese'}\n",
    "role = 'Adult' #parameter, no need to tune \n",
    "\n",
    "for i in range(290):\n",
    "    dt = []\n",
    "    dt.append(np.random.choice(['Obama','Trump','Clinton','Bush','Reagan','Carter','Ford','Nixon','Kennedy','Roosevelt']))\n",
    "    faketokens = [fakeEnglish(length = 4) for j in range(30)]\n",
    "    dt.append(faketokens) #fake utterance\n",
    "    dt.append(i*2+np.random.random()) # start time\n",
    "    dt.append(i*2+1+np.random.random()) # end time\n",
    "    script.append(dt)\n",
    "\n",
    "df_transcript = pandas.DataFrame(script,columns=['name','tokens','start','end']) #\"start\", \"end\" are timestamps of utterances, units don't matter\n",
    "df_transcript[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data into TalkbankXML format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fname = 'USpresident.xml'  #should be .xml\n",
    "language = 'eng' \n",
    "#language = 'chinese'\n",
    "lucem_illud_2020.make_TalkbankXML(df_transcript, output_fname, language = language )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Bayesian Echo Chamber to get estimation.\n",
    "\n",
    "- It may take a couple of hours. ( About 4-5 hours if Vocab_size=600 and sampling_time =2000)\n",
    "- Larger \"Vocab_size\" (see below) will cost more time\n",
    "- Larger \"sampling_time\" will also consume more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Vocab_size = 90 # up to Vocab_size most frequent words will be considered, it should be smaller than the total vocab\n",
    "sampling_time = 1500  #The times of Gibbs sampling sweeps  (500 burn-in not included)\n",
    "lucem_illud_2020.bec_run(output_fname, Vocab_size, language, sampling_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that perform a similar social similarity or influence analysis on a dataset relevant to your final project. Create relationships between actors in a network based on your dataset (e.g., person to person or document to document), and perform analyses that interrogate the structure of their interactions, similarity, and/or influence on one another. (For example, if relevant to your final project, you could explore different soap operas, counting how many times a character may have used the word love in conversation with another character, and identify if characters in love speak like each other. Or do opposites attract?) What does that analysis and its output reveal about the relative influence of each actor on others? What does it reveal about the social game being played?\n",
    "\n",
    "<span style=\"color:red\">Stretch 1:\n",
    "Render the social network with weights (e.g., based on the number of scenes in which actors appear together), then calculate the most central actors in the show.Realtime output can be viewed in shell.\n",
    "\n",
    "<span style=\"color:red\">Stretch 2:\n",
    "Implement more complex measures of similarity based on the papers you have read."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
