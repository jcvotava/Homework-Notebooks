{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week - 8 - Deep Neural Nets and Text\n",
    "\n",
    "In this week we introduce the use of Deep Neural Networks to work with text. We have already seen some uses of neural networks for text in our classification HW, where we used a simple neural network--the one-layer perceptron--to classify text. It performed quite well, but comes up short in more sophisticated classification tasks, such as in predicting intent. We have also seen slightly deeper, 2-level neural nets in the form of word embeddings such as Word2Vec. While they work well, they have some drawbacks, such as representing words with multiple meanings in a singular space. \n",
    "\n",
    "BERT, which is a language model built using bidirectional encoders, allows us to take advantage of a powerful pre-trained model which we can then use to perform our own tasks based on data we analyze. \n",
    "\n",
    "In this notebook we use ```huggingface/transformers```, a python package that allows for easy interface to use pre-trained BERT models. It is built using Tensorflow and PyTorch, two computational graph packages which are built specifically for creating powerful neural networks. We will also be introducing Keras, which allows us to easily build Neural Networks in an abstracted way. Keras is a popular way to understand how we can stack layers to create such Neural Networks, but to reach state-of-the-art results we will stick with using BERT and similar models that can be tuned to extremely high performance on specific language understanding and generation tasks.\n",
    "\n",
    "To demonstrate this, we begin by using the [Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/). We will also use BERT by learning how to extract embeddings from such a model and use it to semantically probe sentences. There are a number of new packages and methods we will be using so be sure to update lucem_illud_2020.\n",
    "\n",
    "## NOTE\n",
    "\n",
    "This notebook **requires** GPUs for training models in section 1 and section 3. To train models, please use this [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) to create the models. Note that I have only given you view access: please create your own colab file to train your models, using the code and instructions I have given in the Colab file. So while you have to do the homework on this notebook, the models which you will train should be done on Google Colab, which has GPU access. If you happen to have GPU access on your personal machines or some other way to train the models, you are welcome to do that too.\n",
    "\n",
    "Note that if you run the computationally intensive models on your local computer they will take a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\super\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\super\\anaconda3\\lib\\site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\super\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tranformers\n",
      "ERROR: No matching distribution found for tranformers\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install tranformers \n",
    "import torch \n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig \n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import lucem_illud #pip install -U git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoLA Dataset and pre-processing\n",
    "\n",
    "We start with loading our dataset and pre-processing it. The pre-processing follows similar steps as we have done in the past, but we will be using pre-written modules offered by the transformers package. These are some of the things we have to take care of when using this particular BERT model.\n",
    "\n",
    "    -special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP])\n",
    "    -tokens that conforms with the fixed vocabulary used in BERT\n",
    "    -token IDs from BERT’s tokenizer\n",
    "    -mask IDs to indicate which elements in the sequence are tokens and which are padding elements\n",
    "    -segment IDs used to distinguish different sentences\n",
    "    -positional embeddings used to show token position within the sequence\n",
    "\n",
    "\n",
    "We will be using parts of the code from [this notebook](https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=BJR6t_gCQe_x) which walks us through the process of using a pre-trained BERT model. The interface to use these models comes from the package [huggingface/transformers](https://github.com/huggingface/transformers). \n",
    "\n",
    "We start by setting up our GPU if we can - this may not work on your machine, so it has been commented out.\n",
    "\n",
    "An aside: check out this tutorial too - https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if gpu:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecw = pd.read_csv(\"../votava_project_data/three_marx_tiny.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_vol(text):\n",
    "    if text==\"Volume 3\":\n",
    "        return 0\n",
    "    elif text==\"Volume 24\":\n",
    "        return 1\n",
    "    elif text==\"Volume 35\":\n",
    "        return 2\n",
    "\n",
    "mecw['vol_num'] = mecw['vol_name'].apply(eliminate_vol) #re.search('[0-9]*$', mecw['vol_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = mecw.raw_sent.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = mecw.vol_num.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c513f17c767c45c394e2beb390b150da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenize the first sentence:\n",
      "['[CLS]', 'the', 'third', 'volume', 'of', 'the', 'works', 'of', 'marx', 'and', 'eng', '##els', 'covers', 'the', 'period', 'between', 'march', '1843', 'and', 'august', '1844', 'before', 'their', 'close', 'collaboration', 'began', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Deep Neural Nets\n",
    "\n",
    "A popular, simplified package for introducing deep neural networks is [Keras](https://keras.io). It is a high level package in that we don't bother with every detail or hyper-parameter associated with the neural network (e.g., regularizers), and can stack on layers directly. For a rapid tutorial on neural networks for text such as the LSTM or the Recurrent Neural Network, Colah's blog is a great start. [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is an article on LSTMs, and if you'd like to  learn about RNN, Andrej Karpathy does a great job in [this blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), in addition to our reading from the newest online verion of Jurafsky & Martin's review of deep learning methods in their book on speech and language processing, chapters [6,7,9,10](https://web.stanford.edu/~jurafsky/slp3/), and the [*Deep Learning*](https://www.deeplearningbook.org/) book by Goodfellow, Bengio & Courville.\n",
    "\n",
    "In the following cells we build a basic deep net that has an embedding layer and an LSTM to perform classification. This is to illustrate the process of using Keras, which is a very popular library for such work. It may not yield state of the art performance because it constrains the hyperparameters you can tune, but is nonetheless an useful tool and works well on some datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 512, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,030,207\n",
      "Trainable params: 1,030,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 3s 160ms/step - loss: 1.1000 - accuracy: 0.3236\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 1s 159ms/step - loss: 1.0988 - accuracy: 0.3453\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 1s 155ms/step - loss: 1.0968 - accuracy: 0.3836\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 1s 152ms/step - loss: 1.1026 - accuracy: 0.3097\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 1s 154ms/step - loss: 1.0989 - accuracy: 0.3304\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 1s 154ms/step - loss: 1.0977 - accuracy: 0.3645\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 1s 163ms/step - loss: 1.1023 - accuracy: 0.2901\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 1s 154ms/step - loss: 1.0973 - accuracy: 0.3551\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 1s 149ms/step - loss: 1.0993 - accuracy: 0.3277\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 1s 150ms/step - loss: 1.0984 - accuracy: 0.3371\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of this model isn't terrible, but it still hovers around 70%. Below there is code for a slightly modified neural network - how does this one perform? Note that in this model, I have added another LSTM layer. You are encouraged to explore the [Keras documentaion](https://keras.io/layers/about-keras-layers/) to explore what kind of layers you can add and how they change performances for different tasks. Different kinds of losses, optimizers, activations and layers can change the flavour of your net dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_lstm2 = Sequential()\n",
    "# model_lstm2.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "# model_lstm2.add(LSTM(units))\n",
    "# model_lstm2.add(LSTM(units))\n",
    "# model_lstm2.add(Dense(1, activation='sigmoid'))\n",
    "# model_lstm2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# history_lstm2 = model_lstm2.fit(input_data_train, labels, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On with BERT!\n",
    "\n",
    "So while Neural Networks can do a good job with some kind of classification tasks, they don't perform too well on intent classification. Let us see how a bidirectional transformer embedding like BERT might do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Models\n",
    "\n",
    "### Train Model\n",
    "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
    "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "### Structure of Fine-Tuning Model\n",
    "\n",
    "As we've showed beforehand, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into cross-entropy loss.\n",
    "\n",
    "### The Fine-Tuning Process\n",
    "\n",
    "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
    "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
    "\n",
    "Credit to Michel Kana's [tutorial](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03) and the [tutorial](https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=GuE5BqICAne2) by Chris McCormick and Nick Ryan who describe the workings of BERT and the way it is used by the ```transformers``` package. \n",
    "\n",
    "## WARNING: SHIFT TO A GPU ENABLED MACHINE (e.g., Google Colab)\n",
    "\n",
    "Note that you only want to run the following code if you have a GPU. Otherwise, rerun the **same** cells we just ran on the Colab file to train your model, download it to your local, and load it by running\n",
    "```model = BertForSequenceClassification.from_pretrained(\"my_model_directory\", num_labels=2)```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b681923042e4402a2aa7c3a23df8071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=433.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c398485cceb34b439cbb7ab6a61259fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-fc1addb66b71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert-base-uncased\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \"\"\"\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    407\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \"\"\"\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 2\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following cell can take upto 12 hours or longer if run without a GPU. The [Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) demonstrates how to fine-tune models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-b74926eb775d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m                     \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                     labels=b_labels)\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# The call to `model` always returns a tuple, so we need to pull the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1503\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1505\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1506\u001b[0m         )\n\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    974\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    977\u001b[0m         )\n\u001b[0;32m    978\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    572\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m                 )\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m         )\n\u001b[0;32m    462\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         \u001b[0mcontext_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m         \u001b[0mnew_context_layer_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Use plot styling from seaborn.\n",
    "# sns.set(style='darkgrid')\n",
    "\n",
    "# # Increase the plot size and font size.\n",
    "# sns.set(font_scale=1.5)\n",
    "# plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# # Plot the learning curve.\n",
    "# plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# # Label the plot.\n",
    "# plt.title(\"Training loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COME BACK TO THIS NOTEBOOK to load and work with your trained model\n",
    "\n",
    "Once you tune your model on Colab (or on your own machine if you decided to do that instead), you load it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"model_save\", num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 11,304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "test_df = pd.read_csv(\"../votava_project_data/three_marx_frag_test.csv\")\n",
    "def eliminate_vol(text):\n",
    "    if text==\"Volume 3\":\n",
    "        return 0\n",
    "    elif text==\"Volume 24\":\n",
    "        return 1\n",
    "    elif text==\"Volume 35\":\n",
    "        return 2\n",
    "\n",
    "test_df['vol_num'] = test_df['vol_name'].apply(eliminate_vol) #re.search('[0-9]*$', mecw['vol_name'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(test_df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = test_df.raw_sent.values\n",
    "labels = test_df.vol_num.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=190, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 516 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 354 of 516 (68.60%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhargavvader/open_source/Content-Analysis-2020/venv/lib/python3.5/site-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.049286405809014416,\n",
       " 0.014456362470655182,\n",
       " 0.4732058754737091,\n",
       " 0.4414147946478204,\n",
       " 0.44440090347500916,\n",
       " 0.7410010097502685,\n",
       " 0.6201736729460423,\n",
       " 0.47519096331149147,\n",
       " 1.0,\n",
       " 0.5659164584181102,\n",
       " 0.7679476477883045,\n",
       " 0.647150228929434,\n",
       " 0.8150678894028793,\n",
       " 0.647150228929434,\n",
       " 0.3268228676411533,\n",
       " 0.5844155844155844,\n",
       " 0.0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.550\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good performance. Note that we used [Matthews Correlation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) to meausure this. It ranges from -1 to 1, with +1 being the best. The Google BERT model has a similar score too, so this model performed quite well. It took a long time though, approximately a day with no GPU. It would be significantly faster if a CUDA enabled machine ran this. Hence, we recommend that you run this on the Collab notebook.\n",
    "\n",
    "The following lines save the model to disk, if you would like to: note that we ran this in the colab file to save it to disk there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "# output_dir = './model_save/'\n",
    "\n",
    "# # Create output directory if needed\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# # They can then be reloaded using `from_pretrained()`\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "# model_to_save.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# # Good practice: save your training arguments together with the trained model\n",
    "# # torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that estimate a deep classification model with Keras (and LSTM) and also BERT in order to predict pre-established data labels relevant to your final project (as for week 3's homework). Which works better? Are the errors the same or different?\n",
    "\n",
    "<span style=\"color:red\">***Stretch***</span>: <span style=\"color:red\">Now alter the neural network by stacking network layers, adjusting the embedding dimension, compare its performance with your model above, and interpret why it might be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecw = pd.read_csv(\"../votava_project_data/three_marx_frag.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_vol(text):\n",
    "    if text==\"Volume 3\":\n",
    "        return 0\n",
    "    elif text==\"Volume 24\":\n",
    "        return 1\n",
    "    elif text==\"Volume 35\":\n",
    "        return 2\n",
    "\n",
    "mecw['vol_num'] = mecw['vol_name'].apply(eliminate_vol) #re.search('[0-9]*$', mecw['vol_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = mecw.raw_sent.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = mecw.vol_num.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'the', 'third', 'volume', 'of', 'the', 'works', 'of', 'marx', 'and', 'eng', '##els', 'covers', 'the', 'period', 'between', 'march', '1843', 'and', 'august', '1844', 'before', 'their', 'close', 'collaboration', 'began', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])\n",
    "\n",
    "#MAX_LEN = 512\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14174, 512)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 512, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,030,207\n",
      "Trainable params: 1,030,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=512))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "443/443 [==============================] - 63s 139ms/step - loss: 1.0941 - accuracy: 0.3831\n",
      "Epoch 2/10\n",
      "443/443 [==============================] - 62s 139ms/step - loss: 1.0921 - accuracy: 0.3903\n",
      "Epoch 3/10\n",
      "443/443 [==============================] - 60s 136ms/step - loss: 1.0908 - accuracy: 0.3939\n",
      "Epoch 4/10\n",
      "443/443 [==============================] - 62s 139ms/step - loss: 1.0943 - accuracy: 0.3773\n",
      "Epoch 5/10\n",
      "443/443 [==============================] - 59s 133ms/step - loss: 1.0927 - accuracy: 0.3866\n",
      "Epoch 6/10\n",
      "443/443 [==============================] - 61s 139ms/step - loss: 1.0934 - accuracy: 0.3823\n",
      "Epoch 7/10\n",
      "443/443 [==============================] - 70s 157ms/step - loss: 1.0921 - accuracy: 0.3883\n",
      "Epoch 8/10\n",
      "443/443 [==============================] - 89s 200ms/step - loss: 1.0916 - accuracy: 0.3927\n",
      "Epoch 9/10\n",
      "443/443 [==============================] - 73s 165ms/step - loss: 1.0939 - accuracy: 0.3783\n",
      "Epoch 10/10\n",
      "443/443 [==============================] - 72s 162ms/step - loss: 1.0936 - accuracy: 0.3812\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"model_save\", num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained on Colab; Holdout data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 11,304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "test_df = pd.read_csv(\"../votava_project_data/three_marx_frag_test.csv\")\n",
    "def eliminate_vol(text):\n",
    "    if text==\"Volume 3\":\n",
    "        return 0\n",
    "    elif text==\"Volume 24\":\n",
    "        return 1\n",
    "    elif text==\"Volume 35\":\n",
    "        return 2\n",
    "\n",
    "test_df['vol_num'] = test_df['vol_name'].apply(eliminate_vol) #re.search('[0-9]*$', mecw['vol_name'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(test_df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = test_df.raw_sent.values\n",
    "labels = test_df.vol_num.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=190, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 11,304 test sentences...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-6b924c3d5378>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m       \u001b[1;31m# Forward pass, calculate logit predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m       outputs = model(b_input_ids, token_type_ids=None, \n\u001b[1;32m---> 24\u001b[1;33m                       attention_mask=b_input_mask)\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1503\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1505\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1506\u001b[0m         )\n\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    974\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    977\u001b[0m         )\n\u001b[0;32m    978\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    572\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m                 )\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[1;32m--> 496\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m         )\n\u001b[0;32m    498\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   1785\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT MCC score = 0.442; CoLAS MCC score = 0.514"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings, Context Words\n",
    "\n",
    "We saw how a bootstrapped BERT model performed so much better than a model trained from scatch. Because BERT's method of capturing context is bidirectional, meaning that words can now have different word embedding values based on their location within a sentence. Let us use the same BERT model to capture sentence and word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the sentence format for the BERT model, as well as how our vocabulary looks like. Note that you have to use the BERT tokenizer to use the BERT model because of the similar vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTS model uses a WordPiece technique to do its tokenizing, as described in the paper. That's why the word embedding is split up the way it is.\n",
    "A quick peek at what the voabulary looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peninsula',\n",
       " 'adults',\n",
       " 'novels',\n",
       " 'emerged',\n",
       " 'vienna',\n",
       " 'metro',\n",
       " 'debuted',\n",
       " 'shoes',\n",
       " 'tamil',\n",
       " 'songwriter',\n",
       " 'meets',\n",
       " 'prove',\n",
       " 'beating',\n",
       " 'instance',\n",
       " 'heaven',\n",
       " 'scared',\n",
       " 'sending',\n",
       " 'marks',\n",
       " 'artistic',\n",
       " 'passage',\n",
       " 'superior',\n",
       " '03',\n",
       " 'significantly',\n",
       " 'shopping',\n",
       " '##tive',\n",
       " 'retained',\n",
       " '##izing',\n",
       " 'malaysia',\n",
       " 'technique',\n",
       " 'cheeks']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[6000:6030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indices.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment ID\n",
    "\n",
    "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in “tokenized_text,” we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence.\n",
    "\n",
    "If you want to process two sentences, assign each word in the first sentence plus the ‘[SEP]’ token a 0, and all tokens of the second sentence a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did for classification, we now convert these segments to tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer is the hidden state layer, and this is what we pick up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model_embedding.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = model_embedding(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0][0][0]), len(output[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "This kind of forward pass returns us the last layer of the net, which we will use to make our vectors. The first object returned contains the batch number, followed by each of the tokens and their vector values. The second object contains a vector value, which I suspect is the sentence vector of the tokens. \n",
    "\n",
    "The first index is the batch size, and our batch size is 1, so we just choose the 0th index and work with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embeddings, sentence_embedding = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
       "        [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
       "        [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
       "        ...,\n",
       "        [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
       "        [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
       "        [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a quick look at the range of values for a given layer and token.\n",
    "\n",
    "You’ll find that the range is fairly similar for all layers and tokens, with the majority of values falling between [-2, 2], and a small smattering of values around -10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec = word_embeddings[0][0]\n",
    "vec = vec.detach().numpy()\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are grouped by layer - we can use the permute function to make it grouped by each individual token instead. Let us look at what the later looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "\n",
    "So each of those tokens have embedding values - let us try and compare them with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_vecs = []\n",
    "# For each token in the sentence...\n",
    "for embedding in word_embeddings[0]:\n",
    "    cat_vec = embedding.detach().numpy()\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs.append(cat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method to create the vectors is to sum the last four layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Vector\n",
    "\n",
    "To get a single vector for our entire sentence we have multiple application-dependent strategies - we could just average all the tokens in our sentence. We can also use this oppurtunity to see if the second vector returned is a sentence vector too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_embedding_0 = sentence_embedding.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_embedding_1 = np.mean(token_vecs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_embedding_0), len(sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the power of these vectors is how they are context dependant - our sentence had multiple uses of the word bank. Let us see the index and the word of the sentence and check the context accordingly. We'll then print the simlarity values for the similar and different meanings and see how it turns out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"bank\".\n",
      "\n",
      "bank vault    [ 0.9001056  -0.53804165 -0.16690847  0.22416186  0.6896585 ]\n",
      "bank robber   [ 0.7977126  -0.52172744 -0.1983698   0.18898535  0.59409326]\n",
      "river bank    [ 0.29608926 -0.28563383 -0.03818326  0.16736214  0.7712624 ]\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"bank\".')\n",
    "print('')\n",
    "print(\"bank vault   \", str(token_vecs[6][:5]))\n",
    "print(\"bank robber  \", str(token_vecs[10][:5]))\n",
    "print(\"river bank   \", str(token_vecs[19][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.95\n",
      "Vector similarity for *different* meanings:  0.70\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs[10], token_vecs[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs[10], token_vecs[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense! Let us see if the mean value of all the tokens and what we think is the sentence vector is the same thing, by checking their cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008313187398016453"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sentence_embedding_0, sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is good - it seems it is indeed the sentence vector, so we can now write two functions which calculate the word and sentence vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_vector(text, word_id, model, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings, sentence_embeddings = model(tokens_tensor)   \n",
    "    vector = word_embeddings[0][word_id].detach().numpy()\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_10 = word_vector(text, 6, model_embedding, tokenizer)\n",
    "word_6 = word_vector(text, 10, model_embedding, tokenizer)\n",
    "word_19 = word_vector(text, 19, model_embedding, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_vector(text, model, tokenizer, method=\"average\"):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings, sentence_embeddings = model(tokens_tensor)\n",
    "    token_vecs = []\n",
    "    \n",
    "    for embedding in word_embeddings[0]:\n",
    "        cat_vec = embedding.detach().numpy()\n",
    "        token_vecs.append(cat_vec)\n",
    "        \n",
    "    if method == \"average\":\n",
    "        sentence_embedding = np.mean(token_vecs, axis=0)\n",
    "    if method == \"model\":\n",
    "        sentence_embedding = sentence_embeddings\n",
    "    # do something\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen_vec_0 = sentence_vector(text, model_embedding, tokenizer)\n",
    "sen_vec_1 = sentence_vector(text, model_embedding, tokenizer, method=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity metrics\n",
    "It is worth noting that word-level similarity comparisons are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence it appears in. This enables direct sensitivity to polysemy so that, e.g., your representation encodes river “bank” and not a financial institution “bank”. Nevertheless, it makes direct word-to-word similarity comparisons less valuable. For sentence embeddings, however, similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs as some similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n",
    "\n",
    "### Using the Vectors\n",
    "\n",
    "Without fine-tuning, BERT features may be less useful than plain GloVe or word2vec.\n",
    "They start to be interesting when you fine-tune a classifier on top of BERT. \n",
    "\n",
    "### Using Transformers Pipelines\n",
    "\n",
    "The context vectors make the other pipeline functions which transformers has built in a lot more powerful. \n",
    "\n",
    "### NOTE\n",
    "The pipeline functionality in transformers is currently being worked on and might be broken, so it is an optional part of the exercise. Do try to uncomment the lines of code and try to see if it works, though! If you have managed to get transformers v2.5.1 installed, it will work - I managed to get it to work sometimes, it can be annoying to get it to work but when it works it works well.\n",
    "\n",
    "Consider the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Allocate a pipeline for sentiment-analysis\n",
    "# nlp_sentiment = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nlp_sentiment(\"This BERT model is so good at classifiying sentiment, I love it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a strong positive sentiment, which we'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nlp_sentiment(\"I'm so sad that I have to spend this weekend just doing HW and readings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative label, bingo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Allocate a pipeline for question-answering\n",
    "# nlp_question = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nlp_question({\n",
    "#     'question': 'What is my favorite thing to do on weekends ?',\n",
    "#     'context': 'There is nothing I like more than analysing complex textual data all weekend '\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also great at question-answering tasks!\n",
    "We can also extract features, as we manually did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nlp_feature = pipeline('feature-extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vec = nlp_feature(\"Just sitting here exploring data all day long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(vec[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huggingface/transformers repository lists the other pipeline functions, such as ner extraction, sequence classification, and masking. You are encouraged to explore them. \n",
    "https://github.com/huggingface/transformers#quick-tour-of-pipelines\n",
    "\n",
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, use the pipeline functions or the word or sentence vector functions (e.g., similarity) to explore the social game underlying the production and meaning of texts associated with your final project. You have used similar, but often weaker versions in previous weeks. How does BERT help you gain insight regarding your research question that is similar and different from prior methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = BertForSequenceClassification.from_pretrained(\"model_save\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8128de279c5049e69e0f6a1404745f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=629.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929dbb78388248b5be8b3fa323795a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=267844284.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c82f2effc148edbca89d904bcb9c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83f97161e2b4349acfe135c33d9ef65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp_sentiment = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997735023498535}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"This BERT model is so good at classifiying sentiment, I love it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c420d2da24e9443e8f356c31ce92ad2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=411.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f037cc4b101243388daa59b44d0b551c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=263273408.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7f886555ca4f7ba68f298ae0ccc983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=213450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fc28fa03ea44b38f8e6991964c75c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435797.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlp_feature = pipeline('feature-extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = nlp_feature(\"Just sitting here exploring data all day long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.28079739212989807,\n",
       "   0.06644448637962341,\n",
       "   -0.1676395833492279,\n",
       "   -0.16896066069602966,\n",
       "   -0.226608008146286,\n",
       "   -0.1412712186574936,\n",
       "   0.42102956771850586,\n",
       "   -0.09199239313602448,\n",
       "   -0.031907789409160614,\n",
       "   -0.8537280559539795,\n",
       "   -0.26838088035583496,\n",
       "   0.08569944649934769,\n",
       "   -0.13404320180416107,\n",
       "   -0.03496011719107628,\n",
       "   -0.5356060266494751,\n",
       "   -0.029006775468587875,\n",
       "   0.20110875368118286,\n",
       "   0.080403171479702,\n",
       "   -0.08755533397197723,\n",
       "   -0.08443005383014679,\n",
       "   0.05363840609788895,\n",
       "   -0.18426790833473206,\n",
       "   0.5852553248405457,\n",
       "   -0.17160028219223022,\n",
       "   0.05120448023080826,\n",
       "   0.12319845706224442,\n",
       "   0.3730120360851288,\n",
       "   0.16151969134807587,\n",
       "   -0.17832441627979279,\n",
       "   0.5117567181587219,\n",
       "   -0.020196471363306046,\n",
       "   0.19047746062278748,\n",
       "   0.06237801909446716,\n",
       "   0.05721729248762131,\n",
       "   -0.36198359727859497,\n",
       "   0.2284064143896103,\n",
       "   -0.18473586440086365,\n",
       "   -0.21770460903644562,\n",
       "   -0.0701812207698822,\n",
       "   -0.09863680601119995,\n",
       "   -0.5033518075942993,\n",
       "   0.11645957082509995,\n",
       "   0.5387319922447205,\n",
       "   -0.18301959335803986,\n",
       "   0.04567187279462814,\n",
       "   -0.4805183410644531,\n",
       "   0.0769483745098114,\n",
       "   0.03903829678893089,\n",
       "   -0.08155317604541779,\n",
       "   0.13631227612495422,\n",
       "   -0.09813513606786728,\n",
       "   0.09356572479009628,\n",
       "   -0.19313713908195496,\n",
       "   -0.040969714522361755,\n",
       "   0.1512814462184906,\n",
       "   0.08282719552516937,\n",
       "   -0.06292310357093811,\n",
       "   0.1353553831577301,\n",
       "   -0.557519257068634,\n",
       "   0.2614961564540863,\n",
       "   -0.07125762104988098,\n",
       "   0.033854980021715164,\n",
       "   0.33907970786094666,\n",
       "   0.06560900807380676,\n",
       "   -0.2229636311531067,\n",
       "   0.07824651151895523,\n",
       "   0.04569856822490692,\n",
       "   0.21588364243507385,\n",
       "   -0.19988077878952026,\n",
       "   -0.21166890859603882,\n",
       "   0.09120085090398788,\n",
       "   0.2271878868341446,\n",
       "   0.303043395280838,\n",
       "   0.890407919883728,\n",
       "   0.22981896996498108,\n",
       "   -0.211568683385849,\n",
       "   0.3337336778640747,\n",
       "   -0.04453232139348984,\n",
       "   -0.027119532227516174,\n",
       "   -0.09593834728002548,\n",
       "   0.05773618072271347,\n",
       "   0.19910460710525513,\n",
       "   -0.11624056845903397,\n",
       "   -0.22660017013549805,\n",
       "   -0.028790153563022614,\n",
       "   -0.13013097643852234,\n",
       "   0.10612834990024567,\n",
       "   -0.11570257693529129,\n",
       "   -0.08082102239131927,\n",
       "   0.15435481071472168,\n",
       "   0.1601402908563614,\n",
       "   -0.15008313953876495,\n",
       "   -0.24606406688690186,\n",
       "   0.04691629484295845,\n",
       "   -0.20780432224273682,\n",
       "   0.10484088212251663,\n",
       "   -0.042709097266197205,\n",
       "   0.07523740082979202,\n",
       "   6.085381507873535,\n",
       "   -0.06166774407029152,\n",
       "   -0.23195812106132507,\n",
       "   0.06998208910226822,\n",
       "   0.09388383477926254,\n",
       "   -0.11683923751115799,\n",
       "   0.2404945343732834,\n",
       "   -0.3288646340370178,\n",
       "   -0.034948572516441345,\n",
       "   -0.46981388330459595,\n",
       "   0.034410007297992706,\n",
       "   0.49799227714538574,\n",
       "   0.3973344564437866,\n",
       "   0.0845702588558197,\n",
       "   0.12022995203733444,\n",
       "   -0.10584191977977753,\n",
       "   -0.09611064195632935,\n",
       "   -0.3282928764820099,\n",
       "   0.09671970456838608,\n",
       "   0.049704186618328094,\n",
       "   0.07413183897733688,\n",
       "   -0.21588188409805298,\n",
       "   0.1565270721912384,\n",
       "   0.005386315286159515,\n",
       "   0.9506380558013916,\n",
       "   0.10336951911449432,\n",
       "   -0.1121620312333107,\n",
       "   -0.0011060535907745361,\n",
       "   -0.001170797273516655,\n",
       "   -0.2742651700973511,\n",
       "   0.1074809730052948,\n",
       "   -0.16263099014759064,\n",
       "   -0.48938655853271484,\n",
       "   -0.2280602902173996,\n",
       "   -0.07158295810222626,\n",
       "   -0.14923258125782013,\n",
       "   0.10891932249069214,\n",
       "   0.008096210658550262,\n",
       "   -0.04215299338102341,\n",
       "   -0.03626145422458649,\n",
       "   -0.8590203523635864,\n",
       "   0.11949522793292999,\n",
       "   0.09703489392995834,\n",
       "   -0.09249436855316162,\n",
       "   0.09470716118812561,\n",
       "   -0.1625538319349289,\n",
       "   -0.2843359708786011,\n",
       "   2.624413251876831,\n",
       "   -0.12164173275232315,\n",
       "   0.03022586554288864,\n",
       "   -0.1260497123003006,\n",
       "   0.05357114225625992,\n",
       "   -0.08468543738126755,\n",
       "   -0.18748117983341217,\n",
       "   -0.2489502876996994,\n",
       "   0.23051662743091583,\n",
       "   -0.2518126964569092,\n",
       "   -0.21527042984962463,\n",
       "   0.18334002792835236,\n",
       "   0.2167530208826065,\n",
       "   0.0543195940554142,\n",
       "   0.09860698878765106,\n",
       "   -1.0202133655548096,\n",
       "   -0.007301457226276398,\n",
       "   -0.22835980355739594,\n",
       "   0.23675405979156494,\n",
       "   0.13464242219924927,\n",
       "   -0.23988942801952362,\n",
       "   -0.0015056096017360687,\n",
       "   -0.6380898952484131,\n",
       "   0.023236213251948357,\n",
       "   0.3602570593357086,\n",
       "   0.04916906729340553,\n",
       "   0.03163645416498184,\n",
       "   -2.1947343349456787,\n",
       "   0.31702831387519836,\n",
       "   0.13879573345184326,\n",
       "   0.13678285479545593,\n",
       "   0.003398442640900612,\n",
       "   0.13875307142734528,\n",
       "   0.005030415952205658,\n",
       "   -0.28041303157806396,\n",
       "   -0.17104019224643707,\n",
       "   0.38404715061187744,\n",
       "   0.13680821657180786,\n",
       "   0.0855024978518486,\n",
       "   -0.4032357931137085,\n",
       "   0.015949806198477745,\n",
       "   0.12988658249378204,\n",
       "   -0.19909211993217468,\n",
       "   -0.03933064639568329,\n",
       "   0.0079212486743927,\n",
       "   -0.13287629187107086,\n",
       "   -0.3336023986339569,\n",
       "   -0.12062512338161469,\n",
       "   0.1958886682987213,\n",
       "   0.1931966096162796,\n",
       "   0.13310930132865906,\n",
       "   0.05858064815402031,\n",
       "   -0.0909571647644043,\n",
       "   -0.06136387586593628,\n",
       "   0.17067182064056396,\n",
       "   -0.18688468635082245,\n",
       "   -0.12096945196390152,\n",
       "   -0.14287227392196655,\n",
       "   0.14427250623703003,\n",
       "   0.36059847474098206,\n",
       "   0.1913677304983139,\n",
       "   -0.037867289036512375,\n",
       "   0.11734570562839508,\n",
       "   0.17427386343479156,\n",
       "   -0.06699500977993011,\n",
       "   -0.1957787275314331,\n",
       "   0.025702524930238724,\n",
       "   0.08491039276123047,\n",
       "   0.42135998606681824,\n",
       "   0.5814639329910278,\n",
       "   -0.3963927924633026,\n",
       "   -0.03314342349767685,\n",
       "   -0.08839766681194305,\n",
       "   0.09495885670185089,\n",
       "   0.04652808979153633,\n",
       "   -0.10590074956417084,\n",
       "   0.12356507033109665,\n",
       "   -0.17063219845294952,\n",
       "   -0.11698144674301147,\n",
       "   -0.23107440769672394,\n",
       "   0.05537473037838936,\n",
       "   0.29407259821891785,\n",
       "   0.20254667103290558,\n",
       "   0.011806117370724678,\n",
       "   -0.33295175433158875,\n",
       "   0.2028825879096985,\n",
       "   0.184933602809906,\n",
       "   0.0973740667104721,\n",
       "   -0.2933533787727356,\n",
       "   -0.014712829142808914,\n",
       "   -0.0379217155277729,\n",
       "   -0.017111942172050476,\n",
       "   0.08761607110500336,\n",
       "   -0.030812039971351624,\n",
       "   -0.08504326641559601,\n",
       "   0.07453308999538422,\n",
       "   -0.0010541900992393494,\n",
       "   0.3211291432380676,\n",
       "   0.02436823584139347,\n",
       "   0.00799017958343029,\n",
       "   0.06423588842153549,\n",
       "   0.34073495864868164,\n",
       "   0.29837697744369507,\n",
       "   0.07283453643321991,\n",
       "   -0.02352588251233101,\n",
       "   0.610021710395813,\n",
       "   0.10051998496055603,\n",
       "   -0.18003572523593903,\n",
       "   -0.2914906442165375,\n",
       "   -0.5349467396736145,\n",
       "   -0.14874976873397827,\n",
       "   -0.04322607442736626,\n",
       "   -1.198394536972046,\n",
       "   -0.11274406313896179,\n",
       "   -0.333660751581192,\n",
       "   0.2787923216819763,\n",
       "   -2.9348483085632324,\n",
       "   0.024573897942900658,\n",
       "   0.2400948405265808,\n",
       "   -0.0015406087040901184,\n",
       "   -0.10958180576562881,\n",
       "   0.29140976071357727,\n",
       "   0.09774919599294662,\n",
       "   -0.4551195502281189,\n",
       "   0.23169535398483276,\n",
       "   -0.4662081003189087,\n",
       "   0.02112693339586258,\n",
       "   -0.6199561357498169,\n",
       "   0.06250546872615814,\n",
       "   -0.12774650752544403,\n",
       "   0.2570406496524811,\n",
       "   -0.062075503170490265,\n",
       "   -0.05455458536744118,\n",
       "   -0.23015941679477692,\n",
       "   -0.21192146837711334,\n",
       "   -0.1458437442779541,\n",
       "   0.05948434770107269,\n",
       "   -0.3123905658721924,\n",
       "   0.4359770715236664,\n",
       "   -0.2780085504055023,\n",
       "   0.3652206063270569,\n",
       "   0.3439832925796509,\n",
       "   0.19921989738941193,\n",
       "   0.10600961744785309,\n",
       "   3.681917428970337,\n",
       "   0.27655014395713806,\n",
       "   -0.1545557677745819,\n",
       "   0.09237848222255707,\n",
       "   -0.30224061012268066,\n",
       "   0.01311991736292839,\n",
       "   0.04309062287211418,\n",
       "   -0.45908603072166443,\n",
       "   0.030745327472686768,\n",
       "   -0.07204131782054901,\n",
       "   -0.0852026641368866,\n",
       "   -0.23873859643936157,\n",
       "   0.0592922642827034,\n",
       "   -0.4444526731967926,\n",
       "   -0.054805781692266464,\n",
       "   -0.4397674798965454,\n",
       "   0.04569707438349724,\n",
       "   0.03808179497718811,\n",
       "   0.10586247593164444,\n",
       "   -0.6242750883102417,\n",
       "   -0.11423885822296143,\n",
       "   0.07951872050762177,\n",
       "   0.08646100759506226,\n",
       "   -0.04021638259291649,\n",
       "   0.06479856371879578,\n",
       "   -0.0052052512764930725,\n",
       "   -0.282501220703125,\n",
       "   -0.3815919756889343,\n",
       "   0.2738404870033264,\n",
       "   0.016155341640114784,\n",
       "   -0.752131462097168,\n",
       "   0.25282731652259827,\n",
       "   0.17298543453216553,\n",
       "   -0.3031436502933502,\n",
       "   -0.05463682860136032,\n",
       "   0.0665103867650032,\n",
       "   -0.06294231116771698,\n",
       "   -0.04816802963614464,\n",
       "   -0.1764613837003708,\n",
       "   -0.23466064035892487,\n",
       "   -0.013834691606462002,\n",
       "   -0.16199511289596558,\n",
       "   0.09453608095645905,\n",
       "   -0.3007567822933197,\n",
       "   -0.17989671230316162,\n",
       "   -0.19597962498664856,\n",
       "   -0.14192041754722595,\n",
       "   -0.10136286914348602,\n",
       "   0.2656053900718689,\n",
       "   -0.2115486115217209,\n",
       "   0.03069956600666046,\n",
       "   -0.1906120479106903,\n",
       "   0.04466821998357773,\n",
       "   -0.16349685192108154,\n",
       "   -0.008528654463589191,\n",
       "   0.2678465247154236,\n",
       "   -0.07709266245365143,\n",
       "   0.32529929280281067,\n",
       "   -0.053822677582502365,\n",
       "   0.07142584025859833,\n",
       "   0.1828598976135254,\n",
       "   -0.028946049511432648,\n",
       "   0.09669435024261475,\n",
       "   0.11392474919557571,\n",
       "   0.10665470361709595,\n",
       "   -0.04228641837835312,\n",
       "   0.0512424111366272,\n",
       "   -0.08662624657154083,\n",
       "   -0.2465372532606125,\n",
       "   -0.01491442322731018,\n",
       "   0.14906881749629974,\n",
       "   -0.06340399384498596,\n",
       "   -1.8984962701797485,\n",
       "   0.1799721121788025,\n",
       "   0.05148740112781525,\n",
       "   -0.27428245544433594,\n",
       "   -0.0226348415017128,\n",
       "   -0.12399696558713913,\n",
       "   0.014951534569263458,\n",
       "   -0.031997643411159515,\n",
       "   0.3755049705505371,\n",
       "   0.507705569267273,\n",
       "   0.05168402940034866,\n",
       "   0.39391621947288513,\n",
       "   -0.01729033887386322,\n",
       "   -0.3436945080757141,\n",
       "   0.12561118602752686,\n",
       "   0.10647247731685638,\n",
       "   0.5563495755195618,\n",
       "   0.1654423326253891,\n",
       "   -0.43806952238082886,\n",
       "   -0.29366323351860046,\n",
       "   0.13457049429416656,\n",
       "   0.412466824054718,\n",
       "   -0.10533400624990463,\n",
       "   0.1439582109451294,\n",
       "   0.03666137903928757,\n",
       "   0.08479961007833481,\n",
       "   -0.3691880702972412,\n",
       "   0.012730244547128677,\n",
       "   -0.16313450038433075,\n",
       "   0.4313427805900574,\n",
       "   -0.15211865305900574,\n",
       "   0.02971784584224224,\n",
       "   -0.1978994607925415,\n",
       "   -0.11578558385372162,\n",
       "   -0.10289064049720764,\n",
       "   -0.11375576257705688,\n",
       "   0.16022783517837524,\n",
       "   0.3630654811859131,\n",
       "   -0.32399043440818787,\n",
       "   -0.06578417122364044,\n",
       "   0.007671643048524857,\n",
       "   0.001818329095840454,\n",
       "   0.13394416868686676,\n",
       "   -0.17648133635520935,\n",
       "   -0.2676340639591217,\n",
       "   0.1519172340631485,\n",
       "   -0.10827618837356567,\n",
       "   -1.3623852729797363,\n",
       "   -0.07092959433794022,\n",
       "   -0.18885061144828796,\n",
       "   -0.16689443588256836,\n",
       "   -0.14233048260211945,\n",
       "   0.14373184740543365,\n",
       "   0.2012765109539032,\n",
       "   -0.05581733584403992,\n",
       "   0.034043073654174805,\n",
       "   -0.1213570237159729,\n",
       "   0.31329813599586487,\n",
       "   0.3236243724822998,\n",
       "   0.3016144037246704,\n",
       "   0.07194540649652481,\n",
       "   -0.1475873738527298,\n",
       "   -0.35291194915771484,\n",
       "   -0.1290404349565506,\n",
       "   0.22641101479530334,\n",
       "   0.12182243168354034,\n",
       "   -0.22765137255191803,\n",
       "   -0.043016061186790466,\n",
       "   0.14558729529380798,\n",
       "   -0.5860742330551147,\n",
       "   -0.1756955236196518,\n",
       "   -0.010849498212337494,\n",
       "   -0.2800426185131073,\n",
       "   -0.3287932872772217,\n",
       "   -0.1969221979379654,\n",
       "   0.047642190009355545,\n",
       "   -0.11027005314826965,\n",
       "   0.08103184401988983,\n",
       "   4.825360298156738,\n",
       "   -0.3243460953235626,\n",
       "   0.10100078582763672,\n",
       "   -0.05974077433347702,\n",
       "   -0.2943132221698761,\n",
       "   -0.12848235666751862,\n",
       "   0.17817433178424835,\n",
       "   0.03799824416637421,\n",
       "   0.7144267559051514,\n",
       "   0.06037362664937973,\n",
       "   -0.2445625364780426,\n",
       "   -0.24537266790866852,\n",
       "   0.04036692529916763,\n",
       "   0.05328492820262909,\n",
       "   -0.44147878885269165,\n",
       "   0.4826922118663788,\n",
       "   0.028515424579381943,\n",
       "   -0.11976723372936249,\n",
       "   0.03353561460971832,\n",
       "   -0.12164297699928284,\n",
       "   -0.0495423898100853,\n",
       "   -0.019580505788326263,\n",
       "   0.2078152894973755,\n",
       "   -0.18230172991752625,\n",
       "   0.12229606509208679,\n",
       "   0.02566385641694069,\n",
       "   0.08802048861980438,\n",
       "   0.09810693562030792,\n",
       "   -0.17344912886619568,\n",
       "   -0.2508700489997864,\n",
       "   -0.30231136083602905,\n",
       "   0.15177962183952332,\n",
       "   0.22892192006111145,\n",
       "   0.24345572292804718,\n",
       "   0.10894381999969482,\n",
       "   -0.11363250017166138,\n",
       "   0.0053324028849601746,\n",
       "   0.016617050394415855,\n",
       "   0.26598069071769714,\n",
       "   0.09045697003602982,\n",
       "   0.3567601442337036,\n",
       "   0.13098546862602234,\n",
       "   0.9233293533325195,\n",
       "   -0.06542515754699707,\n",
       "   0.24041683971881866,\n",
       "   0.3053032457828522,\n",
       "   -0.13723723590373993,\n",
       "   -0.030320368707180023,\n",
       "   0.3635057806968689,\n",
       "   0.1491256207227707,\n",
       "   -0.2227964848279953,\n",
       "   0.22395601868629456,\n",
       "   0.1351228505373001,\n",
       "   -0.15118098258972168,\n",
       "   0.059755727648735046,\n",
       "   0.06742765009403229,\n",
       "   0.020081063732504845,\n",
       "   -0.144367054104805,\n",
       "   -0.13655751943588257,\n",
       "   0.05709066241979599,\n",
       "   -0.0296147670596838,\n",
       "   -0.07548592984676361,\n",
       "   -0.12393014878034592,\n",
       "   -0.17754477262496948,\n",
       "   0.18264053761959076,\n",
       "   -0.25133830308914185,\n",
       "   0.2817426919937134,\n",
       "   -0.168845996260643,\n",
       "   -0.2702639102935791,\n",
       "   -0.17168927192687988,\n",
       "   -0.08322741091251373,\n",
       "   0.010901542380452156,\n",
       "   -0.5502954125404358,\n",
       "   0.26077207922935486,\n",
       "   0.27891847491264343,\n",
       "   -0.014032222330570221,\n",
       "   -0.3823215067386627,\n",
       "   0.1668199598789215,\n",
       "   -0.06155264750123024,\n",
       "   -0.2298756241798401,\n",
       "   -0.11973483115434647,\n",
       "   -0.15649071335792542,\n",
       "   -0.020268039777874947,\n",
       "   -0.3056457042694092,\n",
       "   0.051301147788763046,\n",
       "   0.19486738741397858,\n",
       "   -0.28823667764663696,\n",
       "   -0.31249040365219116,\n",
       "   -0.030300544574856758,\n",
       "   0.2983812093734741,\n",
       "   0.1099202036857605,\n",
       "   -0.2398691177368164,\n",
       "   -0.04719961807131767,\n",
       "   -0.02120727300643921,\n",
       "   0.02098366618156433,\n",
       "   0.349675714969635,\n",
       "   -0.07224900275468826,\n",
       "   -0.11469590663909912,\n",
       "   -0.0013625994324684143,\n",
       "   -0.07586877048015594,\n",
       "   0.3944662809371948,\n",
       "   0.18480414152145386,\n",
       "   0.015379894524812698,\n",
       "   -0.004689311608672142,\n",
       "   0.14852623641490936,\n",
       "   -0.19586938619613647,\n",
       "   -0.11002073436975479,\n",
       "   0.21834540367126465,\n",
       "   -0.03235168755054474,\n",
       "   0.11129436641931534,\n",
       "   0.2836686074733734,\n",
       "   0.10111193358898163,\n",
       "   0.20407013595104218,\n",
       "   -0.19913974404335022,\n",
       "   -0.04581537842750549,\n",
       "   0.1286441534757614,\n",
       "   -0.009162195026874542,\n",
       "   -0.3210268020629883,\n",
       "   -7.354916095733643,\n",
       "   0.2814883291721344,\n",
       "   0.007649082690477371,\n",
       "   0.1433408260345459,\n",
       "   -0.27960604429244995,\n",
       "   -0.09527622163295746,\n",
       "   0.29021400213241577,\n",
       "   0.09965114295482635,\n",
       "   0.17775677144527435,\n",
       "   0.06827162951231003,\n",
       "   -0.29400715231895447,\n",
       "   -0.1337350606918335,\n",
       "   -1.7726619243621826,\n",
       "   0.08600631356239319,\n",
       "   -0.07312250137329102,\n",
       "   0.13071657717227936,\n",
       "   -0.12630951404571533,\n",
       "   -0.8496419787406921,\n",
       "   -0.012155629694461823,\n",
       "   0.34730562567710876,\n",
       "   -0.2447010576725006,\n",
       "   0.1664908230304718,\n",
       "   0.17103545367717743,\n",
       "   0.25135740637779236,\n",
       "   0.2390679270029068,\n",
       "   0.0556868277490139,\n",
       "   -0.028490327298641205,\n",
       "   -0.043944574892520905,\n",
       "   0.0737140104174614,\n",
       "   -0.08893965184688568,\n",
       "   0.024265367537736893,\n",
       "   -0.04721645265817642,\n",
       "   0.3958306610584259,\n",
       "   -0.06935445219278336,\n",
       "   0.0019108275882899761,\n",
       "   -0.27027711272239685,\n",
       "   -0.01083419844508171,\n",
       "   -0.14161309599876404,\n",
       "   0.20045194029808044,\n",
       "   -0.33274394273757935,\n",
       "   -0.09886790812015533,\n",
       "   -0.031036172062158585,\n",
       "   0.1658654808998108,\n",
       "   0.29945430159568787,\n",
       "   -0.36826080083847046,\n",
       "   -0.1847134232521057,\n",
       "   -0.46401044726371765,\n",
       "   -2.1590569019317627,\n",
       "   0.124605193734169,\n",
       "   0.11814296990633011,\n",
       "   0.3193494379520416,\n",
       "   0.13232381641864777,\n",
       "   0.01676204800605774,\n",
       "   -0.06380073726177216,\n",
       "   -0.1537935435771942,\n",
       "   0.183907151222229,\n",
       "   -0.0095058623701334,\n",
       "   0.09738506376743317,\n",
       "   0.25188684463500977,\n",
       "   -0.20578747987747192,\n",
       "   -0.2148228883743286,\n",
       "   -0.016515661031007767,\n",
       "   0.5897840857505798,\n",
       "   0.05243953317403793,\n",
       "   -0.22500230371952057,\n",
       "   0.30145326256752014,\n",
       "   -0.30591702461242676,\n",
       "   -0.05621125549077988,\n",
       "   -0.19069881737232208,\n",
       "   -0.38098040223121643,\n",
       "   -0.12730622291564941,\n",
       "   -0.01314438134431839,\n",
       "   -0.11826542764902115,\n",
       "   0.2174551635980606,\n",
       "   0.24585261940956116,\n",
       "   0.20594319701194763,\n",
       "   -0.13156172633171082,\n",
       "   0.03846170753240585,\n",
       "   -0.21717903017997742,\n",
       "   -0.019957616925239563,\n",
       "   -0.3735862970352173,\n",
       "   -0.13257253170013428,\n",
       "   0.5215299725532532,\n",
       "   0.23171009123325348,\n",
       "   0.2148481160402298,\n",
       "   -0.20708848536014557,\n",
       "   -0.032680220901966095,\n",
       "   0.29884660243988037,\n",
       "   -0.06309200078248978,\n",
       "   -0.04420743137598038,\n",
       "   -0.11364064365625381,\n",
       "   0.005607381463050842,\n",
       "   0.07338646054267883,\n",
       "   -0.07835130393505096,\n",
       "   -0.1860552579164505,\n",
       "   0.19590288400650024,\n",
       "   -0.23281031847000122,\n",
       "   0.07054181396961212,\n",
       "   -0.24805188179016113,\n",
       "   -0.08754757046699524,\n",
       "   -0.18889039754867554,\n",
       "   -7.181242108345032e-05,\n",
       "   0.04395373538136482,\n",
       "   -0.37124672532081604,\n",
       "   0.052450813353061676,\n",
       "   -0.18718110024929047,\n",
       "   2.4119420051574707,\n",
       "   0.04134898632764816,\n",
       "   -0.007798105478286743,\n",
       "   -0.05522530898451805,\n",
       "   -0.032032907009124756,\n",
       "   0.25898635387420654,\n",
       "   -0.0262131430208683,\n",
       "   0.22680597007274628,\n",
       "   1.4288947582244873,\n",
       "   0.09991716593503952,\n",
       "   -0.21881438791751862,\n",
       "   -0.03430666774511337,\n",
       "   -0.011797018349170685,\n",
       "   -0.06754471361637115,\n",
       "   -0.02779706008732319,\n",
       "   0.3310867249965668,\n",
       "   -0.09177816659212112,\n",
       "   0.13208667933940887,\n",
       "   0.06792214512825012,\n",
       "   -0.030220191925764084,\n",
       "   0.2578144073486328,\n",
       "   -0.11922536045312881,\n",
       "   0.20231780409812927,\n",
       "   -0.04075702279806137,\n",
       "   -0.04223091900348663,\n",
       "   -0.06886126846075058,\n",
       "   -0.2922442555427551,\n",
       "   -0.23770852386951447,\n",
       "   0.29817643761634827,\n",
       "   -0.3711102306842804,\n",
       "   0.1391029953956604,\n",
       "   -0.0005590654909610748,\n",
       "   -0.23229853808879852,\n",
       "   0.9053875803947449,\n",
       "   -0.13803409039974213,\n",
       "   0.158310666680336,\n",
       "   0.2712419927120209,\n",
       "   -0.13938620686531067,\n",
       "   0.08495022356510162,\n",
       "   -0.4206405580043793,\n",
       "   -0.06801232695579529,\n",
       "   -0.03080439753830433,\n",
       "   -0.44098350405693054,\n",
       "   0.4754047989845276,\n",
       "   -0.07190141081809998,\n",
       "   0.03937532752752304,\n",
       "   -0.036771781742572784,\n",
       "   -0.20400370657444,\n",
       "   0.15076139569282532,\n",
       "   -0.0846632644534111,\n",
       "   -0.003745436668395996,\n",
       "   -0.23786501586437225,\n",
       "   -0.2641671299934387,\n",
       "   0.26233208179473877,\n",
       "   -0.2391541451215744,\n",
       "   0.07545848190784454,\n",
       "   -0.5911703109741211,\n",
       "   -0.08598636090755463,\n",
       "   0.14061497151851654,\n",
       "   -0.08961525559425354,\n",
       "   -0.049171000719070435,\n",
       "   0.011406809091567993,\n",
       "   -0.01128847524523735,\n",
       "   0.05178020894527435,\n",
       "   -0.043528974056243896,\n",
       "   0.10861779749393463,\n",
       "   0.14760947227478027,\n",
       "   -0.14580801129341125,\n",
       "   -0.11427360028028488,\n",
       "   -0.2650631368160248,\n",
       "   -0.08526571840047836,\n",
       "   0.16805744171142578,\n",
       "   -2.128776788711548,\n",
       "   -0.12182050198316574,\n",
       "   0.13027775287628174,\n",
       "   -0.29155924916267395,\n",
       "   -0.2633644938468933,\n",
       "   -0.2972118854522705,\n",
       "   0.28921279311180115,\n",
       "   0.17075984179973602,\n",
       "   0.17035473883152008,\n",
       "   0.35742107033729553,\n",
       "   -0.0811486542224884,\n",
       "   1.8210911750793457,\n",
       "   -0.10423409938812256,\n",
       "   -0.03336145728826523,\n",
       "   -0.08374723792076111,\n",
       "   -0.017731647938489914,\n",
       "   0.18137226998806,\n",
       "   0.16613134741783142,\n",
       "   -0.12657368183135986,\n",
       "   -0.446445107460022,\n",
       "   -0.09487179666757584,\n",
       "   1.5146446228027344,\n",
       "   0.14150169491767883,\n",
       "   0.3442590534687042,\n",
       "   0.27152761816978455,\n",
       "   -0.20204037427902222,\n",
       "   -0.059525273740291595,\n",
       "   0.10047183185815811,\n",
       "   0.18689695000648499,\n",
       "   -0.00916654895991087,\n",
       "   -0.18844284117221832,\n",
       "   0.2977171540260315,\n",
       "   0.015029393136501312],\n",
       "  [-0.00015268474817276,\n",
       "   0.08012712001800537,\n",
       "   -0.38847222924232483,\n",
       "   0.28976377844810486,\n",
       "   0.10035352408885956,\n",
       "   -0.11797260493040085,\n",
       "   0.26589861512184143,\n",
       "   -0.30863234400749207,\n",
       "   0.11265404522418976,\n",
       "   0.44062694907188416,\n",
       "   0.037027206271886826,\n",
       "   0.4588067829608917,\n",
       "   0.26594170928001404,\n",
       "   0.09318429231643677,\n",
       "   -0.3193677067756653,\n",
       "   -0.011046848259866238,\n",
       "   -0.11461169272661209,\n",
       "   0.060187578201293945,\n",
       "   0.030406109988689423,\n",
       "   0.031369611620903015,\n",
       "   -0.10687682777643204,\n",
       "   -0.0704878717660904,\n",
       "   0.017662402242422104,\n",
       "   0.15770229697227478,\n",
       "   -0.3637045919895172,\n",
       "   -0.06969476491212845,\n",
       "   0.41403478384017944,\n",
       "   0.016984399408102036,\n",
       "   -0.020111873745918274,\n",
       "   0.25843319296836853,\n",
       "   -0.05424096807837486,\n",
       "   0.1188051775097847,\n",
       "   0.3490237891674042,\n",
       "   0.27299168705940247,\n",
       "   -0.12180675566196442,\n",
       "   0.2403692752122879,\n",
       "   0.02343408763408661,\n",
       "   0.6490572094917297,\n",
       "   -0.12018179148435593,\n",
       "   0.050108227878808975,\n",
       "   0.07274462282657623,\n",
       "   0.09980951994657516,\n",
       "   0.02675233595073223,\n",
       "   -0.26077595353126526,\n",
       "   0.26716893911361694,\n",
       "   0.10708983242511749,\n",
       "   -0.1507856845855713,\n",
       "   0.15027447044849396,\n",
       "   0.1682785451412201,\n",
       "   -0.2969585657119751,\n",
       "   0.1583959013223648,\n",
       "   -0.18971025943756104,\n",
       "   -0.11853630095720291,\n",
       "   -0.11579339951276779,\n",
       "   -0.17175503075122833,\n",
       "   -0.15409910678863525,\n",
       "   0.15222035348415375,\n",
       "   0.026889171451330185,\n",
       "   -0.36224889755249023,\n",
       "   0.6280121803283691,\n",
       "   -0.5350468158721924,\n",
       "   0.08478531241416931,\n",
       "   0.189059779047966,\n",
       "   -0.029970891773700714,\n",
       "   -0.18735671043395996,\n",
       "   0.22901201248168945,\n",
       "   -0.07461274415254593,\n",
       "   -0.011661451309919357,\n",
       "   0.12498142570257187,\n",
       "   -0.2056175172328949,\n",
       "   0.4230867326259613,\n",
       "   -0.44935935735702515,\n",
       "   -0.23516714572906494,\n",
       "   0.01431259885430336,\n",
       "   -0.23712769150733948,\n",
       "   -0.3079019784927368,\n",
       "   0.06486805528402328,\n",
       "   -0.12396015971899033,\n",
       "   0.1217426210641861,\n",
       "   -0.2739603817462921,\n",
       "   -0.08850370347499847,\n",
       "   0.3755233585834503,\n",
       "   0.014344096183776855,\n",
       "   -0.2604103982448578,\n",
       "   0.028975337743759155,\n",
       "   0.16294530034065247,\n",
       "   0.23932936787605286,\n",
       "   0.10503639280796051,\n",
       "   -0.275738924741745,\n",
       "   0.266902893781662,\n",
       "   -0.12328870594501495,\n",
       "   -0.22090284526348114,\n",
       "   -0.2563084661960602,\n",
       "   -0.16973049938678741,\n",
       "   -0.2010660022497177,\n",
       "   0.1043202131986618,\n",
       "   -0.18833035230636597,\n",
       "   0.10759980976581573,\n",
       "   0.2190166711807251,\n",
       "   0.22199538350105286,\n",
       "   0.018200388178229332,\n",
       "   0.1314912587404251,\n",
       "   -0.5911523699760437,\n",
       "   -0.03853902965784073,\n",
       "   -0.19770672917366028,\n",
       "   0.16444167494773865,\n",
       "   0.20242060720920563,\n",
       "   -0.169900581240654,\n",
       "   -0.05678637698292732,\n",
       "   0.6623990535736084,\n",
       "   0.27023863792419434,\n",
       "   -0.1240878477692604,\n",
       "   0.15222711861133575,\n",
       "   -0.2540278434753418,\n",
       "   -0.3162496089935303,\n",
       "   0.45455366373062134,\n",
       "   0.1786949336528778,\n",
       "   -0.00036841630935668945,\n",
       "   0.15513572096824646,\n",
       "   0.15763996541500092,\n",
       "   0.13469642400741577,\n",
       "   -0.06942048668861389,\n",
       "   0.16245593130588531,\n",
       "   -0.14433370530605316,\n",
       "   0.15981891751289368,\n",
       "   -0.0763879120349884,\n",
       "   0.13210716843605042,\n",
       "   -0.17590872943401337,\n",
       "   0.3516252338886261,\n",
       "   0.23922239243984222,\n",
       "   -0.3323216736316681,\n",
       "   0.290243536233902,\n",
       "   0.22360803186893463,\n",
       "   -0.39458778500556946,\n",
       "   0.06739261001348495,\n",
       "   0.16413745284080505,\n",
       "   -0.10280174016952515,\n",
       "   0.4057435393333435,\n",
       "   -0.438548743724823,\n",
       "   0.0951891839504242,\n",
       "   0.020760640501976013,\n",
       "   0.07459357380867004,\n",
       "   0.335506409406662,\n",
       "   0.10091523826122284,\n",
       "   -0.10441388934850693,\n",
       "   0.18404659628868103,\n",
       "   -0.6618697047233582,\n",
       "   -0.31592661142349243,\n",
       "   -0.17223939299583435,\n",
       "   0.2372974455356598,\n",
       "   -0.25026223063468933,\n",
       "   -0.01762574538588524,\n",
       "   -0.08399209380149841,\n",
       "   0.101484015583992,\n",
       "   -0.34183767437934875,\n",
       "   -0.10066713392734528,\n",
       "   -0.012531209737062454,\n",
       "   0.2818208932876587,\n",
       "   0.3661154508590698,\n",
       "   0.09728727489709854,\n",
       "   -0.2708795964717865,\n",
       "   -0.04316042363643646,\n",
       "   -0.503059983253479,\n",
       "   0.0758323147892952,\n",
       "   -0.1929440200328827,\n",
       "   0.07756474614143372,\n",
       "   -0.3759895861148834,\n",
       "   0.19966334104537964,\n",
       "   -0.31780320405960083,\n",
       "   0.2505805790424347,\n",
       "   0.038418736308813095,\n",
       "   -0.04967189580202103,\n",
       "   -0.1039886400103569,\n",
       "   0.4728977084159851,\n",
       "   -0.3985269367694855,\n",
       "   -0.3259819746017456,\n",
       "   -0.1102253869175911,\n",
       "   -0.11366555094718933,\n",
       "   -0.3342836797237396,\n",
       "   -0.3177690804004669,\n",
       "   0.14207695424556732,\n",
       "   -0.11092080175876617,\n",
       "   0.13650624454021454,\n",
       "   -0.06089165806770325,\n",
       "   0.004765275865793228,\n",
       "   -0.17864707112312317,\n",
       "   0.1738058626651764,\n",
       "   -0.3304835557937622,\n",
       "   -0.1618756204843521,\n",
       "   0.13877397775650024,\n",
       "   0.26607605814933777,\n",
       "   -0.23538309335708618,\n",
       "   -0.5548014044761658,\n",
       "   0.35084232687950134,\n",
       "   -0.01273420825600624,\n",
       "   -0.0018337815999984741,\n",
       "   -0.007570141926407814,\n",
       "   0.20163080096244812,\n",
       "   0.08954045176506042,\n",
       "   0.15182499587535858,\n",
       "   -0.46814078092575073,\n",
       "   0.15189291536808014,\n",
       "   -0.1099519282579422,\n",
       "   0.0850306898355484,\n",
       "   -0.033146295696496964,\n",
       "   0.038498081266880035,\n",
       "   0.42788711190223694,\n",
       "   -0.09630026668310165,\n",
       "   -0.27916330099105835,\n",
       "   -0.12436113506555557,\n",
       "   -0.29365456104278564,\n",
       "   -0.3437180519104004,\n",
       "   -0.188152015209198,\n",
       "   0.24043616652488708,\n",
       "   -0.04566510021686554,\n",
       "   -0.20900604128837585,\n",
       "   -0.1897972971200943,\n",
       "   -0.2116725891828537,\n",
       "   -0.22652889788150787,\n",
       "   0.2818280756473541,\n",
       "   0.49571549892425537,\n",
       "   -0.1477321982383728,\n",
       "   0.12212111055850983,\n",
       "   0.0011848639696836472,\n",
       "   -0.0054632313549518585,\n",
       "   -0.17044614255428314,\n",
       "   -0.05154659226536751,\n",
       "   0.23919644951820374,\n",
       "   0.10938844084739685,\n",
       "   -0.09988345205783844,\n",
       "   -0.16368813812732697,\n",
       "   0.319743275642395,\n",
       "   0.35496005415916443,\n",
       "   -0.13813504576683044,\n",
       "   0.12288534641265869,\n",
       "   0.04264029115438461,\n",
       "   0.13763174414634705,\n",
       "   -0.14395970106124878,\n",
       "   -0.014802850782871246,\n",
       "   -0.21556255221366882,\n",
       "   -0.10527379810810089,\n",
       "   0.05362590402364731,\n",
       "   0.2700231373310089,\n",
       "   0.3578130602836609,\n",
       "   -0.1615096926689148,\n",
       "   -0.018870744854211807,\n",
       "   -0.08956140279769897,\n",
       "   0.21524286270141602,\n",
       "   0.28005823493003845,\n",
       "   -0.02528999373316765,\n",
       "   0.12048910558223724,\n",
       "   -0.10716895759105682,\n",
       "   -0.060037728399038315,\n",
       "   0.025192953646183014,\n",
       "   0.2860791087150574,\n",
       "   0.23131930828094482,\n",
       "   0.23700499534606934,\n",
       "   0.18062959611415863,\n",
       "   -0.08796071261167526,\n",
       "   -0.6042121052742004,\n",
       "   0.03309302031993866,\n",
       "   0.01919247955083847,\n",
       "   -0.09953463822603226,\n",
       "   0.13113060593605042,\n",
       "   -0.18557558953762054,\n",
       "   0.006712833419442177,\n",
       "   -0.11718909442424774,\n",
       "   -0.05463650822639465,\n",
       "   -0.47971513867378235,\n",
       "   -0.006999855861067772,\n",
       "   0.057807207107543945,\n",
       "   -0.15878590941429138,\n",
       "   -0.09185115993022919,\n",
       "   0.45645856857299805,\n",
       "   -0.3218238353729248,\n",
       "   0.2441239058971405,\n",
       "   -0.012574560940265656,\n",
       "   -0.09229446947574615,\n",
       "   0.1033988818526268,\n",
       "   0.05599648877978325,\n",
       "   0.063684843480587,\n",
       "   0.14838476479053497,\n",
       "   0.06993469595909119,\n",
       "   0.5977005362510681,\n",
       "   0.044499270617961884,\n",
       "   0.25672265887260437,\n",
       "   0.20496825873851776,\n",
       "   0.01716332882642746,\n",
       "   -0.065127894282341,\n",
       "   0.02914923056960106,\n",
       "   -0.033197540789842606,\n",
       "   -0.20121818780899048,\n",
       "   -0.056525129824876785,\n",
       "   0.09818056970834732,\n",
       "   -0.2953506410121918,\n",
       "   -0.11224676668643951,\n",
       "   -0.4462206959724426,\n",
       "   0.4261648952960968,\n",
       "   -0.41575661301612854,\n",
       "   -0.27478885650634766,\n",
       "   0.034169964492321014,\n",
       "   0.05346107482910156,\n",
       "   0.0030583366751670837,\n",
       "   -0.08163134753704071,\n",
       "   -0.27619966864585876,\n",
       "   0.04011720418930054,\n",
       "   0.07726018130779266,\n",
       "   0.34324294328689575,\n",
       "   0.2671560049057007,\n",
       "   0.26913225650787354,\n",
       "   0.3970051407814026,\n",
       "   0.0804559737443924,\n",
       "   0.12817543745040894,\n",
       "   -0.27313005924224854,\n",
       "   -0.07787728309631348,\n",
       "   -0.4939486086368561,\n",
       "   -0.261795312166214,\n",
       "   -0.45987263321876526,\n",
       "   -0.3760283887386322,\n",
       "   -0.28620585799217224,\n",
       "   0.3783940374851227,\n",
       "   0.10633867979049683,\n",
       "   -0.1531217396259308,\n",
       "   0.17783959209918976,\n",
       "   -0.05095313489437103,\n",
       "   -0.04477209225296974,\n",
       "   -0.18031515181064606,\n",
       "   0.387736439704895,\n",
       "   -0.04976797103881836,\n",
       "   0.029178140684962273,\n",
       "   0.05523161590099335,\n",
       "   -0.07215344160795212,\n",
       "   -0.25619375705718994,\n",
       "   0.05175487697124481,\n",
       "   -0.6389032006263733,\n",
       "   -0.12020547688007355,\n",
       "   -0.42154622077941895,\n",
       "   0.28700488805770874,\n",
       "   -0.01677611470222473,\n",
       "   0.4312610626220703,\n",
       "   0.12270764261484146,\n",
       "   -0.05267128720879555,\n",
       "   0.2948865592479706,\n",
       "   -0.13097353279590607,\n",
       "   0.0009658653289079666,\n",
       "   0.12286774814128876,\n",
       "   0.14946061372756958,\n",
       "   0.1669093817472458,\n",
       "   -0.05604271963238716,\n",
       "   0.32926490902900696,\n",
       "   0.016555920243263245,\n",
       "   0.20510390400886536,\n",
       "   -0.1534615457057953,\n",
       "   -0.13668012619018555,\n",
       "   -0.3815597593784332,\n",
       "   0.09475275129079819,\n",
       "   0.09593725949525833,\n",
       "   -0.09257305413484573,\n",
       "   -0.2515582740306854,\n",
       "   0.5767685174942017,\n",
       "   0.18879230320453644,\n",
       "   0.2287161499261856,\n",
       "   0.09541838616132736,\n",
       "   0.2692562937736511,\n",
       "   0.09543095529079437,\n",
       "   0.26321589946746826,\n",
       "   -0.13490718603134155,\n",
       "   0.41186439990997314,\n",
       "   0.16759958863258362,\n",
       "   -0.09127188473939896,\n",
       "   0.6363311409950256,\n",
       "   0.01829792559146881,\n",
       "   0.2899838984012604,\n",
       "   -0.4428275227546692,\n",
       "   -0.18466956913471222,\n",
       "   -0.17261800169944763,\n",
       "   -0.004524890333414078,\n",
       "   0.07285407185554504,\n",
       "   0.1767003983259201,\n",
       "   -0.3789082169532776,\n",
       "   -0.08752289414405823,\n",
       "   0.07013998180627823,\n",
       "   0.6047356128692627,\n",
       "   -0.08484774827957153,\n",
       "   -0.1121625229716301,\n",
       "   -0.42635437846183777,\n",
       "   0.2407151758670807,\n",
       "   -0.2194073498249054,\n",
       "   0.13784575462341309,\n",
       "   0.10833118855953217,\n",
       "   -0.33782583475112915,\n",
       "   -0.04937809705734253,\n",
       "   0.2600056529045105,\n",
       "   -0.00920584425330162,\n",
       "   -0.09147582203149796,\n",
       "   -0.5635213255882263,\n",
       "   0.10002713650465012,\n",
       "   0.394620418548584,\n",
       "   -0.04102349281311035,\n",
       "   -0.4200049936771393,\n",
       "   -0.12763749063014984,\n",
       "   0.27446815371513367,\n",
       "   0.07823348790407181,\n",
       "   0.28290364146232605,\n",
       "   -0.27293041348457336,\n",
       "   -0.5816155076026917,\n",
       "   0.6044449806213379,\n",
       "   0.09110783040523529,\n",
       "   0.4109426438808441,\n",
       "   -0.3419559895992279,\n",
       "   0.10724357515573502,\n",
       "   -0.3546343147754669,\n",
       "   -0.0911509096622467,\n",
       "   -0.020411711186170578,\n",
       "   -0.059699855744838715,\n",
       "   -0.2726476788520813,\n",
       "   0.19546352326869965,\n",
       "   0.03151881322264671,\n",
       "   0.1279887855052948,\n",
       "   -0.003504626452922821,\n",
       "   0.18124255537986755,\n",
       "   0.20460206270217896,\n",
       "   -0.10051044821739197,\n",
       "   -0.4236724376678467,\n",
       "   0.13038799166679382,\n",
       "   -0.14669513702392578,\n",
       "   0.20964230597019196,\n",
       "   -0.06844843178987503,\n",
       "   -0.26373931765556335,\n",
       "   0.02339165285229683,\n",
       "   -0.3223361372947693,\n",
       "   0.025356370955705643,\n",
       "   0.07607362419366837,\n",
       "   0.056849393993616104,\n",
       "   -0.5167761445045471,\n",
       "   0.35459113121032715,\n",
       "   -0.051729507744312286,\n",
       "   0.06382797658443451,\n",
       "   0.14279386401176453,\n",
       "   -0.0033901184797286987,\n",
       "   0.12436976283788681,\n",
       "   -0.15281589329242706,\n",
       "   -0.1600315272808075,\n",
       "   -0.258735716342926,\n",
       "   0.0729074478149414,\n",
       "   -0.3047443628311157,\n",
       "   0.1550658494234085,\n",
       "   0.16690301895141602,\n",
       "   0.20128066837787628,\n",
       "   -0.07304389774799347,\n",
       "   0.09384997189044952,\n",
       "   -0.22091570496559143,\n",
       "   0.0654425248503685,\n",
       "   -0.4045068025588989,\n",
       "   0.3262266516685486,\n",
       "   0.28132936358451843,\n",
       "   -0.31252068281173706,\n",
       "   -0.13841789960861206,\n",
       "   -0.13947242498397827,\n",
       "   -0.3387356102466583,\n",
       "   -0.0736258327960968,\n",
       "   0.1697256714105606,\n",
       "   -0.06862617284059525,\n",
       "   -0.17628581821918488,\n",
       "   -0.21169525384902954,\n",
       "   0.18761390447616577,\n",
       "   0.2377990484237671,\n",
       "   0.3058033585548401,\n",
       "   -0.5417346954345703,\n",
       "   -0.19614125788211823,\n",
       "   0.08289603888988495,\n",
       "   0.13685955107212067,\n",
       "   -0.09357734769582748,\n",
       "   0.43649110198020935,\n",
       "   -0.12487797439098358,\n",
       "   0.28953585028648376,\n",
       "   -0.022411132231354713,\n",
       "   0.008828003890812397,\n",
       "   -0.4945322573184967,\n",
       "   0.1552850753068924,\n",
       "   -0.057119134813547134,\n",
       "   0.1409749537706375,\n",
       "   -0.05202459543943405,\n",
       "   0.09517775475978851,\n",
       "   0.5504317283630371,\n",
       "   -0.41379639506340027,\n",
       "   0.30028367042541504,\n",
       "   0.12164166569709778,\n",
       "   0.5905410647392273,\n",
       "   0.019206929951906204,\n",
       "   0.13608115911483765,\n",
       "   -0.35681360960006714,\n",
       "   0.04468974471092224,\n",
       "   -0.20468348264694214,\n",
       "   0.020638780668377876,\n",
       "   -0.1324816346168518,\n",
       "   -0.38487952947616577,\n",
       "   -0.347711443901062,\n",
       "   0.017020201310515404,\n",
       "   0.07902868837118149,\n",
       "   -0.07676674425601959,\n",
       "   -0.030578888952732086,\n",
       "   -0.16025781631469727,\n",
       "   0.04591904580593109,\n",
       "   -0.42163556814193726,\n",
       "   0.41087597608566284,\n",
       "   0.08581782877445221,\n",
       "   -0.06589163094758987,\n",
       "   0.04895956069231033,\n",
       "   -0.06127230077981949,\n",
       "   0.10846831649541855,\n",
       "   -0.2658383846282959,\n",
       "   0.3738950490951538,\n",
       "   -0.054181646555662155,\n",
       "   0.2312796711921692,\n",
       "   -0.5158987045288086,\n",
       "   0.18100178241729736,\n",
       "   -0.05507626757025719,\n",
       "   -0.048769645392894745,\n",
       "   -0.1360996961593628,\n",
       "   0.20625202357769012,\n",
       "   0.2940974831581116,\n",
       "   -0.2825428247451782,\n",
       "   -0.08931483328342438,\n",
       "   0.12599165737628937,\n",
       "   0.03713374212384224,\n",
       "   -0.0045847706496715546,\n",
       "   0.19247159361839294,\n",
       "   0.21793188154697418,\n",
       "   0.13368777930736542,\n",
       "   -0.29485997557640076,\n",
       "   0.021680690348148346,\n",
       "   -0.3488117456436157,\n",
       "   -0.16656343638896942,\n",
       "   0.7536072134971619,\n",
       "   -0.04590807855129242,\n",
       "   -0.10698412358760834,\n",
       "   0.044154077768325806,\n",
       "   -0.14341257512569427,\n",
       "   0.3505244553089142,\n",
       "   0.3077346384525299,\n",
       "   0.07333298772573471,\n",
       "   -0.28633224964141846,\n",
       "   0.277413934469223,\n",
       "   -0.1514962762594223,\n",
       "   0.011389307677745819,\n",
       "   0.19610415399074554,\n",
       "   0.020384030416607857,\n",
       "   0.15385358035564423,\n",
       "   0.24062395095825195,\n",
       "   0.32627010345458984,\n",
       "   0.1983087658882141,\n",
       "   -0.59698086977005,\n",
       "   0.15447506308555603,\n",
       "   0.24537716805934906,\n",
       "   -0.05640227720141411,\n",
       "   -0.10178381204605103,\n",
       "   -9.280999183654785,\n",
       "   -0.008212162181735039,\n",
       "   -0.22246043384075165,\n",
       "   0.2825571298599243,\n",
       "   -0.13578423857688904,\n",
       "   0.022553838789463043,\n",
       "   -0.05895961821079254,\n",
       "   0.08270803093910217,\n",
       "   -0.3490270972251892,\n",
       "   0.14156943559646606,\n",
       "   0.05112503841519356,\n",
       "   0.0370035395026207,\n",
       "   0.5797574520111084,\n",
       "   -0.4449831545352936,\n",
       "   -0.07033449411392212,\n",
       "   -0.2966596186161041,\n",
       "   0.09307100623846054,\n",
       "   -0.2503166198730469,\n",
       "   0.043513331562280655,\n",
       "   0.05314527079463005,\n",
       "   0.04294025897979736,\n",
       "   0.10169783979654312,\n",
       "   -0.00513206422328949,\n",
       "   0.24255289137363434,\n",
       "   0.40416353940963745,\n",
       "   0.2817429304122925,\n",
       "   -0.3668253421783447,\n",
       "   -0.30723604559898376,\n",
       "   0.3987463414669037,\n",
       "   -0.26636114716529846,\n",
       "   -0.14362160861492157,\n",
       "   0.32195064425468445,\n",
       "   -0.21467699110507965,\n",
       "   -0.012919936329126358,\n",
       "   0.5197741985321045,\n",
       "   -0.050329968333244324,\n",
       "   0.19049429893493652,\n",
       "   -0.013731103390455246,\n",
       "   0.02195511944591999,\n",
       "   -0.381850004196167,\n",
       "   -0.4925757944583893,\n",
       "   -0.08942122012376785,\n",
       "   -0.34651654958724976,\n",
       "   0.2591626048088074,\n",
       "   -0.06432099640369415,\n",
       "   0.15702354907989502,\n",
       "   0.10262415558099747,\n",
       "   0.26395586133003235,\n",
       "   -0.10086296498775482,\n",
       "   -0.051775652915239334,\n",
       "   0.12811246514320374,\n",
       "   0.1597447544336319,\n",
       "   0.3612220287322998,\n",
       "   -0.04579692706465721,\n",
       "   0.15572430193424225,\n",
       "   0.14501705765724182,\n",
       "   0.06949774920940399,\n",
       "   0.42532142996788025,\n",
       "   -0.19818885624408722,\n",
       "   -0.07091161608695984,\n",
       "   0.08120524883270264,\n",
       "   -0.30696532130241394,\n",
       "   0.5813222527503967,\n",
       "   -0.3947482705116272,\n",
       "   -0.2680244445800781,\n",
       "   0.13773827254772186,\n",
       "   0.2123440057039261,\n",
       "   0.0720428079366684,\n",
       "   0.1777617335319519,\n",
       "   -0.3146452307701111,\n",
       "   -0.22207900881767273,\n",
       "   0.38090527057647705,\n",
       "   -0.16518400609493256,\n",
       "   -0.1040218397974968,\n",
       "   -0.14159289002418518,\n",
       "   0.0006812219507992268,\n",
       "   -0.03485635668039322,\n",
       "   0.03225502371788025,\n",
       "   0.12432117015123367,\n",
       "   0.03602923825383186,\n",
       "   -0.09786790609359741,\n",
       "   0.13107216358184814,\n",
       "   0.7350969314575195,\n",
       "   -0.12630516290664673,\n",
       "   -0.04537215456366539,\n",
       "   0.07449895143508911,\n",
       "   0.028340265154838562,\n",
       "   0.44093137979507446,\n",
       "   0.04460572078824043,\n",
       "   0.14423200488090515,\n",
       "   0.007860956713557243,\n",
       "   -0.06530717015266418,\n",
       "   0.08805741369724274,\n",
       "   0.15228474140167236,\n",
       "   0.14748641848564148,\n",
       "   0.33132755756378174,\n",
       "   -0.213608056306839,\n",
       "   0.3747905194759369,\n",
       "   0.03066665306687355,\n",
       "   0.3495236039161682,\n",
       "   -0.00016930513083934784,\n",
       "   0.052258674055337906,\n",
       "   0.1672934740781784,\n",
       "   -0.36438074707984924,\n",
       "   0.2641409635543823,\n",
       "   -0.16352184116840363,\n",
       "   0.20189142227172852,\n",
       "   -0.21488434076309204,\n",
       "   0.2929266095161438,\n",
       "   -0.020521465688943863,\n",
       "   0.0013361796736717224,\n",
       "   0.36190277338027954,\n",
       "   0.2978450655937195,\n",
       "   -0.08858583867549896,\n",
       "   0.2985840439796448,\n",
       "   0.15602782368659973,\n",
       "   -0.05518166720867157,\n",
       "   -0.10142096877098083,\n",
       "   0.4541209042072296,\n",
       "   -0.12852628529071808,\n",
       "   -0.012932628393173218,\n",
       "   -0.4973304271697998,\n",
       "   -0.15248003602027893,\n",
       "   -0.20644305646419525,\n",
       "   0.339447021484375,\n",
       "   0.1366281658411026,\n",
       "   -0.3632943332195282,\n",
       "   0.013815189711749554,\n",
       "   0.06824735552072525,\n",
       "   -0.1540287286043167,\n",
       "   0.316242516040802,\n",
       "   0.41283464431762695,\n",
       "   -0.4770159423351288,\n",
       "   0.45475488901138306,\n",
       "   0.1920887976884842,\n",
       "   -0.19714704155921936,\n",
       "   -0.18194898962974548,\n",
       "   0.3014403283596039,\n",
       "   -0.6145213842391968,\n",
       "   0.202785462141037,\n",
       "   0.01793716847896576,\n",
       "   -0.14070433378219604,\n",
       "   0.18681961297988892,\n",
       "   -0.3969305157661438,\n",
       "   0.5365516543388367,\n",
       "   -0.20806677639484406,\n",
       "   -0.35245877504348755,\n",
       "   -0.25644174218177795,\n",
       "   -0.06417778134346008,\n",
       "   -0.03768645226955414,\n",
       "   -0.09187217056751251,\n",
       "   0.4709036946296692,\n",
       "   0.4521094262599945,\n",
       "   -0.011662431061267853,\n",
       "   0.3087192475795746,\n",
       "   -0.05044427141547203,\n",
       "   0.14938947558403015,\n",
       "   -0.08899812400341034,\n",
       "   -0.08712956309318542,\n",
       "   0.30498209595680237,\n",
       "   -0.28032824397087097,\n",
       "   0.13587383925914764,\n",
       "   0.22019092738628387,\n",
       "   -0.2669579088687897,\n",
       "   -0.03761202469468117,\n",
       "   -0.32007986307144165,\n",
       "   -0.08830094337463379,\n",
       "   -0.10875874757766724,\n",
       "   0.16118450462818146,\n",
       "   -0.24312573671340942,\n",
       "   -0.21227844059467316,\n",
       "   0.14210660755634308,\n",
       "   -0.05903206393122673,\n",
       "   0.22269617021083832,\n",
       "   -0.11649800091981888,\n",
       "   0.3954915702342987,\n",
       "   -0.3693067729473114,\n",
       "   -0.2010613977909088,\n",
       "   -0.0684690922498703,\n",
       "   0.2849240303039551,\n",
       "   0.25130128860473633,\n",
       "   0.1010558009147644,\n",
       "   -0.26584041118621826,\n",
       "   -0.4231833219528198,\n",
       "   0.1640445590019226,\n",
       "   0.26750487089157104,\n",
       "   -0.10294146835803986,\n",
       "   0.22136910259723663,\n",
       "   -0.002261342480778694,\n",
       "   0.18123792111873627,\n",
       "   -0.1193198412656784,\n",
       "   0.0009458661079406738,\n",
       "   0.1542035937309265,\n",
       "   0.21422237157821655,\n",
       "   0.44535988569259644,\n",
       "   0.050382040441036224,\n",
       "   -0.021729405969381332,\n",
       "   -0.07024794816970825,\n",
       "   -0.055935461074113846,\n",
       "   -0.1220831647515297,\n",
       "   0.04233627766370773,\n",
       "   0.16644398868083954,\n",
       "   -0.020931674167513847,\n",
       "   -0.019061829894781113,\n",
       "   0.03800171613693237,\n",
       "   0.11045048385858536,\n",
       "   -0.03967348486185074,\n",
       "   0.6590478420257568,\n",
       "   0.19384007155895233,\n",
       "   0.1713157743215561,\n",
       "   -0.008038967847824097],\n",
       "  [-0.05362853780388832,\n",
       "   -0.20546190440654755,\n",
       "   -0.08306896686553955,\n",
       "   0.05380934476852417,\n",
       "   -0.00755330640822649,\n",
       "   -0.5140759944915771,\n",
       "   0.2195967733860016,\n",
       "   -0.020551513880491257,\n",
       "   0.09421882778406143,\n",
       "   0.11344613879919052,\n",
       "   0.27188828587532043,\n",
       "   0.14522016048431396,\n",
       "   -0.4942183792591095,\n",
       "   0.35814765095710754,\n",
       "   -0.5362508296966553,\n",
       "   -0.14989088475704193,\n",
       "   -0.36375534534454346,\n",
       "   -0.2375941127538681,\n",
       "   -0.01682313159108162,\n",
       "   -0.01863079145550728,\n",
       "   0.08365599811077118,\n",
       "   0.04760143160820007,\n",
       "   -0.16935262084007263,\n",
       "   0.2833702564239502,\n",
       "   0.12189815938472748,\n",
       "   -0.4917047917842865,\n",
       "   0.5224189758300781,\n",
       "   0.08158326894044876,\n",
       "   0.07785212993621826,\n",
       "   0.8906061053276062,\n",
       "   0.10127490013837814,\n",
       "   -0.4020180106163025,\n",
       "   0.17184655368328094,\n",
       "   0.30420660972595215,\n",
       "   -0.3309767544269562,\n",
       "   0.4277556538581848,\n",
       "   -0.28686556220054626,\n",
       "   0.5523383021354675,\n",
       "   -0.2891204357147217,\n",
       "   0.37291020154953003,\n",
       "   -0.040678970515728,\n",
       "   -0.055512890219688416,\n",
       "   -0.13687603175640106,\n",
       "   -0.2999415695667267,\n",
       "   0.24825890362262726,\n",
       "   0.18742318451404572,\n",
       "   0.1012830138206482,\n",
       "   -0.2612585425376892,\n",
       "   -0.32035741209983826,\n",
       "   -0.17901715636253357,\n",
       "   0.0664423257112503,\n",
       "   0.2366742193698883,\n",
       "   -0.07292068749666214,\n",
       "   0.03712804615497589,\n",
       "   0.004287613555788994,\n",
       "   -0.13084182143211365,\n",
       "   0.2671527564525604,\n",
       "   -0.20509986579418182,\n",
       "   -0.33599478006362915,\n",
       "   0.663327157497406,\n",
       "   -0.28976011276245117,\n",
       "   -0.16201458871364594,\n",
       "   -8.545140735805035e-05,\n",
       "   0.044054992496967316,\n",
       "   0.036859214305877686,\n",
       "   -0.01735512726008892,\n",
       "   0.03749500960111618,\n",
       "   -0.2503463625907898,\n",
       "   0.23044916987419128,\n",
       "   -0.19164256751537323,\n",
       "   0.2821589708328247,\n",
       "   0.12021796405315399,\n",
       "   -0.3389100432395935,\n",
       "   -0.15472222864627838,\n",
       "   -0.27563562989234924,\n",
       "   -0.07882480323314667,\n",
       "   0.18077678978443146,\n",
       "   0.061675965785980225,\n",
       "   -0.043881647288799286,\n",
       "   -0.4297829568386078,\n",
       "   -0.12242622673511505,\n",
       "   0.4867519438266754,\n",
       "   -0.0017870105803012848,\n",
       "   0.5329034328460693,\n",
       "   -0.20114365220069885,\n",
       "   0.19622425734996796,\n",
       "   0.1717783659696579,\n",
       "   0.6112409234046936,\n",
       "   -0.2386733591556549,\n",
       "   0.41717615723609924,\n",
       "   -0.3697171211242676,\n",
       "   0.03788106143474579,\n",
       "   0.2343590259552002,\n",
       "   -0.0038295541889965534,\n",
       "   0.0934201255440712,\n",
       "   0.25496208667755127,\n",
       "   -0.3454313278198242,\n",
       "   -0.13669238984584808,\n",
       "   -0.07532775402069092,\n",
       "   0.3132835626602173,\n",
       "   0.1745225340127945,\n",
       "   0.29236435890197754,\n",
       "   -0.4504219591617584,\n",
       "   -0.03181271255016327,\n",
       "   -0.10080809146165848,\n",
       "   0.05315263196825981,\n",
       "   0.28282973170280457,\n",
       "   -0.7349063158035278,\n",
       "   -0.10437411069869995,\n",
       "   0.5179678797721863,\n",
       "   0.2839476466178894,\n",
       "   0.04426068067550659,\n",
       "   0.069682776927948,\n",
       "   -0.25437676906585693,\n",
       "   -0.25282952189445496,\n",
       "   -0.3213362693786621,\n",
       "   0.23128564655780792,\n",
       "   -0.2702184021472931,\n",
       "   -0.11190058290958405,\n",
       "   0.5673362612724304,\n",
       "   0.21817153692245483,\n",
       "   -0.6245008111000061,\n",
       "   -0.1432684063911438,\n",
       "   -0.28146252036094666,\n",
       "   0.019654856994748116,\n",
       "   0.08600956201553345,\n",
       "   -0.012269742786884308,\n",
       "   -0.31468838453292847,\n",
       "   0.08279568701982498,\n",
       "   0.4800523519515991,\n",
       "   -0.2396521121263504,\n",
       "   0.39565375447273254,\n",
       "   -0.00891549326479435,\n",
       "   -0.1587904989719391,\n",
       "   0.15312854945659637,\n",
       "   0.20323234796524048,\n",
       "   -0.32763272523880005,\n",
       "   0.7224743962287903,\n",
       "   -0.5931820869445801,\n",
       "   0.21834895014762878,\n",
       "   -0.08925046026706696,\n",
       "   0.12112452834844589,\n",
       "   0.462675541639328,\n",
       "   0.11335784196853638,\n",
       "   0.453733891248703,\n",
       "   -0.27412742376327515,\n",
       "   -0.17266830801963806,\n",
       "   -0.3696820139884949,\n",
       "   -0.38106656074523926,\n",
       "   0.014234806410968304,\n",
       "   -0.563618004322052,\n",
       "   -0.04017132893204689,\n",
       "   -0.23603136837482452,\n",
       "   0.360065758228302,\n",
       "   -0.5233234763145447,\n",
       "   -0.21323129534721375,\n",
       "   0.028268715366721153,\n",
       "   0.5501099824905396,\n",
       "   0.6047952771186829,\n",
       "   0.28056100010871887,\n",
       "   0.15005294978618622,\n",
       "   -0.21077612042427063,\n",
       "   -0.487657755613327,\n",
       "   0.28796958923339844,\n",
       "   -0.09444668143987656,\n",
       "   0.158705472946167,\n",
       "   -0.35030215978622437,\n",
       "   0.1791495382785797,\n",
       "   -0.17136496305465698,\n",
       "   0.31198030710220337,\n",
       "   0.4843764007091522,\n",
       "   0.1235588863492012,\n",
       "   0.1992393583059311,\n",
       "   0.10623809695243835,\n",
       "   -0.15028762817382812,\n",
       "   -0.49865978956222534,\n",
       "   0.21231704950332642,\n",
       "   -0.4560202956199646,\n",
       "   -0.05247621238231659,\n",
       "   0.002666190266609192,\n",
       "   0.24121512472629547,\n",
       "   -0.38238731026649475,\n",
       "   -0.3125019669532776,\n",
       "   -0.5896746516227722,\n",
       "   -0.2171632945537567,\n",
       "   -0.11549338698387146,\n",
       "   0.21698258817195892,\n",
       "   -0.26323428750038147,\n",
       "   -0.07301435619592667,\n",
       "   -0.06658972054719925,\n",
       "   -0.0639989823102951,\n",
       "   -0.0926826000213623,\n",
       "   -0.5809713006019592,\n",
       "   0.16412903368473053,\n",
       "   -0.08313989639282227,\n",
       "   0.034542687237262726,\n",
       "   -0.12032553553581238,\n",
       "   0.20796066522598267,\n",
       "   -0.028459995985031128,\n",
       "   -0.14473533630371094,\n",
       "   -0.09370674937963486,\n",
       "   -0.3057252764701843,\n",
       "   0.21686796844005585,\n",
       "   0.228092759847641,\n",
       "   0.15786464512348175,\n",
       "   0.30604082345962524,\n",
       "   -0.2894030213356018,\n",
       "   0.263514906167984,\n",
       "   -0.3504877984523773,\n",
       "   -0.04635332524776459,\n",
       "   0.1837247908115387,\n",
       "   -0.13899168372154236,\n",
       "   -0.3103475570678711,\n",
       "   0.22996291518211365,\n",
       "   -0.03741789236664772,\n",
       "   -0.19104580581188202,\n",
       "   -0.16201291978359222,\n",
       "   -0.08577928692102432,\n",
       "   -0.16318324208259583,\n",
       "   -0.08298176527023315,\n",
       "   -0.1488475799560547,\n",
       "   0.07608813792467117,\n",
       "   0.46172893047332764,\n",
       "   -0.26464107632637024,\n",
       "   -0.5016828775405884,\n",
       "   -0.36229848861694336,\n",
       "   0.25897011160850525,\n",
       "   0.2869431674480438,\n",
       "   0.5618934035301208,\n",
       "   -0.0656549483537674,\n",
       "   -0.18244342505931854,\n",
       "   -0.22580932080745697,\n",
       "   0.36543986201286316,\n",
       "   -0.0060098129324615,\n",
       "   0.13818879425525665,\n",
       "   0.4065083861351013,\n",
       "   0.02354702353477478,\n",
       "   -0.026606284081935883,\n",
       "   -0.14492598176002502,\n",
       "   -0.8568020462989807,\n",
       "   0.15001314878463745,\n",
       "   0.1789872944355011,\n",
       "   -0.13954424858093262,\n",
       "   0.49339982867240906,\n",
       "   -0.09181056916713715,\n",
       "   -0.14665913581848145,\n",
       "   -0.2337236851453781,\n",
       "   0.32071277499198914,\n",
       "   0.38918259739875793,\n",
       "   -0.318124383687973,\n",
       "   -0.17354290187358856,\n",
       "   -0.36147984862327576,\n",
       "   0.0012779831886291504,\n",
       "   -0.41590192914009094,\n",
       "   0.5858230590820312,\n",
       "   -0.14128701388835907,\n",
       "   0.5271649360656738,\n",
       "   0.016722191125154495,\n",
       "   0.21780231595039368,\n",
       "   -0.6339843273162842,\n",
       "   0.20317132771015167,\n",
       "   -0.07165416330099106,\n",
       "   0.005371985957026482,\n",
       "   -0.04978414624929428,\n",
       "   -0.3973107933998108,\n",
       "   -0.27036282420158386,\n",
       "   0.09139546751976013,\n",
       "   0.28777050971984863,\n",
       "   -0.4644913673400879,\n",
       "   0.347807914018631,\n",
       "   -0.051306985318660736,\n",
       "   -0.22551849484443665,\n",
       "   -0.06966566294431686,\n",
       "   0.1549808382987976,\n",
       "   -0.19721820950508118,\n",
       "   0.07669782638549805,\n",
       "   -0.060481958091259,\n",
       "   0.0004652068018913269,\n",
       "   -0.4507623612880707,\n",
       "   0.06659835577011108,\n",
       "   0.4352518916130066,\n",
       "   0.4204883575439453,\n",
       "   0.36917322874069214,\n",
       "   0.2501048743724823,\n",
       "   -0.5123942494392395,\n",
       "   0.34246405959129333,\n",
       "   0.33739838004112244,\n",
       "   0.34797295928001404,\n",
       "   -0.21681323647499084,\n",
       "   -0.2651117444038391,\n",
       "   0.1633341759443283,\n",
       "   0.2580568194389343,\n",
       "   -0.7078706622123718,\n",
       "   0.16406825184822083,\n",
       "   -0.11145339906215668,\n",
       "   -0.01019785925745964,\n",
       "   -0.0549352765083313,\n",
       "   0.36400675773620605,\n",
       "   -0.34447604417800903,\n",
       "   -0.25349727272987366,\n",
       "   -0.37674272060394287,\n",
       "   0.0850549265742302,\n",
       "   0.2562457323074341,\n",
       "   0.13639162480831146,\n",
       "   -0.1808418482542038,\n",
       "   -0.8595177531242371,\n",
       "   0.17695659399032593,\n",
       "   -0.1289863884449005,\n",
       "   -0.06778357177972794,\n",
       "   0.5088977813720703,\n",
       "   0.41431093215942383,\n",
       "   0.22648969292640686,\n",
       "   0.19789573550224304,\n",
       "   -0.35914772748947144,\n",
       "   0.14089614152908325,\n",
       "   0.06850379705429077,\n",
       "   -0.38736027479171753,\n",
       "   -0.5178375244140625,\n",
       "   -0.23305077850818634,\n",
       "   -0.1453213095664978,\n",
       "   0.497167706489563,\n",
       "   0.5020581483840942,\n",
       "   -0.5886229276657104,\n",
       "   0.19678464531898499,\n",
       "   -0.06515834480524063,\n",
       "   -0.3492591381072998,\n",
       "   0.013919558376073837,\n",
       "   0.3298571705818176,\n",
       "   0.14004002511501312,\n",
       "   -0.19393061101436615,\n",
       "   0.017172962427139282,\n",
       "   0.1824807971715927,\n",
       "   -0.06340717524290085,\n",
       "   -0.5034670829772949,\n",
       "   -0.3956148028373718,\n",
       "   -0.12905901670455933,\n",
       "   -0.276862233877182,\n",
       "   0.24908871948719025,\n",
       "   -0.11404208838939667,\n",
       "   0.3225291669368744,\n",
       "   0.18218469619750977,\n",
       "   -0.00878952071070671,\n",
       "   0.24241064488887787,\n",
       "   -0.25295892357826233,\n",
       "   0.4048521816730499,\n",
       "   -0.1789896935224533,\n",
       "   -0.051805391907691956,\n",
       "   -0.2244240939617157,\n",
       "   -0.06441394984722137,\n",
       "   0.4386650621891022,\n",
       "   0.008010968565940857,\n",
       "   0.08875039219856262,\n",
       "   -0.023501764982938766,\n",
       "   0.07542292773723602,\n",
       "   0.29027891159057617,\n",
       "   0.2501510977745056,\n",
       "   0.020081693306565285,\n",
       "   0.05902758240699768,\n",
       "   0.09603926539421082,\n",
       "   0.21346552670001984,\n",
       "   0.14963878691196442,\n",
       "   -0.14202740788459778,\n",
       "   -0.5195449590682983,\n",
       "   -0.10715503245592117,\n",
       "   0.03891279548406601,\n",
       "   0.0427703931927681,\n",
       "   0.17188313603401184,\n",
       "   -0.001573219895362854,\n",
       "   0.39097052812576294,\n",
       "   -0.23781253397464752,\n",
       "   0.7424632906913757,\n",
       "   -0.33552953600883484,\n",
       "   0.25690045952796936,\n",
       "   -0.2982502579689026,\n",
       "   -0.06990740448236465,\n",
       "   -0.07204815000295639,\n",
       "   -0.29939892888069153,\n",
       "   0.0061158686876297,\n",
       "   0.02781820110976696,\n",
       "   -0.45587438344955444,\n",
       "   -0.0973544642329216,\n",
       "   0.21095116436481476,\n",
       "   0.686581552028656,\n",
       "   -0.18912482261657715,\n",
       "   0.23749570548534393,\n",
       "   -0.47736236453056335,\n",
       "   0.17059721052646637,\n",
       "   -0.0813262015581131,\n",
       "   0.3654670715332031,\n",
       "   0.13374397158622742,\n",
       "   0.420462965965271,\n",
       "   -0.02323203533887863,\n",
       "   0.3555743396282196,\n",
       "   0.07926097512245178,\n",
       "   -0.2681882083415985,\n",
       "   -0.15400353074073792,\n",
       "   -0.31098833680152893,\n",
       "   0.1597646325826645,\n",
       "   0.07967305183410645,\n",
       "   -0.19272759556770325,\n",
       "   -0.3793777525424957,\n",
       "   0.1990976631641388,\n",
       "   -0.06992696225643158,\n",
       "   -0.19074353575706482,\n",
       "   -0.12638016045093536,\n",
       "   -0.5467634797096252,\n",
       "   0.013998688198626041,\n",
       "   0.18523289263248444,\n",
       "   0.18227823078632355,\n",
       "   0.03600433096289635,\n",
       "   -0.08682483434677124,\n",
       "   -0.3155369162559509,\n",
       "   -0.18816621601581573,\n",
       "   0.5235682129859924,\n",
       "   0.32038938999176025,\n",
       "   -0.39599907398223877,\n",
       "   0.23779186606407166,\n",
       "   -0.12753751873970032,\n",
       "   0.10524678975343704,\n",
       "   0.036330901086330414,\n",
       "   0.09812172502279282,\n",
       "   -0.13765302300453186,\n",
       "   -0.1716836392879486,\n",
       "   -0.1537407785654068,\n",
       "   0.25866395235061646,\n",
       "   -0.16837748885154724,\n",
       "   0.08551982045173645,\n",
       "   -0.3115887939929962,\n",
       "   -0.4288042485713959,\n",
       "   0.507584273815155,\n",
       "   0.0017554759979248047,\n",
       "   -0.30108770728111267,\n",
       "   0.00902608036994934,\n",
       "   0.2797881066799164,\n",
       "   -0.42398858070373535,\n",
       "   0.5470951199531555,\n",
       "   0.3156229853630066,\n",
       "   0.443259060382843,\n",
       "   -0.09581668674945831,\n",
       "   0.16891753673553467,\n",
       "   0.1756751388311386,\n",
       "   -0.25448259711265564,\n",
       "   -0.049544379115104675,\n",
       "   -0.4615735709667206,\n",
       "   -0.23219473659992218,\n",
       "   -0.30620694160461426,\n",
       "   0.27848100662231445,\n",
       "   -0.3672808110713959,\n",
       "   0.28287017345428467,\n",
       "   -0.009174669161438942,\n",
       "   0.2718900740146637,\n",
       "   -0.1991024613380432,\n",
       "   -0.05488849803805351,\n",
       "   -0.3975692391395569,\n",
       "   0.37164780497550964,\n",
       "   -0.12217606604099274,\n",
       "   -0.27177584171295166,\n",
       "   -0.24256271123886108,\n",
       "   -0.1472853720188141,\n",
       "   -0.200848788022995,\n",
       "   0.07986076921224594,\n",
       "   0.2494383156299591,\n",
       "   0.3369887173175812,\n",
       "   0.4729790687561035,\n",
       "   -0.038900651037693024,\n",
       "   0.00037949904799461365,\n",
       "   0.23036205768585205,\n",
       "   0.37665215134620667,\n",
       "   0.08140787482261658,\n",
       "   -0.06075751781463623,\n",
       "   0.5382670760154724,\n",
       "   0.06421864032745361,\n",
       "   0.20213305950164795,\n",
       "   0.41386598348617554,\n",
       "   -0.19301602244377136,\n",
       "   -0.21939034759998322,\n",
       "   -0.16473513841629028,\n",
       "   0.4899190366268158,\n",
       "   -0.5034282207489014,\n",
       "   -0.07250407338142395,\n",
       "   -0.12600840628147125,\n",
       "   -0.10426302254199982,\n",
       "   -0.13484914600849152,\n",
       "   -0.1764868050813675,\n",
       "   0.5261839628219604,\n",
       "   -0.2905014753341675,\n",
       "   0.6408197283744812,\n",
       "   -0.058565087616443634,\n",
       "   0.06075061112642288,\n",
       "   0.30638521909713745,\n",
       "   -0.2671172320842743,\n",
       "   -0.27884525060653687,\n",
       "   0.24276165664196014,\n",
       "   0.11250492930412292,\n",
       "   -0.03693297132849693,\n",
       "   0.02109343744814396,\n",
       "   -0.1577623337507248,\n",
       "   -0.1424432098865509,\n",
       "   -0.304254949092865,\n",
       "   0.12864696979522705,\n",
       "   -0.5330514311790466,\n",
       "   -0.18862178921699524,\n",
       "   0.1428266167640686,\n",
       "   0.031086860224604607,\n",
       "   -0.22845852375030518,\n",
       "   0.8762874007225037,\n",
       "   0.5793570280075073,\n",
       "   -0.4325779974460602,\n",
       "   0.18803822994232178,\n",
       "   -0.21602220833301544,\n",
       "   -0.20937351882457733,\n",
       "   -0.6137093901634216,\n",
       "   -0.02394254505634308,\n",
       "   0.037674594670534134,\n",
       "   -0.12546251714229584,\n",
       "   -0.20988598465919495,\n",
       "   0.3158891797065735,\n",
       "   -0.12539789080619812,\n",
       "   -0.009359452873468399,\n",
       "   0.20354555547237396,\n",
       "   0.658796489238739,\n",
       "   0.24253305792808533,\n",
       "   -0.2114255130290985,\n",
       "   -0.25033852458000183,\n",
       "   -0.044923972338438034,\n",
       "   0.2737962007522583,\n",
       "   0.7606790065765381,\n",
       "   0.09243729710578918,\n",
       "   0.08066044747829437,\n",
       "   0.3366346061229706,\n",
       "   -0.3980688154697418,\n",
       "   0.29194000363349915,\n",
       "   -0.0041014887392520905,\n",
       "   0.07624015212059021,\n",
       "   0.8717881441116333,\n",
       "   -0.049866825342178345,\n",
       "   -0.17173397541046143,\n",
       "   0.096518374979496,\n",
       "   -0.4136779010295868,\n",
       "   0.18127243220806122,\n",
       "   0.050770729780197144,\n",
       "   -0.09530952572822571,\n",
       "   -0.04597412422299385,\n",
       "   0.12423896789550781,\n",
       "   -0.16614371538162231,\n",
       "   0.389090895652771,\n",
       "   0.25796186923980713,\n",
       "   0.28161221742630005,\n",
       "   0.11072968691587448,\n",
       "   0.2618432641029358,\n",
       "   0.0008684918284416199,\n",
       "   -0.06953409314155579,\n",
       "   -0.748127281665802,\n",
       "   0.15289044380187988,\n",
       "   0.22463269531726837,\n",
       "   -0.003638617694377899,\n",
       "   -0.12603388726711273,\n",
       "   -8.99935245513916,\n",
       "   0.32170742750167847,\n",
       "   -0.044646650552749634,\n",
       "   0.16836169362068176,\n",
       "   0.30167967081069946,\n",
       "   -0.03146400675177574,\n",
       "   0.029475271701812744,\n",
       "   -0.6308783888816833,\n",
       "   -0.7517758011817932,\n",
       "   0.1876382827758789,\n",
       "   0.01006334275007248,\n",
       "   0.09255553781986237,\n",
       "   0.41599932312965393,\n",
       "   -0.2805353105068207,\n",
       "   0.1616559773683548,\n",
       "   -0.2639475464820862,\n",
       "   0.22217771410942078,\n",
       "   -0.5653945803642273,\n",
       "   0.3649219870567322,\n",
       "   -0.08855630457401276,\n",
       "   0.06130893528461456,\n",
       "   0.44148769974708557,\n",
       "   -0.22061170637607574,\n",
       "   0.43044981360435486,\n",
       "   0.1245538517832756,\n",
       "   0.3414778709411621,\n",
       "   -0.012087896466255188,\n",
       "   -0.060788851231336594,\n",
       "   0.27137136459350586,\n",
       "   -0.1027599349617958,\n",
       "   -0.13614435493946075,\n",
       "   0.3643709123134613,\n",
       "   0.0029644258320331573,\n",
       "   -0.23015084862709045,\n",
       "   0.47138234972953796,\n",
       "   -0.265562504529953,\n",
       "   0.23824238777160645,\n",
       "   0.09812308847904205,\n",
       "   0.1551550328731537,\n",
       "   -0.5550957918167114,\n",
       "   -0.20755432546138763,\n",
       "   -0.0411839596927166,\n",
       "   -0.17978765070438385,\n",
       "   0.272358775138855,\n",
       "   0.1341431587934494,\n",
       "   0.01953914761543274,\n",
       "   0.3302074074745178,\n",
       "   0.417941153049469,\n",
       "   -0.1428808569908142,\n",
       "   -0.213662788271904,\n",
       "   0.26190048456192017,\n",
       "   0.46507564187049866,\n",
       "   0.08701484650373459,\n",
       "   0.18946821987628937,\n",
       "   0.3481075167655945,\n",
       "   0.23744027316570282,\n",
       "   -0.048566967248916626,\n",
       "   0.6342608332633972,\n",
       "   -0.27459752559661865,\n",
       "   -0.3747440278530121,\n",
       "   0.3956850469112396,\n",
       "   -0.1944480687379837,\n",
       "   0.8582133650779724,\n",
       "   0.016868770122528076,\n",
       "   0.002172756940126419,\n",
       "   0.29701754450798035,\n",
       "   0.07636338472366333,\n",
       "   -0.08508985489606857,\n",
       "   0.20625104010105133,\n",
       "   -0.418404757976532,\n",
       "   -0.4455191493034363,\n",
       "   -0.2013373076915741,\n",
       "   0.10678352415561676,\n",
       "   -0.5432324409484863,\n",
       "   -0.28603461384773254,\n",
       "   -0.2281782329082489,\n",
       "   -0.19513845443725586,\n",
       "   -0.00788998231291771,\n",
       "   -0.020244494080543518,\n",
       "   0.0172015018761158,\n",
       "   -0.4401366710662842,\n",
       "   0.6403215527534485,\n",
       "   0.7450941205024719,\n",
       "   -0.35215139389038086,\n",
       "   -0.1323423683643341,\n",
       "   -0.22960558533668518,\n",
       "   -0.06280328333377838,\n",
       "   0.5212650299072266,\n",
       "   0.230342835187912,\n",
       "   -0.14702101051807404,\n",
       "   -0.08800302445888519,\n",
       "   0.24003997445106506,\n",
       "   0.2587001621723175,\n",
       "   0.24559524655342102,\n",
       "   -0.2131158411502838,\n",
       "   0.4161132872104645,\n",
       "   -0.03294995427131653,\n",
       "   0.01695289835333824,\n",
       "   -0.07967522740364075,\n",
       "   0.007561936974525452,\n",
       "   -0.06319013237953186,\n",
       "   -0.14363007247447968,\n",
       "   -0.6314524412155151,\n",
       "   -0.23479816317558289,\n",
       "   -0.030016813427209854,\n",
       "   -0.13884392380714417,\n",
       "   -0.06113142892718315,\n",
       "   -0.377840131521225,\n",
       "   0.5381002426147461,\n",
       "   0.31611013412475586,\n",
       "   -0.19397971034049988,\n",
       "   0.3004312217235565,\n",
       "   0.5870153307914734,\n",
       "   -0.015773601830005646,\n",
       "   0.27217185497283936,\n",
       "   0.3969937562942505,\n",
       "   -0.2065608948469162,\n",
       "   -0.35150638222694397,\n",
       "   -0.024801114574074745,\n",
       "   -0.07735399901866913,\n",
       "   -0.13374042510986328,\n",
       "   -0.29008427262306213,\n",
       "   -0.3205215334892273,\n",
       "   -0.26359933614730835,\n",
       "   0.2411082237958908,\n",
       "   0.1031961739063263,\n",
       "   -0.22554463148117065,\n",
       "   -0.5072118043899536,\n",
       "   0.017501836642622948,\n",
       "   -0.0285194031894207,\n",
       "   0.6414295434951782,\n",
       "   0.33137810230255127,\n",
       "   -0.33629417419433594,\n",
       "   0.3946353793144226,\n",
       "   0.007589526474475861,\n",
       "   -0.05294451490044594,\n",
       "   -0.27476873993873596,\n",
       "   0.16706405580043793,\n",
       "   -0.45583078265190125,\n",
       "   0.011317426338791847,\n",
       "   -0.2533469796180725,\n",
       "   -0.1674576699733734,\n",
       "   0.832347571849823,\n",
       "   0.06274240463972092,\n",
       "   0.6852894425392151,\n",
       "   0.038306303322315216,\n",
       "   -0.030538175255060196,\n",
       "   -0.21385088562965393,\n",
       "   -0.05297841876745224,\n",
       "   -0.1285756230354309,\n",
       "   -0.07907004654407501,\n",
       "   0.5193736553192139,\n",
       "   0.36844322085380554,\n",
       "   0.08162882179021835,\n",
       "   0.2054123431444168,\n",
       "   -0.46867451071739197,\n",
       "   3.174692392349243e-05,\n",
       "   0.12128914147615433,\n",
       "   -0.6480852365493774,\n",
       "   0.03944787010550499,\n",
       "   0.17222575843334198,\n",
       "   -0.012854881584644318,\n",
       "   0.03632006049156189,\n",
       "   0.011874610558152199,\n",
       "   -0.33758217096328735,\n",
       "   -0.12635093927383423,\n",
       "   -0.11699513345956802,\n",
       "   0.09132561832666397,\n",
       "   0.031856656074523926,\n",
       "   -0.1854023039340973,\n",
       "   -0.17000263929367065,\n",
       "   -0.03367280587553978,\n",
       "   0.11955338716506958,\n",
       "   0.19259686768054962,\n",
       "   0.06709829717874527,\n",
       "   -0.048445459455251694,\n",
       "   0.25349926948547363,\n",
       "   -0.2253168225288391,\n",
       "   0.36905437707901,\n",
       "   0.265166699886322,\n",
       "   0.5691766142845154,\n",
       "   0.28862905502319336,\n",
       "   -0.15276718139648438,\n",
       "   -0.7804217338562012,\n",
       "   -0.2748808264732361,\n",
       "   0.49056100845336914,\n",
       "   0.4545227885246277,\n",
       "   0.054691947996616364,\n",
       "   -0.13031694293022156,\n",
       "   0.3847530484199524,\n",
       "   0.019470714032649994,\n",
       "   0.051459357142448425,\n",
       "   -0.03250511735677719,\n",
       "   0.2289341241121292,\n",
       "   -0.04318103939294815,\n",
       "   0.43716996908187866,\n",
       "   0.08685047924518585,\n",
       "   0.3104165196418762,\n",
       "   -0.3181527256965637,\n",
       "   -0.11609181016683578,\n",
       "   0.19027067720890045,\n",
       "   0.22529113292694092,\n",
       "   -0.022953301668167114,\n",
       "   -0.4304543137550354,\n",
       "   0.2519223093986511,\n",
       "   -0.03491277992725372,\n",
       "   -0.08118083328008652,\n",
       "   0.23829379677772522,\n",
       "   0.382868230342865,\n",
       "   -0.08544889092445374,\n",
       "   0.21378912031650543],\n",
       "  [0.28046557307243347,\n",
       "   -0.19070595502853394,\n",
       "   -0.07161229103803635,\n",
       "   0.13770972192287445,\n",
       "   0.04684164375066757,\n",
       "   -0.18801818788051605,\n",
       "   0.24970123171806335,\n",
       "   -0.01801111549139023,\n",
       "   -0.41722846031188965,\n",
       "   0.2166612446308136,\n",
       "   0.24365916848182678,\n",
       "   0.19053475558757782,\n",
       "   -0.5529881119728088,\n",
       "   0.4117095172405243,\n",
       "   -0.37229952216148376,\n",
       "   0.2776959538459778,\n",
       "   0.28291723132133484,\n",
       "   -0.20407484471797943,\n",
       "   0.04025035351514816,\n",
       "   0.11470463126897812,\n",
       "   0.08038928359746933,\n",
       "   0.05077844113111496,\n",
       "   -0.3093406856060028,\n",
       "   -0.004048919305205345,\n",
       "   -0.0821886658668518,\n",
       "   -0.5025858879089355,\n",
       "   0.2120143324136734,\n",
       "   0.15799164772033691,\n",
       "   0.5911698341369629,\n",
       "   0.43270084261894226,\n",
       "   0.15369084477424622,\n",
       "   -0.274649977684021,\n",
       "   0.47410857677459717,\n",
       "   0.04739490896463394,\n",
       "   -0.25230544805526733,\n",
       "   0.3999541997909546,\n",
       "   -0.08065526932477951,\n",
       "   0.4121236801147461,\n",
       "   -0.13912683725357056,\n",
       "   0.1427614539861679,\n",
       "   0.21543681621551514,\n",
       "   -0.21752893924713135,\n",
       "   -0.15143735706806183,\n",
       "   -0.518211305141449,\n",
       "   0.28600502014160156,\n",
       "   -0.41041380167007446,\n",
       "   0.14714479446411133,\n",
       "   -0.3139074444770813,\n",
       "   -0.20179668068885803,\n",
       "   0.03571139648556709,\n",
       "   -0.005969692021608353,\n",
       "   0.3744497299194336,\n",
       "   -0.6001505255699158,\n",
       "   -0.16806048154830933,\n",
       "   -0.0931527242064476,\n",
       "   -0.3247120976448059,\n",
       "   0.15698376297950745,\n",
       "   -0.0006216280162334442,\n",
       "   -0.42996662855148315,\n",
       "   0.6649530529975891,\n",
       "   -0.3831709027290344,\n",
       "   0.4111851751804352,\n",
       "   -0.018294107168912888,\n",
       "   0.25540608167648315,\n",
       "   0.14531396329402924,\n",
       "   0.3480280041694641,\n",
       "   -0.09926332533359528,\n",
       "   0.005301214754581451,\n",
       "   0.24564114212989807,\n",
       "   -0.47220999002456665,\n",
       "   0.1255161166191101,\n",
       "   0.2110871821641922,\n",
       "   -0.3383447229862213,\n",
       "   -0.04597631096839905,\n",
       "   0.22009718418121338,\n",
       "   -0.40419334173202515,\n",
       "   0.0883544534444809,\n",
       "   -0.13787713646888733,\n",
       "   0.361438512802124,\n",
       "   -0.25795987248420715,\n",
       "   0.1907019317150116,\n",
       "   0.6091408729553223,\n",
       "   -0.04646332561969757,\n",
       "   0.5479348301887512,\n",
       "   0.08290404081344604,\n",
       "   0.033863939344882965,\n",
       "   -0.05032065510749817,\n",
       "   0.38688990473747253,\n",
       "   0.13794593513011932,\n",
       "   0.3616696894168854,\n",
       "   -0.24461831152439117,\n",
       "   -0.15714037418365479,\n",
       "   0.23653210699558258,\n",
       "   -0.215483620762825,\n",
       "   -0.21891888976097107,\n",
       "   0.30615878105163574,\n",
       "   -0.25934457778930664,\n",
       "   -0.3885428309440613,\n",
       "   -0.4661170542240143,\n",
       "   0.2384142130613327,\n",
       "   -0.05474314093589783,\n",
       "   0.08159223198890686,\n",
       "   -0.30595818161964417,\n",
       "   -0.13717257976531982,\n",
       "   -0.036437392234802246,\n",
       "   0.14796017110347748,\n",
       "   -0.20264001190662384,\n",
       "   -0.5799489617347717,\n",
       "   -0.2555873394012451,\n",
       "   0.5630896091461182,\n",
       "   0.02539364993572235,\n",
       "   0.11531241238117218,\n",
       "   -0.16514135897159576,\n",
       "   0.077664315700531,\n",
       "   -0.02265942469239235,\n",
       "   -0.059841688722372055,\n",
       "   -0.1553930640220642,\n",
       "   -0.42498207092285156,\n",
       "   -0.35624662041664124,\n",
       "   0.20757386088371277,\n",
       "   0.17389774322509766,\n",
       "   0.1441880315542221,\n",
       "   -0.07639692723751068,\n",
       "   -0.22204682230949402,\n",
       "   0.10295391082763672,\n",
       "   -0.09547221660614014,\n",
       "   -0.19198819994926453,\n",
       "   -0.3428855240345001,\n",
       "   -0.08687961101531982,\n",
       "   0.4575645327568054,\n",
       "   -0.21950048208236694,\n",
       "   -0.1878894716501236,\n",
       "   0.06836888194084167,\n",
       "   -0.13431087136268616,\n",
       "   0.13328927755355835,\n",
       "   -0.07013242691755295,\n",
       "   -0.31547313928604126,\n",
       "   0.26853251457214355,\n",
       "   -0.9699922204017639,\n",
       "   0.11136515438556671,\n",
       "   -0.2754773199558258,\n",
       "   -0.3714468777179718,\n",
       "   0.4972290098667145,\n",
       "   0.16314572095870972,\n",
       "   0.24034219980239868,\n",
       "   -0.18157833814620972,\n",
       "   -0.2793080806732178,\n",
       "   -0.1883641630411148,\n",
       "   -0.05748417600989342,\n",
       "   0.29946136474609375,\n",
       "   -0.6719815135002136,\n",
       "   -0.04477427154779434,\n",
       "   0.07126793265342712,\n",
       "   0.5851694345474243,\n",
       "   -0.24970456957817078,\n",
       "   -0.12436911463737488,\n",
       "   -0.06188754364848137,\n",
       "   0.4425720274448395,\n",
       "   0.6064181327819824,\n",
       "   0.393094539642334,\n",
       "   0.22473172843456268,\n",
       "   -0.13751597702503204,\n",
       "   -0.3045893907546997,\n",
       "   0.00743488222360611,\n",
       "   0.019765235483646393,\n",
       "   0.04383591189980507,\n",
       "   -0.3631981611251831,\n",
       "   0.2024979442358017,\n",
       "   0.10180553793907166,\n",
       "   0.06803599745035172,\n",
       "   0.33169564604759216,\n",
       "   0.005489988252520561,\n",
       "   -0.02987627685070038,\n",
       "   0.22760553658008575,\n",
       "   -0.057820867747068405,\n",
       "   -0.07238651812076569,\n",
       "   0.039593398571014404,\n",
       "   -0.11480815708637238,\n",
       "   0.13026100397109985,\n",
       "   0.11777311563491821,\n",
       "   0.11285476386547089,\n",
       "   -0.07290921360254288,\n",
       "   -0.22361616790294647,\n",
       "   -0.4147084951400757,\n",
       "   0.17712002992630005,\n",
       "   -0.002991870976984501,\n",
       "   0.41527456045150757,\n",
       "   -0.29806047677993774,\n",
       "   0.6583303213119507,\n",
       "   -0.28136593103408813,\n",
       "   -0.06928715109825134,\n",
       "   -0.13537311553955078,\n",
       "   -0.6296660900115967,\n",
       "   0.20903310179710388,\n",
       "   -0.19579993188381195,\n",
       "   -0.22447043657302856,\n",
       "   0.2507988512516022,\n",
       "   0.041551098227500916,\n",
       "   0.20382028818130493,\n",
       "   0.02075941115617752,\n",
       "   -0.32484549283981323,\n",
       "   0.1260973960161209,\n",
       "   0.05276314914226532,\n",
       "   -0.05727843567728996,\n",
       "   -0.02086399868130684,\n",
       "   0.20032314956188202,\n",
       "   -0.31260091066360474,\n",
       "   0.2171267569065094,\n",
       "   -0.1652587652206421,\n",
       "   -0.29021814465522766,\n",
       "   0.3714309334754944,\n",
       "   -0.48044121265411377,\n",
       "   -0.33746474981307983,\n",
       "   -0.18354356288909912,\n",
       "   -0.24972541630268097,\n",
       "   -0.28815868496894836,\n",
       "   0.34243667125701904,\n",
       "   -0.05960441380739212,\n",
       "   -0.08653940260410309,\n",
       "   -0.021655675023794174,\n",
       "   0.028078272938728333,\n",
       "   -0.07577263563871384,\n",
       "   0.39770054817199707,\n",
       "   -0.4505549669265747,\n",
       "   -0.13066688179969788,\n",
       "   -0.22321335971355438,\n",
       "   0.1535997837781906,\n",
       "   0.24810069799423218,\n",
       "   0.24921539425849915,\n",
       "   -0.17083051800727844,\n",
       "   -0.19797557592391968,\n",
       "   -0.11478938162326813,\n",
       "   0.1790671944618225,\n",
       "   -0.29987943172454834,\n",
       "   -0.012273944914340973,\n",
       "   0.21376824378967285,\n",
       "   0.12966448068618774,\n",
       "   0.12815667688846588,\n",
       "   -0.1688057780265808,\n",
       "   -0.8358519077301025,\n",
       "   0.0334203839302063,\n",
       "   0.12065429985523224,\n",
       "   0.07747845351696014,\n",
       "   0.07218131422996521,\n",
       "   -0.06992574781179428,\n",
       "   0.06738441437482834,\n",
       "   -0.2633572518825531,\n",
       "   0.37408730387687683,\n",
       "   0.3364998400211334,\n",
       "   -0.08286429941654205,\n",
       "   0.18065772950649261,\n",
       "   -0.1677098572254181,\n",
       "   0.10610965639352798,\n",
       "   -0.2790921926498413,\n",
       "   0.5945281982421875,\n",
       "   -0.5345036387443542,\n",
       "   0.002979494631290436,\n",
       "   0.22131097316741943,\n",
       "   -0.005305096507072449,\n",
       "   -0.47401857376098633,\n",
       "   0.10756264626979828,\n",
       "   0.13850945234298706,\n",
       "   0.16551098227500916,\n",
       "   0.10023961961269379,\n",
       "   -0.22550226747989655,\n",
       "   -0.20865754783153534,\n",
       "   0.09439361095428467,\n",
       "   0.3515360355377197,\n",
       "   -0.36058294773101807,\n",
       "   0.6094028949737549,\n",
       "   0.2058490812778473,\n",
       "   -0.39897406101226807,\n",
       "   -0.2662483751773834,\n",
       "   0.416972815990448,\n",
       "   -0.1262470781803131,\n",
       "   0.17250391840934753,\n",
       "   -0.04548453539609909,\n",
       "   -0.35755985975265503,\n",
       "   -0.27623435854911804,\n",
       "   -0.045150019228458405,\n",
       "   0.6578981280326843,\n",
       "   0.20727436244487762,\n",
       "   -0.02116517350077629,\n",
       "   0.15300914645195007,\n",
       "   -0.49907368421554565,\n",
       "   -0.224941223859787,\n",
       "   -0.14443060755729675,\n",
       "   0.3264307677745819,\n",
       "   -0.40263473987579346,\n",
       "   -0.3188908100128174,\n",
       "   -0.1874605268239975,\n",
       "   0.23551779985427856,\n",
       "   -0.23759403824806213,\n",
       "   -0.08646425604820251,\n",
       "   -0.14811861515045166,\n",
       "   -0.04754447191953659,\n",
       "   -0.3354332149028778,\n",
       "   0.3778957426548004,\n",
       "   -0.3946055769920349,\n",
       "   -0.12479211390018463,\n",
       "   -0.10522515326738358,\n",
       "   0.1860560178756714,\n",
       "   0.28010740876197815,\n",
       "   0.30427923798561096,\n",
       "   -0.1993744671344757,\n",
       "   -0.47311311960220337,\n",
       "   0.2818135619163513,\n",
       "   0.4218592047691345,\n",
       "   0.5846203565597534,\n",
       "   0.3771345317363739,\n",
       "   0.5932139158248901,\n",
       "   0.1946154236793518,\n",
       "   -0.35963982343673706,\n",
       "   -0.3920896649360657,\n",
       "   0.150447279214859,\n",
       "   0.06536254286766052,\n",
       "   -0.07896739989519119,\n",
       "   -0.4464736878871918,\n",
       "   -0.3335627317428589,\n",
       "   0.3704761862754822,\n",
       "   0.444586843252182,\n",
       "   0.23213061690330505,\n",
       "   -0.16872306168079376,\n",
       "   0.21552979946136475,\n",
       "   0.12799319624900818,\n",
       "   -0.09816670417785645,\n",
       "   0.06661930680274963,\n",
       "   0.10992497205734253,\n",
       "   -0.18555162847042084,\n",
       "   -0.04074058681726456,\n",
       "   0.04666130617260933,\n",
       "   -0.2762299180030823,\n",
       "   0.06632214784622192,\n",
       "   -0.46238094568252563,\n",
       "   -0.1945381760597229,\n",
       "   0.12559442222118378,\n",
       "   -0.37924349308013916,\n",
       "   0.5554714202880859,\n",
       "   -0.36829674243927,\n",
       "   0.35640984773635864,\n",
       "   0.1262059509754181,\n",
       "   0.13493013381958008,\n",
       "   0.2428823709487915,\n",
       "   -0.3725599944591522,\n",
       "   0.2883260250091553,\n",
       "   0.041585229337215424,\n",
       "   -0.12451446801424026,\n",
       "   0.06824325025081635,\n",
       "   0.0007360391318798065,\n",
       "   -0.03811446204781532,\n",
       "   -0.23182764649391174,\n",
       "   -0.11581234633922577,\n",
       "   -0.20385083556175232,\n",
       "   -0.1381770819425583,\n",
       "   0.2427222579717636,\n",
       "   0.33839643001556396,\n",
       "   0.26184672117233276,\n",
       "   0.056791361421346664,\n",
       "   0.2653266489505768,\n",
       "   0.42104899883270264,\n",
       "   0.25107085704803467,\n",
       "   0.1824571043252945,\n",
       "   -0.24122263491153717,\n",
       "   -0.18292655050754547,\n",
       "   0.1152326762676239,\n",
       "   -0.11466901749372482,\n",
       "   0.09393605589866638,\n",
       "   0.35631614923477173,\n",
       "   -0.17196165025234222,\n",
       "   0.05633072182536125,\n",
       "   0.7249981164932251,\n",
       "   -0.3890085816383362,\n",
       "   0.5116391181945801,\n",
       "   0.006958469748497009,\n",
       "   0.07264641672372818,\n",
       "   0.11919646710157394,\n",
       "   -0.19112750887870789,\n",
       "   0.01564394123852253,\n",
       "   0.1742992103099823,\n",
       "   -0.6224804520606995,\n",
       "   0.21334406733512878,\n",
       "   0.1548261195421219,\n",
       "   0.7324673533439636,\n",
       "   -0.3307558596134186,\n",
       "   0.09260223805904388,\n",
       "   -0.06254221498966217,\n",
       "   -0.2569938898086548,\n",
       "   -0.09689118713140488,\n",
       "   0.06716381758451462,\n",
       "   0.05510401353240013,\n",
       "   0.5388461947441101,\n",
       "   -0.15824812650680542,\n",
       "   0.22277668118476868,\n",
       "   -0.3230806589126587,\n",
       "   -0.16120518743991852,\n",
       "   -0.26929861307144165,\n",
       "   -0.4310671389102936,\n",
       "   0.09347141534090042,\n",
       "   0.01527431607246399,\n",
       "   -0.36194926500320435,\n",
       "   -0.17242760956287384,\n",
       "   -0.09507375210523605,\n",
       "   -0.16932207345962524,\n",
       "   -0.13111354410648346,\n",
       "   -0.3933829665184021,\n",
       "   -0.5529776215553284,\n",
       "   -0.02529267780482769,\n",
       "   0.4064948856830597,\n",
       "   0.09032636880874634,\n",
       "   0.17174065113067627,\n",
       "   0.13950753211975098,\n",
       "   -0.3407818078994751,\n",
       "   -0.08669842034578323,\n",
       "   0.13900059461593628,\n",
       "   0.42649513483047485,\n",
       "   -0.6357009410858154,\n",
       "   0.2565879821777344,\n",
       "   -0.23800028860569,\n",
       "   0.16183239221572876,\n",
       "   0.24388505518436432,\n",
       "   -0.039871953427791595,\n",
       "   0.5732671022415161,\n",
       "   -0.17869316041469574,\n",
       "   -0.30883902311325073,\n",
       "   0.1618811935186386,\n",
       "   0.26147064566612244,\n",
       "   0.08322949707508087,\n",
       "   -0.36345115303993225,\n",
       "   -0.44759127497673035,\n",
       "   -0.003006990998983383,\n",
       "   -0.222470223903656,\n",
       "   0.043032459914684296,\n",
       "   0.18310986459255219,\n",
       "   0.5187607407569885,\n",
       "   -0.6175908446311951,\n",
       "   0.3594653308391571,\n",
       "   0.2451011836528778,\n",
       "   0.23091039061546326,\n",
       "   0.0836283341050148,\n",
       "   0.06908190995454788,\n",
       "   -0.1306106448173523,\n",
       "   -0.24879002571105957,\n",
       "   0.2996731102466583,\n",
       "   -0.41831716895103455,\n",
       "   -0.19879500567913055,\n",
       "   -0.07655678689479828,\n",
       "   0.3115040957927704,\n",
       "   0.05329884588718414,\n",
       "   -0.14320312440395355,\n",
       "   -0.4544604420661926,\n",
       "   0.44997334480285645,\n",
       "   -0.12363800406455994,\n",
       "   -0.11600083857774734,\n",
       "   -0.4902775287628174,\n",
       "   0.4913444221019745,\n",
       "   -0.48790857195854187,\n",
       "   -0.17198659479618073,\n",
       "   -0.0207383930683136,\n",
       "   -0.09735164046287537,\n",
       "   -0.0601009726524353,\n",
       "   -0.20680323243141174,\n",
       "   -0.161549910902977,\n",
       "   0.3298477232456207,\n",
       "   0.37927794456481934,\n",
       "   -0.026785630732774734,\n",
       "   -0.02874608337879181,\n",
       "   0.5901309251785278,\n",
       "   0.11200922727584839,\n",
       "   -0.20354224741458893,\n",
       "   0.0185189601033926,\n",
       "   0.4563841223716736,\n",
       "   0.3196359872817993,\n",
       "   0.27324721217155457,\n",
       "   0.740415632724762,\n",
       "   0.22738970816135406,\n",
       "   -0.22645597159862518,\n",
       "   0.1136474758386612,\n",
       "   0.19661587476730347,\n",
       "   -0.5145407915115356,\n",
       "   -0.025699114426970482,\n",
       "   -0.2851204574108124,\n",
       "   -0.33010122179985046,\n",
       "   -0.32183337211608887,\n",
       "   -0.32021722197532654,\n",
       "   0.3539380133152008,\n",
       "   0.061068203300237656,\n",
       "   0.8587260246276855,\n",
       "   -0.07265783101320267,\n",
       "   -0.2960417866706848,\n",
       "   0.29788219928741455,\n",
       "   0.07465598732233047,\n",
       "   0.13059554994106293,\n",
       "   0.09960107505321503,\n",
       "   0.25301650166511536,\n",
       "   -0.3091275990009308,\n",
       "   -0.07268334180116653,\n",
       "   -0.013109516352415085,\n",
       "   0.02692420780658722,\n",
       "   -0.4313255548477173,\n",
       "   -0.09348937124013901,\n",
       "   -0.08733426034450531,\n",
       "   -0.13201633095741272,\n",
       "   -0.1711329221725464,\n",
       "   0.22280290722846985,\n",
       "   -0.13218143582344055,\n",
       "   0.6625163555145264,\n",
       "   0.4187995195388794,\n",
       "   0.05763881653547287,\n",
       "   -0.09143861383199692,\n",
       "   -0.04850081726908684,\n",
       "   -0.04883643612265587,\n",
       "   -0.3522736430168152,\n",
       "   0.01107344776391983,\n",
       "   0.379830002784729,\n",
       "   -0.2128392457962036,\n",
       "   -0.01624862104654312,\n",
       "   0.322691947221756,\n",
       "   0.13656790554523468,\n",
       "   0.029070626944303513,\n",
       "   0.18946824967861176,\n",
       "   0.17845794558525085,\n",
       "   0.17018690705299377,\n",
       "   -0.6106258630752563,\n",
       "   -0.5298742055892944,\n",
       "   -0.042397793382406235,\n",
       "   -0.1407003253698349,\n",
       "   0.8901746869087219,\n",
       "   0.10680036246776581,\n",
       "   -0.20574602484703064,\n",
       "   0.4974723160266876,\n",
       "   -0.08822905272245407,\n",
       "   0.04627564549446106,\n",
       "   0.35831981897354126,\n",
       "   -0.16294531524181366,\n",
       "   0.6583920121192932,\n",
       "   -0.16156244277954102,\n",
       "   -0.011414147913455963,\n",
       "   -0.16684766113758087,\n",
       "   -0.3905298113822937,\n",
       "   0.15429770946502686,\n",
       "   0.004722483456134796,\n",
       "   0.14956772327423096,\n",
       "   0.1385543793439865,\n",
       "   0.1728665828704834,\n",
       "   -0.29761645197868347,\n",
       "   0.3166877031326294,\n",
       "   0.020758677273988724,\n",
       "   0.3975759744644165,\n",
       "   0.257524698972702,\n",
       "   0.4360785484313965,\n",
       "   0.18686175346374512,\n",
       "   -0.18917438387870789,\n",
       "   -0.9979378581047058,\n",
       "   0.272482693195343,\n",
       "   -0.23106573522090912,\n",
       "   -0.13643886148929596,\n",
       "   -0.11473612487316132,\n",
       "   -9.012744903564453,\n",
       "   0.5318710803985596,\n",
       "   -0.07489795982837677,\n",
       "   0.1264379620552063,\n",
       "   -0.3192713260650635,\n",
       "   0.15777616202831268,\n",
       "   -0.36587727069854736,\n",
       "   -0.2825007140636444,\n",
       "   -0.4359091818332672,\n",
       "   0.23797713220119476,\n",
       "   -0.09316716343164444,\n",
       "   -0.015111744403839111,\n",
       "   0.1810448169708252,\n",
       "   -0.2438150942325592,\n",
       "   0.19390206038951874,\n",
       "   -0.217032328248024,\n",
       "   0.15240105986595154,\n",
       "   -0.22333300113677979,\n",
       "   0.3002901077270508,\n",
       "   -0.20284906029701233,\n",
       "   0.3240406811237335,\n",
       "   0.44774001836776733,\n",
       "   0.17055246233940125,\n",
       "   0.3797561526298523,\n",
       "   -0.1764664202928543,\n",
       "   0.37136518955230713,\n",
       "   -0.17747697234153748,\n",
       "   0.04994628578424454,\n",
       "   0.1088150292634964,\n",
       "   -0.3875490725040436,\n",
       "   -0.38509440422058105,\n",
       "   0.23167464137077332,\n",
       "   -0.09221141040325165,\n",
       "   0.09799408912658691,\n",
       "   0.22706222534179688,\n",
       "   -0.13233289122581482,\n",
       "   0.057732317596673965,\n",
       "   -0.47539064288139343,\n",
       "   0.06873935461044312,\n",
       "   -0.5873450040817261,\n",
       "   -0.48588696122169495,\n",
       "   -0.3095933794975281,\n",
       "   -0.12415549159049988,\n",
       "   0.24487285315990448,\n",
       "   0.45385441184043884,\n",
       "   0.17694661021232605,\n",
       "   -0.040169574320316315,\n",
       "   0.45367780327796936,\n",
       "   0.04983220994472504,\n",
       "   0.26083821058273315,\n",
       "   0.32540735602378845,\n",
       "   0.24420547485351562,\n",
       "   0.3181007206439972,\n",
       "   -0.17165257036685944,\n",
       "   0.07670100033283234,\n",
       "   0.06019161641597748,\n",
       "   0.07876379787921906,\n",
       "   0.3510846495628357,\n",
       "   -0.06287984549999237,\n",
       "   -0.17710916697978973,\n",
       "   0.08651479333639145,\n",
       "   0.20341099798679352,\n",
       "   0.8572869300842285,\n",
       "   0.1027376726269722,\n",
       "   -0.2722927927970886,\n",
       "   0.5857032537460327,\n",
       "   0.05783204734325409,\n",
       "   0.23818229138851166,\n",
       "   0.12721651792526245,\n",
       "   -0.38824740052223206,\n",
       "   -0.3170222043991089,\n",
       "   -0.2569201588630676,\n",
       "   0.12753324210643768,\n",
       "   -0.45262590050697327,\n",
       "   0.3498848080635071,\n",
       "   -0.6254689693450928,\n",
       "   0.08023436367511749,\n",
       "   0.08813552558422089,\n",
       "   -0.17659887671470642,\n",
       "   0.08983932435512543,\n",
       "   -0.458646684885025,\n",
       "   0.6088522672653198,\n",
       "   0.5694794058799744,\n",
       "   -0.032406289130449295,\n",
       "   0.21027304232120514,\n",
       "   -0.1990334689617157,\n",
       "   -0.2199031412601471,\n",
       "   0.33829542994499207,\n",
       "   0.3215998113155365,\n",
       "   -0.21131379902362823,\n",
       "   -0.3494700491428375,\n",
       "   0.3773778975009918,\n",
       "   -0.05590285360813141,\n",
       "   0.26431405544281006,\n",
       "   0.12503062188625336,\n",
       "   0.5956053733825684,\n",
       "   -0.1550147533416748,\n",
       "   0.11176414787769318,\n",
       "   0.0015484914183616638,\n",
       "   0.008603464812040329,\n",
       "   -0.09948667138814926,\n",
       "   0.002257253974676132,\n",
       "   -0.3819851875305176,\n",
       "   -0.2612496316432953,\n",
       "   -0.2898060381412506,\n",
       "   -0.06655296683311462,\n",
       "   -0.07927404344081879,\n",
       "   -0.24030587077140808,\n",
       "   0.24676452577114105,\n",
       "   -0.09016168117523193,\n",
       "   -0.2637746334075928,\n",
       "   0.2654886543750763,\n",
       "   0.17124931514263153,\n",
       "   -0.40307295322418213,\n",
       "   0.014071628451347351,\n",
       "   0.17717093229293823,\n",
       "   -0.1104394719004631,\n",
       "   -0.1111949235200882,\n",
       "   0.25318044424057007,\n",
       "   0.022094756364822388,\n",
       "   -0.17217771708965302,\n",
       "   -0.23512423038482666,\n",
       "   -0.012981332838535309,\n",
       "   -0.26364800333976746,\n",
       "   0.17397886514663696,\n",
       "   0.22605352103710175,\n",
       "   0.16334937512874603,\n",
       "   -0.17419734597206116,\n",
       "   -0.02237538807094097,\n",
       "   0.014007091522216797,\n",
       "   0.21922650933265686,\n",
       "   0.5423415899276733,\n",
       "   -0.43577510118484497,\n",
       "   0.4426122307777405,\n",
       "   -0.12476173043251038,\n",
       "   -0.10771061480045319,\n",
       "   0.052580829709768295,\n",
       "   -0.13532230257987976,\n",
       "   -0.3215433657169342,\n",
       "   0.39030081033706665,\n",
       "   -0.9367334246635437,\n",
       "   0.025651685893535614,\n",
       "   0.39900726079940796,\n",
       "   0.298729807138443,\n",
       "   0.41469070315361023,\n",
       "   0.13804098963737488,\n",
       "   -0.2722960412502289,\n",
       "   0.2098560631275177,\n",
       "   0.12815546989440918,\n",
       "   0.12096139788627625,\n",
       "   -0.4018988311290741,\n",
       "   0.3790362477302551,\n",
       "   0.175209641456604,\n",
       "   0.16110047698020935,\n",
       "   -0.1368798315525055,\n",
       "   -0.4379035532474518,\n",
       "   -0.008802711963653564,\n",
       "   0.16840724647045135,\n",
       "   -0.2878929376602173,\n",
       "   0.07405625283718109,\n",
       "   0.0023681912571191788,\n",
       "   0.11198639869689941,\n",
       "   0.17422397434711456,\n",
       "   -0.2043468952178955,\n",
       "   -0.27330878376960754,\n",
       "   -0.25948506593704224,\n",
       "   -0.12906937301158905,\n",
       "   -0.18747247755527496,\n",
       "   0.17341355979442596,\n",
       "   -0.05481608957052231,\n",
       "   -0.2318316549062729,\n",
       "   -0.0014098756946623325,\n",
       "   -0.1888982206583023,\n",
       "   0.30897802114486694,\n",
       "   0.07258915156126022,\n",
       "   -0.14138062298297882,\n",
       "   -0.28124043345451355,\n",
       "   -0.018570171669125557,\n",
       "   1.08963942527771,\n",
       "   -0.10443143546581268,\n",
       "   0.21196822822093964,\n",
       "   0.23415717482566833,\n",
       "   0.1242343932390213,\n",
       "   -0.47476834058761597,\n",
       "   -0.15677428245544434,\n",
       "   0.25994038581848145,\n",
       "   0.6076133847236633,\n",
       "   0.3590202033519745,\n",
       "   -0.38546833395957947,\n",
       "   0.19434219598770142,\n",
       "   0.516679048538208,\n",
       "   0.02785993367433548,\n",
       "   -0.2610566020011902,\n",
       "   0.4319879710674286,\n",
       "   -0.214145690202713,\n",
       "   0.44343477487564087,\n",
       "   0.0858568549156189,\n",
       "   0.3925279676914215,\n",
       "   -0.2191081941127777,\n",
       "   -0.14944559335708618,\n",
       "   -0.0655263364315033,\n",
       "   0.2725330591201782,\n",
       "   0.21361573040485382,\n",
       "   -0.16300344467163086,\n",
       "   -0.03665315359830856,\n",
       "   0.16648602485656738,\n",
       "   -0.1487254947423935,\n",
       "   0.21673919260501862,\n",
       "   -0.03554842248558998,\n",
       "   0.34119606018066406,\n",
       "   -0.033236026763916016],\n",
       "  [0.20413924753665924,\n",
       "   -0.35685622692108154,\n",
       "   0.12449605762958527,\n",
       "   0.054011017084121704,\n",
       "   0.06376796215772629,\n",
       "   -0.2477383315563202,\n",
       "   0.5897762775421143,\n",
       "   -0.41930821537971497,\n",
       "   -0.34902113676071167,\n",
       "   0.5029516816139221,\n",
       "   -0.16014376282691956,\n",
       "   -0.06938157975673676,\n",
       "   -0.2466188371181488,\n",
       "   0.38921624422073364,\n",
       "   -0.5427658557891846,\n",
       "   0.12053623050451279,\n",
       "   -0.0647013932466507,\n",
       "   0.2772042155265808,\n",
       "   0.16415275633335114,\n",
       "   0.15801778435707092,\n",
       "   -0.5916622281074524,\n",
       "   -0.08048602938652039,\n",
       "   -0.1338081955909729,\n",
       "   0.0869503915309906,\n",
       "   0.07232527434825897,\n",
       "   0.09370211511850357,\n",
       "   -0.04333208501338959,\n",
       "   0.19702087342739105,\n",
       "   0.3243483304977417,\n",
       "   0.8598070740699768,\n",
       "   0.177827849984169,\n",
       "   -0.01336496789008379,\n",
       "   -0.1386844664812088,\n",
       "   0.13171309232711792,\n",
       "   -0.12296131998300552,\n",
       "   0.3470032811164856,\n",
       "   -0.10825317353010178,\n",
       "   0.24402470886707306,\n",
       "   -0.2567266821861267,\n",
       "   0.3196115791797638,\n",
       "   -0.08875977993011475,\n",
       "   -0.18075813353061676,\n",
       "   0.2191535383462906,\n",
       "   -0.4836289882659912,\n",
       "   0.1004229336977005,\n",
       "   -0.2928461730480194,\n",
       "   0.41351404786109924,\n",
       "   -0.006402021273970604,\n",
       "   -0.12705887854099274,\n",
       "   0.3766433000564575,\n",
       "   -0.19644227623939514,\n",
       "   0.3314371109008789,\n",
       "   -0.2768297493457794,\n",
       "   -0.22581450641155243,\n",
       "   0.1539282351732254,\n",
       "   -0.1420401632785797,\n",
       "   0.13676100969314575,\n",
       "   -0.13211335241794586,\n",
       "   -0.22413650155067444,\n",
       "   0.3188759982585907,\n",
       "   -0.5182226300239563,\n",
       "   -0.05533142760396004,\n",
       "   0.2297421097755432,\n",
       "   0.0943818986415863,\n",
       "   -0.02983660250902176,\n",
       "   0.08631903678178787,\n",
       "   -0.12409517168998718,\n",
       "   -0.039738744497299194,\n",
       "   -0.0861000195145607,\n",
       "   0.03943774104118347,\n",
       "   -0.045752182602882385,\n",
       "   0.026875652372837067,\n",
       "   -0.4478415250778198,\n",
       "   -0.3651163578033447,\n",
       "   -0.34233978390693665,\n",
       "   -0.2857629060745239,\n",
       "   -0.4527856707572937,\n",
       "   0.16042780876159668,\n",
       "   -0.03481654077768326,\n",
       "   -0.15802103281021118,\n",
       "   -0.00391358882188797,\n",
       "   0.06728557497262955,\n",
       "   0.20450042188167572,\n",
       "   0.14259517192840576,\n",
       "   -0.02484866976737976,\n",
       "   0.15605172514915466,\n",
       "   0.02093062549829483,\n",
       "   0.3011791706085205,\n",
       "   0.09652339667081833,\n",
       "   0.4251946210861206,\n",
       "   0.08147657662630081,\n",
       "   -0.14234136044979095,\n",
       "   0.17658130824565887,\n",
       "   0.18009184300899506,\n",
       "   -0.38452592492103577,\n",
       "   -0.02515387535095215,\n",
       "   -0.1157185435295105,\n",
       "   0.030664026737213135,\n",
       "   -0.8927046060562134,\n",
       "   0.1313207596540451,\n",
       "   -0.2145010381937027,\n",
       "   0.41365787386894226,\n",
       "   -0.10435233265161514,\n",
       "   0.40742549300193787,\n",
       "   -0.2116011381149292,\n",
       "   0.3413998782634735,\n",
       "   0.21361581981182098,\n",
       "   -0.2016957402229309,\n",
       "   -0.19992487132549286,\n",
       "   0.5722947120666504,\n",
       "   -0.008429117500782013,\n",
       "   0.3151065409183502,\n",
       "   0.38656389713287354,\n",
       "   -0.07625607401132584,\n",
       "   -0.24153457581996918,\n",
       "   -0.01160886138677597,\n",
       "   -0.24754668772220612,\n",
       "   -0.16888132691383362,\n",
       "   -0.13505102694034576,\n",
       "   0.5064759850502014,\n",
       "   0.27662187814712524,\n",
       "   0.07928943634033203,\n",
       "   0.22615939378738403,\n",
       "   -0.4253872036933899,\n",
       "   0.22959186136722565,\n",
       "   0.3486523926258087,\n",
       "   -0.18475130200386047,\n",
       "   -0.02507707104086876,\n",
       "   -0.2305079698562622,\n",
       "   0.16532611846923828,\n",
       "   -0.2043350338935852,\n",
       "   0.2557592988014221,\n",
       "   0.5871425271034241,\n",
       "   0.10397349298000336,\n",
       "   0.03240277245640755,\n",
       "   0.2468976378440857,\n",
       "   -0.4167802929878235,\n",
       "   0.7314234375953674,\n",
       "   -0.25835445523262024,\n",
       "   -0.36937177181243896,\n",
       "   -0.30445536971092224,\n",
       "   0.16840815544128418,\n",
       "   0.3246763348579407,\n",
       "   0.10354000329971313,\n",
       "   0.1147727221250534,\n",
       "   -0.2844041585922241,\n",
       "   -0.39313697814941406,\n",
       "   0.011565469205379486,\n",
       "   -0.041056472808122635,\n",
       "   0.16180816292762756,\n",
       "   -0.0873948186635971,\n",
       "   0.1889624446630478,\n",
       "   -0.34952598810195923,\n",
       "   0.055218230932950974,\n",
       "   -0.17611652612686157,\n",
       "   -0.3269850015640259,\n",
       "   -0.1605120450258255,\n",
       "   0.37652337551116943,\n",
       "   0.5480063557624817,\n",
       "   0.2831481993198395,\n",
       "   0.17984355986118317,\n",
       "   -0.16101036965847015,\n",
       "   -0.5059979557991028,\n",
       "   -0.23875506222248077,\n",
       "   0.25182217359542847,\n",
       "   -0.0341489315032959,\n",
       "   -0.25865644216537476,\n",
       "   0.4247259199619293,\n",
       "   -0.07105747610330582,\n",
       "   0.49106040596961975,\n",
       "   0.05745600536465645,\n",
       "   0.058250196278095245,\n",
       "   0.020327791571617126,\n",
       "   0.1255102902650833,\n",
       "   -0.29210346937179565,\n",
       "   0.3596058487892151,\n",
       "   0.13779900968074799,\n",
       "   -0.1940128058195114,\n",
       "   0.045492999255657196,\n",
       "   0.13345551490783691,\n",
       "   -0.00953606516122818,\n",
       "   0.21172499656677246,\n",
       "   -0.3405124843120575,\n",
       "   -0.6508039236068726,\n",
       "   -0.24383196234703064,\n",
       "   0.11267960071563721,\n",
       "   0.10437874495983124,\n",
       "   -0.32604023814201355,\n",
       "   0.04888741672039032,\n",
       "   -0.427819162607193,\n",
       "   -0.08329924941062927,\n",
       "   -0.023325122892856598,\n",
       "   -0.030785193666815758,\n",
       "   0.04294031485915184,\n",
       "   -0.4753344655036926,\n",
       "   0.21344763040542603,\n",
       "   -0.12406443059444427,\n",
       "   0.3151223063468933,\n",
       "   0.1710931658744812,\n",
       "   -0.04064950346946716,\n",
       "   0.1687089502811432,\n",
       "   -0.19998513162136078,\n",
       "   0.20386134088039398,\n",
       "   -0.007402658928185701,\n",
       "   0.06613171845674515,\n",
       "   0.6220706105232239,\n",
       "   -0.22548867762088776,\n",
       "   0.010340642184019089,\n",
       "   -0.027643714100122452,\n",
       "   -0.3119485080242157,\n",
       "   -0.04112827777862549,\n",
       "   -0.4198856055736542,\n",
       "   -0.36290282011032104,\n",
       "   -0.02689497545361519,\n",
       "   0.04736163467168808,\n",
       "   -0.33966100215911865,\n",
       "   -0.034982189536094666,\n",
       "   0.05400891229510307,\n",
       "   -0.46790242195129395,\n",
       "   0.038920167833566666,\n",
       "   0.16825607419013977,\n",
       "   0.06522736698389053,\n",
       "   0.03315449506044388,\n",
       "   -0.02389083243906498,\n",
       "   -0.8163595795631409,\n",
       "   0.102408766746521,\n",
       "   0.059489984065294266,\n",
       "   0.14238406717777252,\n",
       "   0.23671025037765503,\n",
       "   0.10433425009250641,\n",
       "   -0.34860527515411377,\n",
       "   0.10581012070178986,\n",
       "   0.03212369978427887,\n",
       "   -0.03559424728155136,\n",
       "   0.022767672315239906,\n",
       "   -0.27733510732650757,\n",
       "   0.03033825010061264,\n",
       "   -0.08208553493022919,\n",
       "   -0.4185676872730255,\n",
       "   -0.3435911536216736,\n",
       "   0.46280381083488464,\n",
       "   0.4831521511077881,\n",
       "   0.11337719857692719,\n",
       "   0.028936708346009254,\n",
       "   0.1454240083694458,\n",
       "   0.004311498254537582,\n",
       "   -0.38771066069602966,\n",
       "   0.2999010384082794,\n",
       "   0.3584798276424408,\n",
       "   -0.2841598093509674,\n",
       "   0.015583239495754242,\n",
       "   -0.25904881954193115,\n",
       "   -0.004991762340068817,\n",
       "   -0.2889528274536133,\n",
       "   -0.04581163078546524,\n",
       "   -0.061146222054958344,\n",
       "   0.15516093373298645,\n",
       "   0.14080438017845154,\n",
       "   0.2753957509994507,\n",
       "   -0.5157511234283447,\n",
       "   0.4342174232006073,\n",
       "   0.37702569365501404,\n",
       "   0.015593798831105232,\n",
       "   -0.19382250308990479,\n",
       "   -0.06328605115413666,\n",
       "   -0.15428514778614044,\n",
       "   -0.2398669570684433,\n",
       "   0.42759546637535095,\n",
       "   -0.4105147123336792,\n",
       "   0.20167894661426544,\n",
       "   -0.05898994207382202,\n",
       "   0.08890866488218307,\n",
       "   -0.07808126509189606,\n",
       "   0.25283458828926086,\n",
       "   0.2716301381587982,\n",
       "   0.5206980109214783,\n",
       "   0.1880873590707779,\n",
       "   -0.4510645866394043,\n",
       "   0.0664767250418663,\n",
       "   -0.14869768917560577,\n",
       "   0.03736907243728638,\n",
       "   0.3471037745475769,\n",
       "   0.09061798453330994,\n",
       "   0.6289491653442383,\n",
       "   -0.5643739700317383,\n",
       "   -0.1921769082546234,\n",
       "   0.111136794090271,\n",
       "   0.5242509841918945,\n",
       "   -0.1074259951710701,\n",
       "   -0.2080475091934204,\n",
       "   -0.1420208364725113,\n",
       "   0.051321305334568024,\n",
       "   -0.20669196546077728,\n",
       "   0.24216535687446594,\n",
       "   0.5033590793609619,\n",
       "   0.23848535120487213,\n",
       "   -0.08631026744842529,\n",
       "   0.38121989369392395,\n",
       "   -0.4680095613002777,\n",
       "   -0.08760681748390198,\n",
       "   0.11819572001695633,\n",
       "   0.31410491466522217,\n",
       "   -0.015903618186712265,\n",
       "   -0.09388621896505356,\n",
       "   -0.22886984050273895,\n",
       "   0.14701959490776062,\n",
       "   0.205849289894104,\n",
       "   0.2829587161540985,\n",
       "   -0.2402837872505188,\n",
       "   -0.08887577801942825,\n",
       "   0.3137509822845459,\n",
       "   0.02649734541773796,\n",
       "   -0.22250550985336304,\n",
       "   0.12060517072677612,\n",
       "   0.19975930452346802,\n",
       "   -0.1851871907711029,\n",
       "   -0.16018955409526825,\n",
       "   -0.41504791378974915,\n",
       "   0.26232224702835083,\n",
       "   0.055131033062934875,\n",
       "   0.17069987952709198,\n",
       "   0.12081313878297806,\n",
       "   -0.4014418125152588,\n",
       "   0.5467467904090881,\n",
       "   0.18703465163707733,\n",
       "   0.3563252389431,\n",
       "   -0.023669209331274033,\n",
       "   0.10427062213420868,\n",
       "   -0.1753585785627365,\n",
       "   0.0870451107621193,\n",
       "   0.19808942079544067,\n",
       "   0.2247779816389084,\n",
       "   0.16756218671798706,\n",
       "   -0.5818105340003967,\n",
       "   -0.45196232199668884,\n",
       "   -0.30383604764938354,\n",
       "   0.08357900381088257,\n",
       "   0.644950270652771,\n",
       "   -0.8334813714027405,\n",
       "   -0.18829897046089172,\n",
       "   0.03740139305591583,\n",
       "   0.1292024552822113,\n",
       "   -0.005194477736949921,\n",
       "   -0.42626509070396423,\n",
       "   0.1281365305185318,\n",
       "   0.21312613785266876,\n",
       "   0.11222302913665771,\n",
       "   0.07903359830379486,\n",
       "   0.18491651117801666,\n",
       "   0.24816212058067322,\n",
       "   -0.3195272386074066,\n",
       "   0.08504926413297653,\n",
       "   0.09663716703653336,\n",
       "   -0.11551845073699951,\n",
       "   0.6483937501907349,\n",
       "   0.21044188737869263,\n",
       "   -0.265013188123703,\n",
       "   0.05628208443522453,\n",
       "   0.3742728531360626,\n",
       "   -0.09004148095846176,\n",
       "   -0.1311429888010025,\n",
       "   0.03401859104633331,\n",
       "   -0.0014100372791290283,\n",
       "   -0.6454715132713318,\n",
       "   -0.4906446635723114,\n",
       "   0.03945169597864151,\n",
       "   -0.26639631390571594,\n",
       "   0.16466091573238373,\n",
       "   -0.021851595491170883,\n",
       "   0.04728503152728081,\n",
       "   0.7948013544082642,\n",
       "   -0.17563866078853607,\n",
       "   -0.1363200694322586,\n",
       "   -0.42995360493659973,\n",
       "   -0.16805976629257202,\n",
       "   -0.09924175590276718,\n",
       "   -0.19902746379375458,\n",
       "   -0.23036904633045197,\n",
       "   0.22935213148593903,\n",
       "   -0.2725226879119873,\n",
       "   0.39323702454566956,\n",
       "   0.23371800780296326,\n",
       "   0.34290608763694763,\n",
       "   -0.32414713501930237,\n",
       "   0.198301300406456,\n",
       "   -0.1941903829574585,\n",
       "   0.15269292891025543,\n",
       "   0.14078180491924286,\n",
       "   0.460111528635025,\n",
       "   -0.009904030710458755,\n",
       "   0.5689188241958618,\n",
       "   0.013101901859045029,\n",
       "   0.3602549731731415,\n",
       "   0.3600277602672577,\n",
       "   0.009259054437279701,\n",
       "   0.027540981769561768,\n",
       "   -0.41947534680366516,\n",
       "   0.42504146695137024,\n",
       "   0.24219770729541779,\n",
       "   -0.3798332214355469,\n",
       "   -0.14216278493404388,\n",
       "   -0.17126932740211487,\n",
       "   0.3080233633518219,\n",
       "   -0.39227598905563354,\n",
       "   -0.45599499344825745,\n",
       "   -0.02117747813463211,\n",
       "   -0.3390558362007141,\n",
       "   0.2278280407190323,\n",
       "   0.17317521572113037,\n",
       "   -0.05403823405504227,\n",
       "   0.011919766664505005,\n",
       "   -0.27222809195518494,\n",
       "   -0.16716399788856506,\n",
       "   -0.10849045217037201,\n",
       "   0.1750207543373108,\n",
       "   -0.2726098299026489,\n",
       "   -0.1262592077255249,\n",
       "   -0.289785772562027,\n",
       "   -0.1677132248878479,\n",
       "   0.33397990465164185,\n",
       "   0.24075168371200562,\n",
       "   -0.22731733322143555,\n",
       "   -0.32676148414611816,\n",
       "   0.06564660370349884,\n",
       "   -0.09753464162349701,\n",
       "   0.35585007071495056,\n",
       "   -0.005295421928167343,\n",
       "   -0.2050648331642151,\n",
       "   -0.331449031829834,\n",
       "   0.4007318615913391,\n",
       "   0.40134280920028687,\n",
       "   -0.19581486284732819,\n",
       "   0.05053006857633591,\n",
       "   0.08564907312393188,\n",
       "   -0.1917766034603119,\n",
       "   0.041296251118183136,\n",
       "   -0.017527306452393532,\n",
       "   0.22203409671783447,\n",
       "   0.24723483622074127,\n",
       "   -0.14993663132190704,\n",
       "   0.12263491004705429,\n",
       "   -0.3551817834377289,\n",
       "   -0.19171205163002014,\n",
       "   -0.43682143092155457,\n",
       "   -0.2659628689289093,\n",
       "   0.20161384344100952,\n",
       "   0.3064132034778595,\n",
       "   -0.25068700313568115,\n",
       "   -0.23162631690502167,\n",
       "   0.07353177666664124,\n",
       "   0.3896803855895996,\n",
       "   -0.24919983744621277,\n",
       "   0.1291109174489975,\n",
       "   -0.1538492739200592,\n",
       "   0.18101710081100464,\n",
       "   0.4239819645881653,\n",
       "   -0.3057171702384949,\n",
       "   -0.3133441209793091,\n",
       "   -0.03743361681699753,\n",
       "   -0.206621915102005,\n",
       "   -0.36608052253723145,\n",
       "   0.36366531252861023,\n",
       "   0.20085975527763367,\n",
       "   0.11704513430595398,\n",
       "   0.005278861150145531,\n",
       "   -0.3777421712875366,\n",
       "   0.26421594619750977,\n",
       "   0.5350065231323242,\n",
       "   -0.05167795717716217,\n",
       "   0.0773485079407692,\n",
       "   0.40770474076271057,\n",
       "   0.17531827092170715,\n",
       "   0.5437820553779602,\n",
       "   0.4609120786190033,\n",
       "   0.11989153921604156,\n",
       "   -0.03599365055561066,\n",
       "   -0.21057920157909393,\n",
       "   0.3736521601676941,\n",
       "   -0.6677947044372559,\n",
       "   0.1632331758737564,\n",
       "   0.07735443860292435,\n",
       "   -0.188872292637825,\n",
       "   -0.11793272942304611,\n",
       "   -0.27148425579071045,\n",
       "   0.1911955177783966,\n",
       "   0.1206931620836258,\n",
       "   0.2579183876514435,\n",
       "   -0.0018988550873473287,\n",
       "   0.09523754566907883,\n",
       "   0.10531577467918396,\n",
       "   -0.28823527693748474,\n",
       "   -0.1267394870519638,\n",
       "   0.21780328452587128,\n",
       "   0.3007882535457611,\n",
       "   -0.14732521772384644,\n",
       "   -0.09554007649421692,\n",
       "   0.08822400867938995,\n",
       "   0.2356766015291214,\n",
       "   -0.26523536443710327,\n",
       "   -0.0455775111913681,\n",
       "   -0.2973255217075348,\n",
       "   0.026095867156982422,\n",
       "   -0.007502079010009766,\n",
       "   -0.10835930705070496,\n",
       "   -0.29483628273010254,\n",
       "   0.6132370829582214,\n",
       "   0.07140594720840454,\n",
       "   -0.25085702538490295,\n",
       "   0.07564771175384521,\n",
       "   -0.1265706866979599,\n",
       "   -0.04642266780138016,\n",
       "   -0.39509254693984985,\n",
       "   0.03164491429924965,\n",
       "   -0.46707841753959656,\n",
       "   0.10601825267076492,\n",
       "   -0.21391135454177856,\n",
       "   -0.2483639121055603,\n",
       "   0.09532545506954193,\n",
       "   -0.04805497080087662,\n",
       "   0.519348680973053,\n",
       "   0.508939802646637,\n",
       "   -0.05451720952987671,\n",
       "   0.06415553390979767,\n",
       "   -0.1605851799249649,\n",
       "   0.689529538154602,\n",
       "   -0.30648675560951233,\n",
       "   0.20508486032485962,\n",
       "   0.15800800919532776,\n",
       "   -0.07156446576118469,\n",
       "   -0.03594778850674629,\n",
       "   -0.06488984823226929,\n",
       "   0.1277029812335968,\n",
       "   0.14497193694114685,\n",
       "   -0.03592314571142197,\n",
       "   0.5531872510910034,\n",
       "   0.006970876827836037,\n",
       "   -0.1899244785308838,\n",
       "   -0.05781823396682739,\n",
       "   -0.7433053255081177,\n",
       "   -0.07452128827571869,\n",
       "   0.16472233831882477,\n",
       "   0.008282050490379333,\n",
       "   0.05042392387986183,\n",
       "   0.22148454189300537,\n",
       "   -0.012744445353746414,\n",
       "   -0.03967218101024628,\n",
       "   0.013196336105465889,\n",
       "   0.1627412587404251,\n",
       "   0.37912046909332275,\n",
       "   0.06910298019647598,\n",
       "   0.16025996208190918,\n",
       "   -0.007719404995441437,\n",
       "   -0.1757822036743164,\n",
       "   -0.014501139521598816,\n",
       "   -0.0765954926609993,\n",
       "   0.20302970707416534,\n",
       "   -0.5195914506912231,\n",
       "   -9.118025779724121,\n",
       "   0.28347906470298767,\n",
       "   -0.3070467710494995,\n",
       "   0.4747937023639679,\n",
       "   -0.35564368963241577,\n",
       "   0.13476483523845673,\n",
       "   0.3029319941997528,\n",
       "   -0.04679151251912117,\n",
       "   -0.16644936800003052,\n",
       "   -0.0947628915309906,\n",
       "   0.12792886793613434,\n",
       "   -0.07252533733844757,\n",
       "   -0.025974594056606293,\n",
       "   -0.006741046905517578,\n",
       "   0.17865534126758575,\n",
       "   -0.09961658716201782,\n",
       "   0.19130939245224,\n",
       "   -0.13887114822864532,\n",
       "   -0.2716746926307678,\n",
       "   -0.009677547961473465,\n",
       "   -0.045785050839185715,\n",
       "   -0.33503827452659607,\n",
       "   -0.20709829032421112,\n",
       "   0.830843985080719,\n",
       "   0.030986761674284935,\n",
       "   0.28723597526550293,\n",
       "   0.07659127563238144,\n",
       "   -0.22239455580711365,\n",
       "   -0.11558002978563309,\n",
       "   -0.07059033215045929,\n",
       "   -0.05235820263624191,\n",
       "   0.15311294794082642,\n",
       "   -0.14616042375564575,\n",
       "   0.23150666058063507,\n",
       "   0.06975054740905762,\n",
       "   -0.16529956459999084,\n",
       "   -0.09772984683513641,\n",
       "   0.1797570139169693,\n",
       "   -0.01980646327137947,\n",
       "   -0.3082616329193115,\n",
       "   0.20657844841480255,\n",
       "   0.03182324022054672,\n",
       "   -0.07340377569198608,\n",
       "   0.4481484293937683,\n",
       "   -0.06679198145866394,\n",
       "   0.05887167155742645,\n",
       "   0.09893357008695602,\n",
       "   -0.10483035445213318,\n",
       "   0.014864958822727203,\n",
       "   0.03466956689953804,\n",
       "   0.5009470582008362,\n",
       "   0.2217051386833191,\n",
       "   0.6296207308769226,\n",
       "   0.35476428270339966,\n",
       "   0.10546647012233734,\n",
       "   0.20796847343444824,\n",
       "   0.1645161360502243,\n",
       "   0.4388739764690399,\n",
       "   -0.27578967809677124,\n",
       "   -0.6501743793487549,\n",
       "   0.031138285994529724,\n",
       "   -0.01024787500500679,\n",
       "   0.3951868712902069,\n",
       "   0.05085263401269913,\n",
       "   -0.2547387480735779,\n",
       "   0.2220679074525833,\n",
       "   -0.36587077379226685,\n",
       "   -0.0798877701163292,\n",
       "   -0.0071698809042572975,\n",
       "   -0.25560417771339417,\n",
       "   -0.5023273825645447,\n",
       "   -0.39411285519599915,\n",
       "   0.2662326693534851,\n",
       "   -0.24248789250850677,\n",
       "   0.07840683311223984,\n",
       "   -0.11499195545911789,\n",
       "   -0.04550927132368088,\n",
       "   0.2730453908443451,\n",
       "   0.11502458155155182,\n",
       "   0.33978772163391113,\n",
       "   -0.6479335427284241,\n",
       "   0.22305560111999512,\n",
       "   0.433117151260376,\n",
       "   0.031722959131002426,\n",
       "   0.33025774359703064,\n",
       "   -0.2006945163011551,\n",
       "   -0.2277626395225525,\n",
       "   0.030992738902568817,\n",
       "   -0.055515144020318985,\n",
       "   0.33876657485961914,\n",
       "   -0.344008207321167,\n",
       "   0.18899226188659668,\n",
       "   -0.056187354028224945,\n",
       "   0.22342562675476074,\n",
       "   -0.30415356159210205,\n",
       "   0.3847416639328003,\n",
       "   -0.09401170909404755,\n",
       "   -0.08709017187356949,\n",
       "   -0.012767918407917023,\n",
       "   0.13414393365383148,\n",
       "   0.2561579644680023,\n",
       "   -0.07115170359611511,\n",
       "   0.09551382809877396,\n",
       "   -0.5203419923782349,\n",
       "   -0.48477786779403687,\n",
       "   -0.030244827270507812,\n",
       "   0.022204119712114334,\n",
       "   -0.052354272454977036,\n",
       "   0.24888376891613007,\n",
       "   0.35359013080596924,\n",
       "   -0.18211564421653748,\n",
       "   0.06022220849990845,\n",
       "   0.052520040422677994,\n",
       "   0.0021442463621497154,\n",
       "   0.18971773982048035,\n",
       "   0.28272268176078796,\n",
       "   -0.008365927264094353,\n",
       "   -0.4214588403701782,\n",
       "   -0.00651155412197113,\n",
       "   -0.2378808856010437,\n",
       "   0.0949181541800499,\n",
       "   -0.7180009484291077,\n",
       "   0.11016739904880524,\n",
       "   -0.3432791531085968,\n",
       "   -0.05652056261897087,\n",
       "   0.047039132565259933,\n",
       "   0.006499525159597397,\n",
       "   0.08746582269668579,\n",
       "   0.10795767605304718,\n",
       "   0.03601663559675217,\n",
       "   0.31577378511428833,\n",
       "   0.34196388721466064,\n",
       "   0.5567551255226135,\n",
       "   -0.1516329050064087,\n",
       "   0.129899799823761,\n",
       "   -0.07308994978666306,\n",
       "   0.15937207639217377,\n",
       "   -0.08595293760299683,\n",
       "   -0.199741393327713,\n",
       "   0.16645197570323944,\n",
       "   -0.5543856620788574,\n",
       "   -0.04041234403848648,\n",
       "   0.16396942734718323,\n",
       "   0.19336774945259094,\n",
       "   0.35774093866348267,\n",
       "   -0.1711895614862442,\n",
       "   -0.3495044708251953,\n",
       "   0.3526315689086914,\n",
       "   0.03847856447100639,\n",
       "   -0.16705164313316345,\n",
       "   -0.25239264965057373,\n",
       "   0.11041751503944397,\n",
       "   0.26704803109169006,\n",
       "   6.804615259170532e-05,\n",
       "   0.29747122526168823,\n",
       "   -0.3462948203086853,\n",
       "   -0.1728082299232483,\n",
       "   0.34860366582870483,\n",
       "   -0.044970232993364334,\n",
       "   0.06309528648853302,\n",
       "   -0.3987112045288086,\n",
       "   -0.08995197713375092,\n",
       "   0.0553627572953701,\n",
       "   -0.5251460075378418,\n",
       "   -0.21295469999313354,\n",
       "   -0.07076635956764221,\n",
       "   -0.2876069247722626,\n",
       "   -0.4250192642211914,\n",
       "   0.132330983877182,\n",
       "   0.17970217764377594,\n",
       "   0.3321038782596588,\n",
       "   0.019392838701605797,\n",
       "   0.30764544010162354,\n",
       "   -0.0499083437025547,\n",
       "   0.18875810503959656,\n",
       "   -0.05908416211605072,\n",
       "   -0.06327171623706818,\n",
       "   -0.3218717575073242,\n",
       "   0.31719207763671875,\n",
       "   -0.06633537262678146,\n",
       "   0.09714165329933167,\n",
       "   0.6615356206893921,\n",
       "   0.0792456716299057,\n",
       "   -0.4919229745864868,\n",
       "   -0.3306054472923279,\n",
       "   0.25160688161849976,\n",
       "   0.5288070440292358,\n",
       "   0.1966138333082199,\n",
       "   -0.1017194464802742,\n",
       "   0.0053969696164131165,\n",
       "   0.31913676857948303,\n",
       "   0.22122614085674286,\n",
       "   -0.08569303900003433,\n",
       "   -0.05345280095934868,\n",
       "   0.17569638788700104,\n",
       "   0.1680646538734436,\n",
       "   0.11552353203296661,\n",
       "   0.47911974787712097,\n",
       "   -0.2773284316062927,\n",
       "   -0.20398259162902832,\n",
       "   -0.24475012719631195,\n",
       "   0.4472590386867523,\n",
       "   0.05992000177502632,\n",
       "   0.1466711461544037,\n",
       "   -0.022956622764468193,\n",
       "   -0.01572677493095398,\n",
       "   -0.11855358630418777,\n",
       "   0.056117430329322815,\n",
       "   -0.025354944169521332,\n",
       "   0.16087929904460907,\n",
       "   0.23543056845664978],\n",
       "  [0.43803057074546814,\n",
       "   0.21090011298656464,\n",
       "   -0.05267062410712242,\n",
       "   -0.1262253373861313,\n",
       "   -0.05874520540237427,\n",
       "   0.05492736026644707,\n",
       "   0.32029154896736145,\n",
       "   0.1991441547870636,\n",
       "   -0.17336894571781158,\n",
       "   -0.00048588216304779053,\n",
       "   -0.1617426872253418,\n",
       "   -0.09440390765666962,\n",
       "   -0.19045314192771912,\n",
       "   0.22495293617248535,\n",
       "   -0.3698141574859619,\n",
       "   -0.023018114268779755,\n",
       "   0.31124618649482727,\n",
       "   -0.04847853630781174,\n",
       "   0.05093785375356674,\n",
       "   -0.033178623765707016,\n",
       "   -0.24663209915161133,\n",
       "   -0.18018198013305664,\n",
       "   -0.3680282533168793,\n",
       "   0.19830217957496643,\n",
       "   -0.24989841878414154,\n",
       "   -0.03530952334403992,\n",
       "   0.25435948371887207,\n",
       "   0.2446196973323822,\n",
       "   -0.0007476359605789185,\n",
       "   0.5119879841804504,\n",
       "   -0.12161312997341156,\n",
       "   -0.34501513838768005,\n",
       "   0.2330549657344818,\n",
       "   0.1488608866930008,\n",
       "   0.23916545510292053,\n",
       "   -0.04996958374977112,\n",
       "   -0.07122371345758438,\n",
       "   0.07648081332445145,\n",
       "   -0.20264548063278198,\n",
       "   0.23693016171455383,\n",
       "   0.012701969593763351,\n",
       "   -0.02176370844244957,\n",
       "   0.10153030604124069,\n",
       "   0.06759721040725708,\n",
       "   0.6030600666999817,\n",
       "   -0.29588234424591064,\n",
       "   -0.10587017238140106,\n",
       "   -0.007551966235041618,\n",
       "   -0.186116561293602,\n",
       "   0.19917310774326324,\n",
       "   -0.25504016876220703,\n",
       "   0.07514608651399612,\n",
       "   -0.36273807287216187,\n",
       "   -0.00914516020566225,\n",
       "   0.1708245724439621,\n",
       "   -0.17435839772224426,\n",
       "   0.14619268476963043,\n",
       "   -0.041095513850450516,\n",
       "   -0.2135714590549469,\n",
       "   0.05461300536990166,\n",
       "   -0.2720883786678314,\n",
       "   0.13409672677516937,\n",
       "   0.22321288287639618,\n",
       "   0.0317157506942749,\n",
       "   -0.27917349338531494,\n",
       "   -0.20939360558986664,\n",
       "   -0.0077778976410627365,\n",
       "   0.20247074961662292,\n",
       "   -0.1975713074207306,\n",
       "   -0.04267420247197151,\n",
       "   -0.11777587234973907,\n",
       "   -0.13324317336082458,\n",
       "   -0.03235368803143501,\n",
       "   0.018398895859718323,\n",
       "   0.2175949513912201,\n",
       "   -0.3008435368537903,\n",
       "   0.13714560866355896,\n",
       "   -0.1988222897052765,\n",
       "   0.05096932128071785,\n",
       "   0.09462475031614304,\n",
       "   0.304765909910202,\n",
       "   0.33934929966926575,\n",
       "   -0.019480064511299133,\n",
       "   -0.07364856451749802,\n",
       "   -0.05286876857280731,\n",
       "   -0.1293059140443802,\n",
       "   -0.004092296585440636,\n",
       "   0.13941460847854614,\n",
       "   0.21452558040618896,\n",
       "   0.17876942455768585,\n",
       "   0.20739403367042542,\n",
       "   -0.27849844098091125,\n",
       "   0.0178066436201334,\n",
       "   -0.15293113887310028,\n",
       "   -0.4102533459663391,\n",
       "   -0.3874671459197998,\n",
       "   -0.0400223582983017,\n",
       "   -0.10330242663621902,\n",
       "   -0.08789055049419403,\n",
       "   0.2192867547273636,\n",
       "   0.16402216255664825,\n",
       "   -0.06744562089443207,\n",
       "   -0.03665206581354141,\n",
       "   -0.03238920122385025,\n",
       "   0.06484410166740417,\n",
       "   0.37366124987602234,\n",
       "   0.019250359386205673,\n",
       "   -0.4348296523094177,\n",
       "   -0.27180007100105286,\n",
       "   0.48058050870895386,\n",
       "   0.29454272985458374,\n",
       "   -0.15215325355529785,\n",
       "   0.25192153453826904,\n",
       "   0.07214242219924927,\n",
       "   -0.06667055934667587,\n",
       "   -0.10644862800836563,\n",
       "   -0.2267448753118515,\n",
       "   -0.046892277896404266,\n",
       "   -0.1651047170162201,\n",
       "   0.00654572993516922,\n",
       "   0.42086777091026306,\n",
       "   0.14052635431289673,\n",
       "   0.3808513581752777,\n",
       "   0.04547513276338577,\n",
       "   0.28689050674438477,\n",
       "   0.19731344282627106,\n",
       "   0.004574451129883528,\n",
       "   -0.15157613158226013,\n",
       "   -0.13122960925102234,\n",
       "   0.16861310601234436,\n",
       "   -0.39454489946365356,\n",
       "   -0.08477749675512314,\n",
       "   0.23227335512638092,\n",
       "   0.020448699593544006,\n",
       "   0.039276815950870514,\n",
       "   0.06817281246185303,\n",
       "   -0.517918050289154,\n",
       "   0.33212795853614807,\n",
       "   -0.8057745099067688,\n",
       "   -0.015311509370803833,\n",
       "   -0.05798104405403137,\n",
       "   0.15777933597564697,\n",
       "   0.22208164632320404,\n",
       "   -0.00800255686044693,\n",
       "   -0.00823092833161354,\n",
       "   -0.16372501850128174,\n",
       "   -0.4570912718772888,\n",
       "   -0.26431185007095337,\n",
       "   0.30875256657600403,\n",
       "   0.1878865361213684,\n",
       "   -0.4048817455768585,\n",
       "   -0.06350304186344147,\n",
       "   -0.03999927639961243,\n",
       "   0.1918586790561676,\n",
       "   -0.039502158761024475,\n",
       "   0.031954266130924225,\n",
       "   -0.08923669159412384,\n",
       "   0.16681423783302307,\n",
       "   0.3545747995376587,\n",
       "   0.18125857412815094,\n",
       "   -0.1423753947019577,\n",
       "   -0.43860113620758057,\n",
       "   0.027753926813602448,\n",
       "   -0.0638742744922638,\n",
       "   0.056450605392456055,\n",
       "   0.3692861795425415,\n",
       "   -0.27953436970710754,\n",
       "   0.06912171840667725,\n",
       "   0.1424749344587326,\n",
       "   0.1947825402021408,\n",
       "   0.30899378657341003,\n",
       "   -0.051172710955142975,\n",
       "   -0.16798630356788635,\n",
       "   0.1893695741891861,\n",
       "   -0.1040785014629364,\n",
       "   0.20873068273067474,\n",
       "   0.024733226746320724,\n",
       "   -0.3882734775543213,\n",
       "   -0.03560194373130798,\n",
       "   0.1583177149295807,\n",
       "   0.1632518619298935,\n",
       "   -0.011504538357257843,\n",
       "   -0.05520843714475632,\n",
       "   -0.36992189288139343,\n",
       "   -0.10951142758131027,\n",
       "   -0.36167198419570923,\n",
       "   0.20348629355430603,\n",
       "   -0.3994663655757904,\n",
       "   -0.00221971794962883,\n",
       "   -0.20102673768997192,\n",
       "   0.03275098651647568,\n",
       "   -0.029845982789993286,\n",
       "   -0.11224512755870819,\n",
       "   0.033744584769010544,\n",
       "   -0.35441190004348755,\n",
       "   -0.03676104173064232,\n",
       "   0.06937777996063232,\n",
       "   0.3444731533527374,\n",
       "   0.25859349966049194,\n",
       "   0.15039141476154327,\n",
       "   -0.3442443609237671,\n",
       "   -0.08351216465234756,\n",
       "   0.17500172555446625,\n",
       "   -0.00775391049683094,\n",
       "   0.11199803650379181,\n",
       "   0.2038855403661728,\n",
       "   0.010104220360517502,\n",
       "   -0.05796503275632858,\n",
       "   -0.0870349109172821,\n",
       "   -0.11421183496713638,\n",
       "   0.2956785559654236,\n",
       "   -0.10118891298770905,\n",
       "   0.09944596886634827,\n",
       "   0.2533281445503235,\n",
       "   -0.1121392622590065,\n",
       "   -0.4262566566467285,\n",
       "   0.19721710681915283,\n",
       "   -0.2510586977005005,\n",
       "   -0.381645143032074,\n",
       "   -0.204304039478302,\n",
       "   0.24328702688217163,\n",
       "   -0.11683943122625351,\n",
       "   0.4173729419708252,\n",
       "   -0.1549942046403885,\n",
       "   -0.3758144676685333,\n",
       "   0.2192123979330063,\n",
       "   0.3188774287700653,\n",
       "   0.40734121203422546,\n",
       "   0.17026261985301971,\n",
       "   -0.0618838332593441,\n",
       "   0.1880161464214325,\n",
       "   0.13301607966423035,\n",
       "   0.08371853083372116,\n",
       "   -0.3115854561328888,\n",
       "   0.15769746899604797,\n",
       "   -0.3944272994995117,\n",
       "   -0.18282127380371094,\n",
       "   0.10773362219333649,\n",
       "   -0.3037923574447632,\n",
       "   -0.17745719850063324,\n",
       "   0.2519146800041199,\n",
       "   0.20117561519145966,\n",
       "   0.061101753264665604,\n",
       "   -0.0221049003303051,\n",
       "   -0.18423785269260406,\n",
       "   -0.01032920554280281,\n",
       "   -0.21459829807281494,\n",
       "   0.45014622807502747,\n",
       "   0.7791001796722412,\n",
       "   0.04754434525966644,\n",
       "   -0.03791992738842964,\n",
       "   -0.4102310240268707,\n",
       "   0.28360652923583984,\n",
       "   -0.303986132144928,\n",
       "   0.12932898104190826,\n",
       "   -0.08981719613075256,\n",
       "   0.2958921492099762,\n",
       "   -0.05062828212976456,\n",
       "   0.10319311171770096,\n",
       "   -0.12104976177215576,\n",
       "   0.23213957250118256,\n",
       "   0.1686306744813919,\n",
       "   0.2162063717842102,\n",
       "   0.2343049943447113,\n",
       "   -0.1485919952392578,\n",
       "   -0.34676656126976013,\n",
       "   -0.004560835659503937,\n",
       "   0.08379525691270828,\n",
       "   -0.35075634717941284,\n",
       "   -0.19479314982891083,\n",
       "   -0.06932724267244339,\n",
       "   -0.11023727804422379,\n",
       "   0.18135809898376465,\n",
       "   0.4403134286403656,\n",
       "   0.3217199146747589,\n",
       "   0.008398076519370079,\n",
       "   0.30616503953933716,\n",
       "   -0.49409961700439453,\n",
       "   0.03551333397626877,\n",
       "   -0.31654539704322815,\n",
       "   0.11276313662528992,\n",
       "   0.0365232415497303,\n",
       "   0.1899537742137909,\n",
       "   0.4397033751010895,\n",
       "   -0.23830077052116394,\n",
       "   -0.20870937407016754,\n",
       "   0.15968328714370728,\n",
       "   0.11831554770469666,\n",
       "   0.04039246588945389,\n",
       "   -0.21522074937820435,\n",
       "   0.028479725122451782,\n",
       "   0.1811409592628479,\n",
       "   -0.24297617375850677,\n",
       "   0.03136242926120758,\n",
       "   0.2955434024333954,\n",
       "   0.16462905704975128,\n",
       "   -0.1199546530842781,\n",
       "   0.09964799135923386,\n",
       "   -0.025733593851327896,\n",
       "   -0.16909325122833252,\n",
       "   0.03782234340906143,\n",
       "   0.005222141742706299,\n",
       "   0.02671799808740616,\n",
       "   -0.05419581010937691,\n",
       "   -0.26980119943618774,\n",
       "   -0.0639931708574295,\n",
       "   0.006289288401603699,\n",
       "   0.35960936546325684,\n",
       "   0.18837012350559235,\n",
       "   0.16775941848754883,\n",
       "   0.0486307330429554,\n",
       "   -0.09324608743190765,\n",
       "   -0.07806337624788284,\n",
       "   -0.055350251495838165,\n",
       "   -0.28791946172714233,\n",
       "   -0.3368679881095886,\n",
       "   -0.429518461227417,\n",
       "   -0.14738117158412933,\n",
       "   -0.14600470662117004,\n",
       "   -0.018825557082891464,\n",
       "   0.29057377576828003,\n",
       "   0.10377664864063263,\n",
       "   -0.4138585925102234,\n",
       "   0.45998767018318176,\n",
       "   0.015808068215847015,\n",
       "   0.4257695972919464,\n",
       "   -0.22173765301704407,\n",
       "   0.25761640071868896,\n",
       "   0.14323261380195618,\n",
       "   -0.16359537839889526,\n",
       "   -0.31659260392189026,\n",
       "   -0.1656171679496765,\n",
       "   0.16619227826595306,\n",
       "   0.04196146875619888,\n",
       "   -0.4118788242340088,\n",
       "   -0.2760026454925537,\n",
       "   0.17035046219825745,\n",
       "   0.6534947156906128,\n",
       "   -0.6824847459793091,\n",
       "   -0.13542474806308746,\n",
       "   0.08721181005239487,\n",
       "   0.27125656604766846,\n",
       "   0.20926250517368317,\n",
       "   -0.1208391860127449,\n",
       "   -0.1576596200466156,\n",
       "   0.032329317182302475,\n",
       "   0.2601531445980072,\n",
       "   0.05075002461671829,\n",
       "   0.08390246331691742,\n",
       "   0.014329099096357822,\n",
       "   -0.21115657687187195,\n",
       "   -0.13296356797218323,\n",
       "   0.02965468168258667,\n",
       "   -0.3067527413368225,\n",
       "   0.35032105445861816,\n",
       "   -0.13680580258369446,\n",
       "   -0.0036112461239099503,\n",
       "   -0.12180423736572266,\n",
       "   0.15057888627052307,\n",
       "   0.11526438593864441,\n",
       "   0.2967047691345215,\n",
       "   -0.15221062302589417,\n",
       "   -0.20553620159626007,\n",
       "   -0.0678924098610878,\n",
       "   -0.11760546267032623,\n",
       "   0.001206614077091217,\n",
       "   0.026815012097358704,\n",
       "   0.6970221996307373,\n",
       "   -0.19296196103096008,\n",
       "   -0.20109008252620697,\n",
       "   0.5711537599563599,\n",
       "   0.37415337562561035,\n",
       "   -0.03553853556513786,\n",
       "   -0.28386014699935913,\n",
       "   0.13142052292823792,\n",
       "   -0.22025078535079956,\n",
       "   -0.061581213027238846,\n",
       "   0.007960723713040352,\n",
       "   -0.2794681489467621,\n",
       "   -0.27277374267578125,\n",
       "   -0.001176394522190094,\n",
       "   0.2478623241186142,\n",
       "   0.2142655998468399,\n",
       "   -0.18941839039325714,\n",
       "   0.05922737717628479,\n",
       "   0.10297951102256775,\n",
       "   -0.0782880187034607,\n",
       "   -0.06953835487365723,\n",
       "   0.07679567486047745,\n",
       "   -0.0024041421711444855,\n",
       "   -0.443233847618103,\n",
       "   0.08208033442497253,\n",
       "   0.36594992876052856,\n",
       "   -0.053423408418893814,\n",
       "   -0.13028542697429657,\n",
       "   -0.05336296558380127,\n",
       "   -0.40387290716171265,\n",
       "   -0.05483848601579666,\n",
       "   0.1883615106344223,\n",
       "   -0.3929038345813751,\n",
       "   0.05019306018948555,\n",
       "   0.053416166454553604,\n",
       "   0.02955443412065506,\n",
       "   -0.1148829311132431,\n",
       "   -0.2406919002532959,\n",
       "   -0.42196303606033325,\n",
       "   0.041309528052806854,\n",
       "   -0.048056092113256454,\n",
       "   0.07495459914207458,\n",
       "   0.24770759046077728,\n",
       "   -0.1826329231262207,\n",
       "   -0.24363788962364197,\n",
       "   -0.13797695934772491,\n",
       "   -0.5324183106422424,\n",
       "   -0.001710452139377594,\n",
       "   -0.47825101017951965,\n",
       "   0.07046841084957123,\n",
       "   -0.17923936247825623,\n",
       "   0.13301268219947815,\n",
       "   0.1441756635904312,\n",
       "   0.19596067070960999,\n",
       "   0.28581446409225464,\n",
       "   0.25897929072380066,\n",
       "   -0.40216004848480225,\n",
       "   -0.12183113396167755,\n",
       "   0.3362334668636322,\n",
       "   0.1774376928806305,\n",
       "   -0.10306411981582642,\n",
       "   -0.04938957467675209,\n",
       "   0.12972933053970337,\n",
       "   -0.07298319041728973,\n",
       "   0.018082493916153908,\n",
       "   -0.1228199154138565,\n",
       "   0.059543024748563766,\n",
       "   -0.27247869968414307,\n",
       "   0.1553383767604828,\n",
       "   0.13148340582847595,\n",
       "   0.07626058161258698,\n",
       "   0.06830252707004547,\n",
       "   0.14919281005859375,\n",
       "   -0.08339180052280426,\n",
       "   -0.33649611473083496,\n",
       "   0.03880205750465393,\n",
       "   -0.15943819284439087,\n",
       "   0.15880019962787628,\n",
       "   -0.04620678350329399,\n",
       "   0.24676650762557983,\n",
       "   0.07561320066452026,\n",
       "   0.18306384980678558,\n",
       "   -0.03845854103565216,\n",
       "   0.07522797584533691,\n",
       "   -0.42648008465766907,\n",
       "   0.5815994143486023,\n",
       "   -0.23995637893676758,\n",
       "   0.4808430075645447,\n",
       "   -0.29133716225624084,\n",
       "   -0.20187893509864807,\n",
       "   -0.12091848999261856,\n",
       "   -0.2218812257051468,\n",
       "   -0.033799588680267334,\n",
       "   -0.32990697026252747,\n",
       "   0.1102544367313385,\n",
       "   0.11386322975158691,\n",
       "   0.03980273753404617,\n",
       "   0.005319366231560707,\n",
       "   0.13600938022136688,\n",
       "   0.6356154680252075,\n",
       "   0.09758813679218292,\n",
       "   0.009849458932876587,\n",
       "   0.05500709265470505,\n",
       "   0.05220901221036911,\n",
       "   0.24203170835971832,\n",
       "   0.30488109588623047,\n",
       "   0.48102501034736633,\n",
       "   0.3098185360431671,\n",
       "   -0.39013680815696716,\n",
       "   -0.12614575028419495,\n",
       "   0.13722261786460876,\n",
       "   -0.3269100785255432,\n",
       "   0.18290261924266815,\n",
       "   0.17920434474945068,\n",
       "   0.08139156550168991,\n",
       "   -0.3141728639602661,\n",
       "   0.0902700424194336,\n",
       "   0.3150789439678192,\n",
       "   0.1326594203710556,\n",
       "   0.17703282833099365,\n",
       "   0.07696742564439774,\n",
       "   0.08245820552110672,\n",
       "   -0.023447290062904358,\n",
       "   -0.19062840938568115,\n",
       "   0.23024243116378784,\n",
       "   0.3259516656398773,\n",
       "   0.13202327489852905,\n",
       "   0.256411075592041,\n",
       "   -0.011144893243908882,\n",
       "   -0.08377411961555481,\n",
       "   -0.059751737862825394,\n",
       "   0.04745215177536011,\n",
       "   0.21019196510314941,\n",
       "   0.0762118399143219,\n",
       "   0.1798323690891266,\n",
       "   0.11683399230241776,\n",
       "   0.2646830677986145,\n",
       "   -0.08653208613395691,\n",
       "   0.4500013589859009,\n",
       "   0.25867632031440735,\n",
       "   0.0017352104187011719,\n",
       "   0.07030819356441498,\n",
       "   -0.08616574108600616,\n",
       "   0.08902307599782944,\n",
       "   -0.44970786571502686,\n",
       "   -0.03960372507572174,\n",
       "   -0.29388704895973206,\n",
       "   -0.05764053016901016,\n",
       "   -0.2789362668991089,\n",
       "   0.17882320284843445,\n",
       "   0.11600084602832794,\n",
       "   -0.15016409754753113,\n",
       "   0.3212968111038208,\n",
       "   0.030591541901230812,\n",
       "   0.3201100826263428,\n",
       "   -0.25425755977630615,\n",
       "   -0.32755717635154724,\n",
       "   0.18034978210926056,\n",
       "   -0.33816343545913696,\n",
       "   0.2838672399520874,\n",
       "   -0.23182281851768494,\n",
       "   -0.19019366800785065,\n",
       "   0.2431989163160324,\n",
       "   0.06920718401670456,\n",
       "   0.2447895109653473,\n",
       "   0.4446460008621216,\n",
       "   -0.25175708532333374,\n",
       "   0.2790301740169525,\n",
       "   -0.0732557401061058,\n",
       "   -0.028923802077770233,\n",
       "   0.09787864238023758,\n",
       "   -0.10459627211093903,\n",
       "   -0.19441691040992737,\n",
       "   0.2514326870441437,\n",
       "   -0.12311767041683197,\n",
       "   0.08852344751358032,\n",
       "   -0.03095255233347416,\n",
       "   -0.3993411660194397,\n",
       "   0.40472927689552307,\n",
       "   0.12288522720336914,\n",
       "   0.45952683687210083,\n",
       "   0.44761979579925537,\n",
       "   0.20812711119651794,\n",
       "   0.046648554503917694,\n",
       "   -0.19744452834129333,\n",
       "   -0.4105336666107178,\n",
       "   0.12212561070919037,\n",
       "   0.16474969685077667,\n",
       "   0.14503294229507446,\n",
       "   -0.3445426821708679,\n",
       "   -9.322747230529785,\n",
       "   0.24078300595283508,\n",
       "   -0.44471287727355957,\n",
       "   0.3956414461135864,\n",
       "   -0.38408392667770386,\n",
       "   0.036498308181762695,\n",
       "   -0.05595581233501434,\n",
       "   -0.03218643739819527,\n",
       "   -0.030994707718491554,\n",
       "   0.10308735072612762,\n",
       "   0.09320291876792908,\n",
       "   0.15789757668972015,\n",
       "   0.20670777559280396,\n",
       "   0.01684356853365898,\n",
       "   -0.12958018481731415,\n",
       "   -0.10823728144168854,\n",
       "   0.2999609112739563,\n",
       "   -0.4848332107067108,\n",
       "   0.11482183635234833,\n",
       "   0.021415919065475464,\n",
       "   0.02962317317724228,\n",
       "   -0.28744199872016907,\n",
       "   -0.09994475543498993,\n",
       "   0.2839827835559845,\n",
       "   -0.17824183404445648,\n",
       "   -0.019583821296691895,\n",
       "   0.14008834958076477,\n",
       "   -0.017837122082710266,\n",
       "   0.4497254192829132,\n",
       "   0.016170775517821312,\n",
       "   0.14662986993789673,\n",
       "   -0.13026849925518036,\n",
       "   -0.3358595371246338,\n",
       "   0.14714756608009338,\n",
       "   0.11855447292327881,\n",
       "   -0.1868990957736969,\n",
       "   0.11877818405628204,\n",
       "   -0.6414977312088013,\n",
       "   0.6867642402648926,\n",
       "   -0.393684059381485,\n",
       "   0.046809613704681396,\n",
       "   -0.17705702781677246,\n",
       "   0.00552310049533844,\n",
       "   0.05874696746468544,\n",
       "   0.2009425312280655,\n",
       "   0.06209652125835419,\n",
       "   -0.02483520284295082,\n",
       "   0.051789868623018265,\n",
       "   -0.04807131737470627,\n",
       "   0.4306737780570984,\n",
       "   0.1339682638645172,\n",
       "   -0.1548430472612381,\n",
       "   0.01418609730899334,\n",
       "   -0.13235270977020264,\n",
       "   -0.24366748332977295,\n",
       "   0.0006667338311672211,\n",
       "   -0.002824939787387848,\n",
       "   0.098679319024086,\n",
       "   0.2533518075942993,\n",
       "   -0.37478208541870117,\n",
       "   -0.009525768458843231,\n",
       "   0.5556350946426392,\n",
       "   0.6756359934806824,\n",
       "   -0.005514286458492279,\n",
       "   -0.16983285546302795,\n",
       "   0.5373853445053101,\n",
       "   -0.23973232507705688,\n",
       "   0.04505385458469391,\n",
       "   0.007625292055308819,\n",
       "   0.28085586428642273,\n",
       "   -0.21621102094650269,\n",
       "   -0.15523940324783325,\n",
       "   0.16535794734954834,\n",
       "   -0.18761226534843445,\n",
       "   -0.07248151302337646,\n",
       "   -0.3388786315917969,\n",
       "   0.03757450729608536,\n",
       "   0.11687025427818298,\n",
       "   -0.025732144713401794,\n",
       "   0.3523544371128082,\n",
       "   -0.11610651761293411,\n",
       "   0.13035619258880615,\n",
       "   0.4767470955848694,\n",
       "   -0.5442947745323181,\n",
       "   0.01888291910290718,\n",
       "   0.2023543268442154,\n",
       "   -0.15825766324996948,\n",
       "   0.0498209223151207,\n",
       "   -0.35557448863983154,\n",
       "   0.20818188786506653,\n",
       "   -0.49924352765083313,\n",
       "   0.12082985788583755,\n",
       "   0.1167198047041893,\n",
       "   -0.04799903184175491,\n",
       "   -0.008167024701833725,\n",
       "   0.4135654866695404,\n",
       "   0.004664532840251923,\n",
       "   -0.08934324234724045,\n",
       "   0.1742318868637085,\n",
       "   0.1494629681110382,\n",
       "   -0.16346976161003113,\n",
       "   0.20827548205852509,\n",
       "   0.05947354808449745,\n",
       "   -0.40804323554039,\n",
       "   0.2730099558830261,\n",
       "   0.28856778144836426,\n",
       "   0.21905888617038727,\n",
       "   -0.027915367856621742,\n",
       "   0.035528071224689484,\n",
       "   -0.3440447747707367,\n",
       "   -0.1563568115234375,\n",
       "   0.03787922114133835,\n",
       "   0.27698105573654175,\n",
       "   0.12806951999664307,\n",
       "   -0.07923856377601624,\n",
       "   0.22679227590560913,\n",
       "   0.15942831337451935,\n",
       "   0.23874184489250183,\n",
       "   0.182816281914711,\n",
       "   -0.1755516529083252,\n",
       "   0.2552702724933624,\n",
       "   -0.49288439750671387,\n",
       "   0.2391616702079773,\n",
       "   -0.015249610878527164,\n",
       "   -0.04801688715815544,\n",
       "   0.23036615550518036,\n",
       "   -0.027865882962942123,\n",
       "   0.2692214548587799,\n",
       "   0.08893643319606781,\n",
       "   -0.13743841648101807,\n",
       "   0.09915448725223541,\n",
       "   0.43327245116233826,\n",
       "   -0.21602006256580353,\n",
       "   0.10483358800411224,\n",
       "   -0.04306720942258835,\n",
       "   0.10380139946937561,\n",
       "   -0.44680365920066833,\n",
       "   0.01953236386179924,\n",
       "   0.1452454775571823,\n",
       "   0.4038349390029907,\n",
       "   -0.2994631230831146,\n",
       "   0.09826619923114777,\n",
       "   0.07292849570512772,\n",
       "   -0.1037830039858818,\n",
       "   0.3837406039237976,\n",
       "   -0.16728132963180542,\n",
       "   -0.16628283262252808,\n",
       "   0.10470845550298691,\n",
       "   0.35058870911598206,\n",
       "   0.029159920290112495,\n",
       "   -0.13187554478645325,\n",
       "   0.3131965398788452,\n",
       "   0.4171103835105896,\n",
       "   -0.11177974194288254,\n",
       "   0.1397964358329773,\n",
       "   -0.31767117977142334,\n",
       "   0.07381893694400787,\n",
       "   -0.19556014239788055,\n",
       "   0.015735439956188202,\n",
       "   0.19369181990623474,\n",
       "   -0.22244428098201752,\n",
       "   0.043318942189216614,\n",
       "   -0.2491779327392578,\n",
       "   0.04778743162751198,\n",
       "   0.018834050744771957,\n",
       "   -0.08522103726863861,\n",
       "   0.12228269875049591,\n",
       "   -0.28363001346588135,\n",
       "   0.04088722914457321,\n",
       "   -0.32678544521331787,\n",
       "   -0.1663251370191574,\n",
       "   0.2025245726108551,\n",
       "   -0.038067758083343506,\n",
       "   -0.07733143121004105,\n",
       "   -0.1420140117406845,\n",
       "   -0.1600363403558731,\n",
       "   -0.15139292180538177,\n",
       "   -0.18066731095314026,\n",
       "   0.3577122688293457,\n",
       "   -0.06347166001796722,\n",
       "   0.1809980869293213,\n",
       "   0.30264636874198914,\n",
       "   -0.05101541802287102,\n",
       "   -0.44160839915275574,\n",
       "   -0.27330854535102844,\n",
       "   0.06614187359809875,\n",
       "   0.10400630533695221,\n",
       "   0.0020872745662927628,\n",
       "   0.19865722954273224,\n",
       "   0.07123613357543945,\n",
       "   0.1062459871172905,\n",
       "   -0.4558655023574829,\n",
       "   -0.13405238091945648,\n",
       "   0.05632400885224342,\n",
       "   0.052114687860012054,\n",
       "   0.4160921275615692,\n",
       "   0.11906853318214417,\n",
       "   0.37769976258277893,\n",
       "   -0.03881402313709259,\n",
       "   0.08613460510969162,\n",
       "   -0.0031883828341960907,\n",
       "   0.1740264594554901,\n",
       "   0.007821016944944859,\n",
       "   0.07826592773199081,\n",
       "   -0.10115706920623779,\n",
       "   0.03784414380788803,\n",
       "   -0.1557052582502365,\n",
       "   -0.4175282418727875,\n",
       "   0.07057555019855499,\n",
       "   0.035553544759750366,\n",
       "   0.11988211423158646],\n",
       "  [-0.1611807495355606,\n",
       "   0.25487735867500305,\n",
       "   -0.33907362818717957,\n",
       "   -0.12996608018875122,\n",
       "   0.5241678953170776,\n",
       "   -0.23995763063430786,\n",
       "   0.3960500955581665,\n",
       "   -0.0818411111831665,\n",
       "   0.09046107530593872,\n",
       "   0.47681108117103577,\n",
       "   0.3645860254764557,\n",
       "   0.08496485650539398,\n",
       "   -0.21909365057945251,\n",
       "   0.20740464329719543,\n",
       "   -0.6709028482437134,\n",
       "   -0.502063512802124,\n",
       "   0.0706387609243393,\n",
       "   -0.4079698324203491,\n",
       "   -0.34148257970809937,\n",
       "   -0.04170176014304161,\n",
       "   0.28540679812431335,\n",
       "   0.30541524291038513,\n",
       "   -0.09589706361293793,\n",
       "   -0.027225494384765625,\n",
       "   0.16270044445991516,\n",
       "   -0.07065755128860474,\n",
       "   0.32896336913108826,\n",
       "   0.7368723154067993,\n",
       "   0.10515350103378296,\n",
       "   0.3984680473804474,\n",
       "   0.33371487259864807,\n",
       "   0.05020969361066818,\n",
       "   0.019615106284618378,\n",
       "   0.037762075662612915,\n",
       "   0.5269654393196106,\n",
       "   0.5777071118354797,\n",
       "   -0.618889331817627,\n",
       "   0.4102226793766022,\n",
       "   0.022482028231024742,\n",
       "   0.5657498836517334,\n",
       "   -0.015380071476101875,\n",
       "   -0.1056622862815857,\n",
       "   -0.009173408150672913,\n",
       "   -0.03448779135942459,\n",
       "   0.360817551612854,\n",
       "   0.030257448554039,\n",
       "   -0.03941427916288376,\n",
       "   -0.1264026015996933,\n",
       "   -0.38163870573043823,\n",
       "   -0.016926901414990425,\n",
       "   0.01709664985537529,\n",
       "   -0.3749004900455475,\n",
       "   -0.26225584745407104,\n",
       "   -0.27541616559028625,\n",
       "   0.05243941396474838,\n",
       "   -0.189970463514328,\n",
       "   0.18090739846229553,\n",
       "   -0.08110492676496506,\n",
       "   -0.20975980162620544,\n",
       "   0.8003363013267517,\n",
       "   -0.13251087069511414,\n",
       "   0.2743007242679596,\n",
       "   -0.05620821565389633,\n",
       "   -0.460865318775177,\n",
       "   0.2837352752685547,\n",
       "   0.2074088454246521,\n",
       "   0.07655005902051926,\n",
       "   -0.13816779851913452,\n",
       "   0.49509197473526,\n",
       "   0.07816608250141144,\n",
       "   -0.23057398200035095,\n",
       "   0.295487642288208,\n",
       "   -0.16200420260429382,\n",
       "   -0.25953733921051025,\n",
       "   -0.2688930928707123,\n",
       "   -0.3849630355834961,\n",
       "   0.24702486395835876,\n",
       "   0.055131908506155014,\n",
       "   0.024577800184488297,\n",
       "   -0.0393873006105423,\n",
       "   0.1445179134607315,\n",
       "   0.6435463428497314,\n",
       "   -0.10290677845478058,\n",
       "   0.3885154724121094,\n",
       "   -0.5587700605392456,\n",
       "   0.01672045886516571,\n",
       "   -0.5611451268196106,\n",
       "   0.03553338721394539,\n",
       "   0.4389627277851105,\n",
       "   0.734992265701294,\n",
       "   0.19315211474895477,\n",
       "   -0.5062922835350037,\n",
       "   0.19690990447998047,\n",
       "   -0.3151560425758362,\n",
       "   -0.22122791409492493,\n",
       "   -0.001290149986743927,\n",
       "   -0.3790851831436157,\n",
       "   0.08574904501438141,\n",
       "   -0.5695253610610962,\n",
       "   -0.09390734881162643,\n",
       "   -0.33695685863494873,\n",
       "   -0.01993967592716217,\n",
       "   -0.06315135955810547,\n",
       "   -0.17143525183200836,\n",
       "   -0.1548852175474167,\n",
       "   0.12075009942054749,\n",
       "   -0.1299823373556137,\n",
       "   -0.7910168766975403,\n",
       "   -0.3719598054885864,\n",
       "   0.3715522289276123,\n",
       "   0.1942865550518036,\n",
       "   0.1816430389881134,\n",
       "   0.318106472492218,\n",
       "   0.312289297580719,\n",
       "   -0.333897203207016,\n",
       "   -0.2993658483028412,\n",
       "   -0.1441805362701416,\n",
       "   0.3052653670310974,\n",
       "   0.26496943831443787,\n",
       "   -0.2768632769584656,\n",
       "   0.1857905387878418,\n",
       "   0.15355277061462402,\n",
       "   -0.2371608167886734,\n",
       "   -0.17795725166797638,\n",
       "   0.10366494953632355,\n",
       "   -0.3558695316314697,\n",
       "   -0.06384395062923431,\n",
       "   -0.36368414759635925,\n",
       "   0.20966137945652008,\n",
       "   -0.17850054800510406,\n",
       "   -0.26084524393081665,\n",
       "   -0.11780254542827606,\n",
       "   -0.1839793473482132,\n",
       "   0.2641681432723999,\n",
       "   0.18772991001605988,\n",
       "   0.2708500921726227,\n",
       "   -0.8909933567047119,\n",
       "   0.505607545375824,\n",
       "   -0.911632776260376,\n",
       "   -0.2875382900238037,\n",
       "   0.03411014378070831,\n",
       "   0.4675022065639496,\n",
       "   0.5237364768981934,\n",
       "   -0.32237544655799866,\n",
       "   -0.04780405014753342,\n",
       "   -0.5050094127655029,\n",
       "   -0.3783015012741089,\n",
       "   -0.06776932626962662,\n",
       "   0.3586300313472748,\n",
       "   0.15622839331626892,\n",
       "   -0.47065457701683044,\n",
       "   0.11395490169525146,\n",
       "   0.18747852742671967,\n",
       "   -0.6924288272857666,\n",
       "   -0.12418034672737122,\n",
       "   -0.13461758196353912,\n",
       "   -0.3887910842895508,\n",
       "   0.031791236251592636,\n",
       "   0.6316362023353577,\n",
       "   0.5841690301895142,\n",
       "   -0.049560438841581345,\n",
       "   -0.5320765972137451,\n",
       "   -0.3892451226711273,\n",
       "   0.2933294475078583,\n",
       "   0.16334906220436096,\n",
       "   0.09418615698814392,\n",
       "   -0.011521931737661362,\n",
       "   0.17687790095806122,\n",
       "   -0.5471401810646057,\n",
       "   0.49979710578918457,\n",
       "   0.5758668780326843,\n",
       "   -0.008184880949556828,\n",
       "   -0.010202422738075256,\n",
       "   0.12731952965259552,\n",
       "   -0.045491598546504974,\n",
       "   -0.054817114025354385,\n",
       "   0.7858955264091492,\n",
       "   0.03676241636276245,\n",
       "   -0.2523183226585388,\n",
       "   -0.05207820236682892,\n",
       "   0.40173864364624023,\n",
       "   0.01542247086763382,\n",
       "   -0.43250295519828796,\n",
       "   -0.6307572722434998,\n",
       "   0.10409297794103622,\n",
       "   -0.5508578419685364,\n",
       "   0.2506115734577179,\n",
       "   -0.7649783492088318,\n",
       "   0.5634003281593323,\n",
       "   0.2590898871421814,\n",
       "   0.047441355884075165,\n",
       "   0.025842837989330292,\n",
       "   -0.4543452858924866,\n",
       "   0.15765434503555298,\n",
       "   -0.5229417681694031,\n",
       "   0.3572027385234833,\n",
       "   0.08482255041599274,\n",
       "   0.10434741526842117,\n",
       "   -0.17223159968852997,\n",
       "   0.32637178897857666,\n",
       "   -0.07457459717988968,\n",
       "   0.25615012645721436,\n",
       "   -0.20128624141216278,\n",
       "   -0.05744719132781029,\n",
       "   0.18718771636486053,\n",
       "   0.3243117332458496,\n",
       "   -0.2923356592655182,\n",
       "   0.24213221669197083,\n",
       "   -0.32563698291778564,\n",
       "   -0.5171034932136536,\n",
       "   0.5864724516868591,\n",
       "   -0.9867945313453674,\n",
       "   -0.29782384634017944,\n",
       "   -0.22253157198429108,\n",
       "   0.4641242027282715,\n",
       "   -0.3671755790710449,\n",
       "   0.18604415655136108,\n",
       "   -0.0775747150182724,\n",
       "   0.04644860327243805,\n",
       "   0.08029434829950333,\n",
       "   -0.3733523190021515,\n",
       "   -0.08275221288204193,\n",
       "   0.04682152345776558,\n",
       "   -0.48164209723472595,\n",
       "   -0.6080593466758728,\n",
       "   0.7808124423027039,\n",
       "   0.004603150300681591,\n",
       "   0.4872890114784241,\n",
       "   0.0717693567276001,\n",
       "   -0.20549023151397705,\n",
       "   0.2305455505847931,\n",
       "   -0.0826139748096466,\n",
       "   0.26670926809310913,\n",
       "   -0.36862123012542725,\n",
       "   0.20790685713291168,\n",
       "   0.41685032844543457,\n",
       "   -0.12099803984165192,\n",
       "   -0.005754934623837471,\n",
       "   -0.4716522693634033,\n",
       "   -0.6004129648208618,\n",
       "   0.14185205101966858,\n",
       "   0.24783475697040558,\n",
       "   -0.17491209506988525,\n",
       "   0.0939834788441658,\n",
       "   -0.07574450224637985,\n",
       "   -0.18352989852428436,\n",
       "   -0.24794037640094757,\n",
       "   0.19859632849693298,\n",
       "   -0.04839090257883072,\n",
       "   -0.2008782923221588,\n",
       "   -0.28997528553009033,\n",
       "   -0.29209011793136597,\n",
       "   -0.1576337218284607,\n",
       "   -0.1739378571510315,\n",
       "   0.5212514996528625,\n",
       "   0.002799184061586857,\n",
       "   -0.1152179092168808,\n",
       "   0.5686090588569641,\n",
       "   0.7254942059516907,\n",
       "   -1.0454332828521729,\n",
       "   0.18580184876918793,\n",
       "   0.5087228417396545,\n",
       "   0.28874945640563965,\n",
       "   -0.12014211714267731,\n",
       "   -0.33374276757240295,\n",
       "   -0.27898064255714417,\n",
       "   -0.33108267188072205,\n",
       "   -0.435924768447876,\n",
       "   -0.8061230182647705,\n",
       "   0.29678961634635925,\n",
       "   0.17564544081687927,\n",
       "   -0.3080475926399231,\n",
       "   0.10638795047998428,\n",
       "   0.28880706429481506,\n",
       "   -0.03798481076955795,\n",
       "   -0.11371494829654694,\n",
       "   0.18010738492012024,\n",
       "   -0.21715644001960754,\n",
       "   -0.8490079045295715,\n",
       "   -0.5200294256210327,\n",
       "   0.6392513513565063,\n",
       "   0.3721683621406555,\n",
       "   0.44778913259506226,\n",
       "   0.004984377417713404,\n",
       "   -0.5795625448226929,\n",
       "   -0.460709810256958,\n",
       "   -0.05018427222967148,\n",
       "   0.36190348863601685,\n",
       "   -0.03871099650859833,\n",
       "   -0.572948157787323,\n",
       "   -0.3277945816516876,\n",
       "   0.01590919867157936,\n",
       "   -0.6231561303138733,\n",
       "   -0.11371512711048126,\n",
       "   0.1300823986530304,\n",
       "   0.12161830067634583,\n",
       "   -0.057417403906583786,\n",
       "   0.24719014763832092,\n",
       "   -0.45256343483924866,\n",
       "   -0.17439676821231842,\n",
       "   -0.2690065801143646,\n",
       "   -0.09504732489585876,\n",
       "   0.47320815920829773,\n",
       "   -0.07921601831912994,\n",
       "   -0.1759805530309677,\n",
       "   -0.004227990284562111,\n",
       "   0.39032894372940063,\n",
       "   0.3683956563472748,\n",
       "   0.4590759873390198,\n",
       "   0.3725590407848358,\n",
       "   0.4673811197280884,\n",
       "   -0.09202762693166733,\n",
       "   -0.04713971167802811,\n",
       "   -0.021669764071702957,\n",
       "   -0.010674238204956055,\n",
       "   -0.2628558576107025,\n",
       "   -0.047015197575092316,\n",
       "   -0.19194287061691284,\n",
       "   -0.21011430025100708,\n",
       "   -0.08524817228317261,\n",
       "   -0.3213306665420532,\n",
       "   0.2805410623550415,\n",
       "   -0.3578856587409973,\n",
       "   0.47136062383651733,\n",
       "   0.020952444523572922,\n",
       "   0.21679672598838806,\n",
       "   0.11991263926029205,\n",
       "   0.48703956604003906,\n",
       "   0.01572388783097267,\n",
       "   -0.07474104315042496,\n",
       "   0.1761404424905777,\n",
       "   -0.5614816546440125,\n",
       "   -0.22983355820178986,\n",
       "   0.17573648691177368,\n",
       "   -0.6686013340950012,\n",
       "   -0.25734999775886536,\n",
       "   -0.2801733613014221,\n",
       "   0.32306569814682007,\n",
       "   -0.866256594657898,\n",
       "   0.6142315864562988,\n",
       "   0.20499294996261597,\n",
       "   0.41967910528182983,\n",
       "   0.2565876841545105,\n",
       "   -0.3012479245662689,\n",
       "   0.5664309859275818,\n",
       "   0.4279719591140747,\n",
       "   -0.062226418405771255,\n",
       "   0.40130361914634705,\n",
       "   -0.11381086707115173,\n",
       "   0.03217322379350662,\n",
       "   -0.4732300341129303,\n",
       "   -0.3058851361274719,\n",
       "   -0.03218173235654831,\n",
       "   -0.385312020778656,\n",
       "   0.6459418535232544,\n",
       "   0.045085370540618896,\n",
       "   -0.06170361489057541,\n",
       "   0.38781145215034485,\n",
       "   0.09355363249778748,\n",
       "   0.6372920274734497,\n",
       "   0.24056540429592133,\n",
       "   -0.003538161516189575,\n",
       "   0.47376197576522827,\n",
       "   0.04744135960936546,\n",
       "   0.2823474407196045,\n",
       "   -0.1681348979473114,\n",
       "   -0.8622671961784363,\n",
       "   -0.20753340423107147,\n",
       "   -0.8606033325195312,\n",
       "   -0.27916422486305237,\n",
       "   1.1827247142791748,\n",
       "   -0.23245543241500854,\n",
       "   0.225824236869812,\n",
       "   -0.19172102212905884,\n",
       "   0.45858681201934814,\n",
       "   -0.3879397511482239,\n",
       "   -0.6489841938018799,\n",
       "   0.050284311175346375,\n",
       "   0.0007892362773418427,\n",
       "   -0.6255217790603638,\n",
       "   -0.27062907814979553,\n",
       "   0.32252392172813416,\n",
       "   0.2522116005420685,\n",
       "   -0.1672648787498474,\n",
       "   0.0301496759057045,\n",
       "   -0.30435484647750854,\n",
       "   0.019072072580456734,\n",
       "   0.4379347562789917,\n",
       "   -0.03912600874900818,\n",
       "   -0.6419830918312073,\n",
       "   0.3682540953159332,\n",
       "   0.32277238368988037,\n",
       "   0.3955478072166443,\n",
       "   -0.41644352674484253,\n",
       "   -0.1389285773038864,\n",
       "   0.3417990207672119,\n",
       "   -0.464706689119339,\n",
       "   0.4481540322303772,\n",
       "   0.16648195683956146,\n",
       "   -0.12956517934799194,\n",
       "   -0.3028043806552887,\n",
       "   0.25494199991226196,\n",
       "   -0.040789615362882614,\n",
       "   -0.3203641474246979,\n",
       "   -0.6121867895126343,\n",
       "   -0.3908578157424927,\n",
       "   -0.3095930516719818,\n",
       "   0.27793025970458984,\n",
       "   0.08609682321548462,\n",
       "   0.10871458053588867,\n",
       "   -0.26142916083335876,\n",
       "   -0.40568578243255615,\n",
       "   0.014902276918292046,\n",
       "   0.28275907039642334,\n",
       "   0.4760775566101074,\n",
       "   -0.40821078419685364,\n",
       "   0.06047002226114273,\n",
       "   -0.09930190443992615,\n",
       "   -0.005339205265045166,\n",
       "   0.04056224226951599,\n",
       "   -0.020960189402103424,\n",
       "   0.04225369542837143,\n",
       "   0.19403882324695587,\n",
       "   -0.16550981998443604,\n",
       "   -0.02631242573261261,\n",
       "   0.27633583545684814,\n",
       "   0.04080640524625778,\n",
       "   -0.034036148339509964,\n",
       "   -0.41647496819496155,\n",
       "   0.09210706502199173,\n",
       "   -0.11870824545621872,\n",
       "   0.7137218117713928,\n",
       "   -0.018477410078048706,\n",
       "   0.8696326613426208,\n",
       "   -0.06789614260196686,\n",
       "   0.15488164126873016,\n",
       "   -0.19119812548160553,\n",
       "   0.1361716389656067,\n",
       "   -0.2600694000720978,\n",
       "   0.24030621349811554,\n",
       "   -0.27038973569869995,\n",
       "   -0.4293617606163025,\n",
       "   -0.6342845559120178,\n",
       "   -0.7949785590171814,\n",
       "   -0.03399833291769028,\n",
       "   -0.44016870856285095,\n",
       "   -0.42362895607948303,\n",
       "   -0.5722860097885132,\n",
       "   0.028717923909425735,\n",
       "   -0.35516640543937683,\n",
       "   0.23839156329631805,\n",
       "   0.05603954941034317,\n",
       "   -0.3583437502384186,\n",
       "   0.08771766722202301,\n",
       "   0.2978261113166809,\n",
       "   -0.4206429123878479,\n",
       "   -0.6134313941001892,\n",
       "   -0.31061458587646484,\n",
       "   -0.36819177865982056,\n",
       "   0.00800643116235733,\n",
       "   -1.1116644144058228,\n",
       "   0.31659209728240967,\n",
       "   0.04180487245321274,\n",
       "   0.5331407785415649,\n",
       "   -0.13732554018497467,\n",
       "   -0.11776562035083771,\n",
       "   0.5778921842575073,\n",
       "   0.4515661895275116,\n",
       "   0.8493819236755371,\n",
       "   0.12393447011709213,\n",
       "   0.7081283926963806,\n",
       "   0.6008548736572266,\n",
       "   0.1698656529188156,\n",
       "   1.044695496559143,\n",
       "   0.046111803501844406,\n",
       "   -0.9326905012130737,\n",
       "   0.028310274705290794,\n",
       "   0.3845921754837036,\n",
       "   -0.17224472761154175,\n",
       "   -0.147386834025383,\n",
       "   -0.6794406771659851,\n",
       "   0.05287382751703262,\n",
       "   -0.3958609104156494,\n",
       "   -0.23576274514198303,\n",
       "   0.03321177139878273,\n",
       "   -0.2599145174026489,\n",
       "   1.1132663488388062,\n",
       "   0.15778310596942902,\n",
       "   0.40022730827331543,\n",
       "   0.39104321599006653,\n",
       "   0.050805553793907166,\n",
       "   -0.23026379942893982,\n",
       "   -0.023123150691390038,\n",
       "   0.143898144364357,\n",
       "   -0.6486459374427795,\n",
       "   0.13170555233955383,\n",
       "   -0.03208694979548454,\n",
       "   0.13205315172672272,\n",
       "   0.178217813372612,\n",
       "   -0.4725237488746643,\n",
       "   -0.06972020864486694,\n",
       "   0.1837294101715088,\n",
       "   0.24692556262016296,\n",
       "   0.3128528594970703,\n",
       "   -0.6673089861869812,\n",
       "   1.0399264097213745,\n",
       "   0.32757657766342163,\n",
       "   0.05406244099140167,\n",
       "   0.4357719421386719,\n",
       "   0.3967992663383484,\n",
       "   -0.07823678851127625,\n",
       "   -0.2924973964691162,\n",
       "   0.038145922124385834,\n",
       "   -0.023923449218273163,\n",
       "   -0.012194152921438217,\n",
       "   -0.10499182343482971,\n",
       "   0.4143117070198059,\n",
       "   0.11188873648643494,\n",
       "   0.15597578883171082,\n",
       "   -0.22245971858501434,\n",
       "   0.43737536668777466,\n",
       "   0.24591383337974548,\n",
       "   0.20151963829994202,\n",
       "   0.36282825469970703,\n",
       "   0.06501860171556473,\n",
       "   -0.054843202233314514,\n",
       "   0.7511821985244751,\n",
       "   0.6218429803848267,\n",
       "   0.08290021121501923,\n",
       "   0.007583539932966232,\n",
       "   -0.16646920144557953,\n",
       "   -0.020261716097593307,\n",
       "   0.2650858759880066,\n",
       "   -0.30006521940231323,\n",
       "   1.0192818641662598,\n",
       "   -0.2523295283317566,\n",
       "   0.549095630645752,\n",
       "   -0.19170311093330383,\n",
       "   -0.303382009267807,\n",
       "   -0.24649709463119507,\n",
       "   -0.0846155658364296,\n",
       "   0.022817838937044144,\n",
       "   0.18356797099113464,\n",
       "   0.09961258620023727,\n",
       "   0.15479302406311035,\n",
       "   0.3834019899368286,\n",
       "   -0.20922943949699402,\n",
       "   0.3950156569480896,\n",
       "   0.5017166137695312,\n",
       "   -0.14048168063163757,\n",
       "   0.3479824662208557,\n",
       "   -0.006775073707103729,\n",
       "   -1.2905393838882446,\n",
       "   0.2572507858276367,\n",
       "   0.3070984184741974,\n",
       "   -0.26462411880493164,\n",
       "   -0.10250471532344818,\n",
       "   -8.461233139038086,\n",
       "   0.42074066400527954,\n",
       "   -0.06890013813972473,\n",
       "   0.35919222235679626,\n",
       "   0.343711793422699,\n",
       "   0.676390528678894,\n",
       "   0.2526792883872986,\n",
       "   -0.6744295358657837,\n",
       "   0.01684458553791046,\n",
       "   -0.03893860802054405,\n",
       "   -0.3312196433544159,\n",
       "   -0.33403652906417847,\n",
       "   0.5931397676467896,\n",
       "   -0.06325148791074753,\n",
       "   0.35472139716148376,\n",
       "   0.11524206399917603,\n",
       "   -0.08524637669324875,\n",
       "   0.2307676374912262,\n",
       "   0.6242288947105408,\n",
       "   -0.3788490891456604,\n",
       "   -0.07490311563014984,\n",
       "   0.45675212144851685,\n",
       "   0.2507927417755127,\n",
       "   0.14842356741428375,\n",
       "   -0.08614359050989151,\n",
       "   0.1364707350730896,\n",
       "   0.32248497009277344,\n",
       "   0.019159726798534393,\n",
       "   0.1271648108959198,\n",
       "   -0.0871860608458519,\n",
       "   -0.14094458520412445,\n",
       "   0.07003948092460632,\n",
       "   -0.14966784417629242,\n",
       "   -0.16681717336177826,\n",
       "   0.3882991373538971,\n",
       "   -0.3328094780445099,\n",
       "   0.05080334469676018,\n",
       "   -0.18961961567401886,\n",
       "   0.8266004920005798,\n",
       "   -0.4771985709667206,\n",
       "   -0.055155251175165176,\n",
       "   -0.22593002021312714,\n",
       "   -0.3276798129081726,\n",
       "   -0.22564689815044403,\n",
       "   0.26996681094169617,\n",
       "   0.11049838364124298,\n",
       "   0.2023545205593109,\n",
       "   -0.18657240271568298,\n",
       "   -0.6951894164085388,\n",
       "   0.39505767822265625,\n",
       "   0.5411195755004883,\n",
       "   0.27807220816612244,\n",
       "   0.4399771988391876,\n",
       "   0.23380495607852936,\n",
       "   -0.629167377948761,\n",
       "   -0.21259218454360962,\n",
       "   -0.207047238945961,\n",
       "   0.7813321948051453,\n",
       "   0.06425879150629044,\n",
       "   -0.24195262789726257,\n",
       "   0.1982850432395935,\n",
       "   -0.3871176540851593,\n",
       "   1.0662949085235596,\n",
       "   -0.0108179971575737,\n",
       "   -0.1649174839258194,\n",
       "   0.42062586545944214,\n",
       "   0.008915618062019348,\n",
       "   0.36211663484573364,\n",
       "   0.06667620688676834,\n",
       "   -0.2655065357685089,\n",
       "   -0.12798309326171875,\n",
       "   -0.40621599555015564,\n",
       "   0.15574216842651367,\n",
       "   -0.8797314763069153,\n",
       "   0.10944865643978119,\n",
       "   -1.0463296175003052,\n",
       "   -0.6048271656036377,\n",
       "   -0.05775978043675423,\n",
       "   -0.32270190119743347,\n",
       "   0.41138580441474915,\n",
       "   -0.2712939381599426,\n",
       "   0.5715921521186829,\n",
       "   0.2749093770980835,\n",
       "   -0.6586940884590149,\n",
       "   0.03106851503252983,\n",
       "   0.11090236902236938,\n",
       "   -0.24582555890083313,\n",
       "   0.24054735898971558,\n",
       "   -0.1413227766752243,\n",
       "   0.061777763068675995,\n",
       "   0.08935696631669998,\n",
       "   0.4066145718097687,\n",
       "   -0.01121424324810505,\n",
       "   -0.02183939889073372,\n",
       "   0.14420296251773834,\n",
       "   0.2681195139884949,\n",
       "   -0.20947504043579102,\n",
       "   -0.22786858677864075,\n",
       "   0.06600230932235718,\n",
       "   0.0002632550895214081,\n",
       "   -0.6900690793991089,\n",
       "   -0.08256642520427704,\n",
       "   -0.4670441150665283,\n",
       "   0.41129744052886963,\n",
       "   -0.12397710233926773,\n",
       "   -0.3021606504917145,\n",
       "   0.4677993655204773,\n",
       "   -0.6343556046485901,\n",
       "   0.47595757246017456,\n",
       "   0.3509058952331543,\n",
       "   -0.41110411286354065,\n",
       "   0.48059168457984924,\n",
       "   0.9691582918167114,\n",
       "   -0.6198944449424744,\n",
       "   0.17560693621635437,\n",
       "   0.46370136737823486,\n",
       "   0.6311014890670776,\n",
       "   -0.11954252421855927,\n",
       "   0.2885432243347168,\n",
       "   -0.6181806921958923,\n",
       "   -0.26434820890426636,\n",
       "   -0.311006635427475,\n",
       "   0.20582890510559082,\n",
       "   -0.0826258435845375,\n",
       "   0.5847301483154297,\n",
       "   -0.34600451588630676,\n",
       "   0.1025838851928711,\n",
       "   -0.0833224430680275,\n",
       "   0.38459137082099915,\n",
       "   0.016180161386728287,\n",
       "   0.20843811333179474,\n",
       "   -0.018626540899276733,\n",
       "   -0.242498978972435,\n",
       "   0.7938210964202881,\n",
       "   0.27902865409851074,\n",
       "   0.019204765558242798,\n",
       "   -0.17275631427764893,\n",
       "   -0.4892137050628662,\n",
       "   -0.8415648937225342,\n",
       "   -0.3021145761013031,\n",
       "   -0.5301536917686462,\n",
       "   -0.3505967855453491,\n",
       "   0.8121252655982971,\n",
       "   0.46847620606422424,\n",
       "   0.7174307107925415,\n",
       "   -0.40908610820770264,\n",
       "   0.15861095488071442,\n",
       "   0.25298991799354553,\n",
       "   0.03455594554543495,\n",
       "   -0.3481197655200958,\n",
       "   -0.3595316708087921,\n",
       "   0.12215232104063034,\n",
       "   0.8088842034339905,\n",
       "   0.6441748142242432,\n",
       "   0.5998246669769287,\n",
       "   -0.7174039483070374,\n",
       "   0.515008270740509,\n",
       "   0.5524470210075378,\n",
       "   0.03858599811792374,\n",
       "   0.09241287410259247,\n",
       "   0.17091430723667145,\n",
       "   -0.08983221650123596,\n",
       "   0.22865717113018036,\n",
       "   1.0073153972625732,\n",
       "   -0.15557794272899628,\n",
       "   0.3272562026977539,\n",
       "   -0.25700464844703674,\n",
       "   0.5124136209487915,\n",
       "   0.12970766425132751,\n",
       "   0.4151151776313782,\n",
       "   -0.06049371510744095,\n",
       "   -0.6515085101127625,\n",
       "   0.45937472581863403,\n",
       "   0.3152366280555725,\n",
       "   -0.062345102429389954,\n",
       "   -0.4168170988559723,\n",
       "   0.08525663614273071,\n",
       "   -0.3192555904388428,\n",
       "   0.46335822343826294,\n",
       "   -0.06699128448963165,\n",
       "   0.32003435492515564,\n",
       "   0.48793914914131165,\n",
       "   -0.3741370439529419,\n",
       "   -0.18712776899337769,\n",
       "   -0.80240797996521,\n",
       "   -0.12542976438999176,\n",
       "   0.29424235224723816,\n",
       "   0.20694339275360107,\n",
       "   0.08666456490755081,\n",
       "   -0.29756593704223633,\n",
       "   0.07736902683973312,\n",
       "   0.2699863314628601,\n",
       "   -0.09897088259458542,\n",
       "   0.154282346367836,\n",
       "   0.06912270188331604,\n",
       "   0.21941189467906952,\n",
       "   -0.023836545646190643,\n",
       "   0.32676398754119873,\n",
       "   -0.06583919376134872,\n",
       "   -0.22021931409835815,\n",
       "   0.10840219259262085,\n",
       "   0.25811341404914856,\n",
       "   -0.33948948979377747,\n",
       "   -0.31116119027137756,\n",
       "   -0.3627333641052246,\n",
       "   -0.14684969186782837,\n",
       "   0.10231976211071014,\n",
       "   0.667039692401886,\n",
       "   -0.0714198648929596,\n",
       "   0.23693780601024628,\n",
       "   0.03896516561508179],\n",
       "  [-0.2580370604991913,\n",
       "   -0.026581062003970146,\n",
       "   -0.00020887190476059914,\n",
       "   0.09458553791046143,\n",
       "   0.16868825256824493,\n",
       "   -0.07082981616258621,\n",
       "   0.13221096992492676,\n",
       "   -0.08993549644947052,\n",
       "   -0.3115691542625427,\n",
       "   0.30776384472846985,\n",
       "   -0.09806586056947708,\n",
       "   0.04183107614517212,\n",
       "   -0.31415489315986633,\n",
       "   -0.15492771565914154,\n",
       "   -0.501987099647522,\n",
       "   -0.607144296169281,\n",
       "   0.13161256909370422,\n",
       "   -0.5406485199928284,\n",
       "   -0.05289708077907562,\n",
       "   -0.07065104693174362,\n",
       "   0.05446379631757736,\n",
       "   -0.14147281646728516,\n",
       "   -0.23325812816619873,\n",
       "   -0.15816345810890198,\n",
       "   0.14160670340061188,\n",
       "   -0.5551356673240662,\n",
       "   -0.1259886920452118,\n",
       "   0.7447522878646851,\n",
       "   -0.20747999846935272,\n",
       "   0.4583291709423065,\n",
       "   0.28302493691444397,\n",
       "   -0.22520437836647034,\n",
       "   0.49485963582992554,\n",
       "   0.02336837910115719,\n",
       "   0.03805502504110336,\n",
       "   0.24787358939647675,\n",
       "   -0.2894933223724365,\n",
       "   0.5485416650772095,\n",
       "   0.07099634408950806,\n",
       "   0.3927099406719208,\n",
       "   0.16960209608078003,\n",
       "   0.26407790184020996,\n",
       "   0.05051083117723465,\n",
       "   0.3119935989379883,\n",
       "   -0.7102120518684387,\n",
       "   -0.1754753291606903,\n",
       "   -0.06917250156402588,\n",
       "   -0.5312821865081787,\n",
       "   -0.2797755002975464,\n",
       "   0.10280601680278778,\n",
       "   0.5880902409553528,\n",
       "   -0.13363143801689148,\n",
       "   -0.20380620658397675,\n",
       "   -0.1065853089094162,\n",
       "   0.11379899084568024,\n",
       "   -0.48533207178115845,\n",
       "   0.15005241334438324,\n",
       "   0.1257888227701187,\n",
       "   -0.24781018495559692,\n",
       "   0.37401771545410156,\n",
       "   -0.0607326477766037,\n",
       "   -0.15053261816501617,\n",
       "   0.009287668392062187,\n",
       "   -0.05344797298312187,\n",
       "   0.05637025833129883,\n",
       "   0.04451409727334976,\n",
       "   0.3206360340118408,\n",
       "   -0.4170415699481964,\n",
       "   0.20449060201644897,\n",
       "   0.2001619040966034,\n",
       "   -0.1975078284740448,\n",
       "   -0.1016572117805481,\n",
       "   -0.2779811918735504,\n",
       "   -0.2658194899559021,\n",
       "   0.06533127278089523,\n",
       "   -0.08551543205976486,\n",
       "   -0.3567047715187073,\n",
       "   0.06505756080150604,\n",
       "   0.14206010103225708,\n",
       "   0.17886510491371155,\n",
       "   0.26966917514801025,\n",
       "   0.14091065526008606,\n",
       "   -0.20916764438152313,\n",
       "   0.5363062024116516,\n",
       "   0.10246153175830841,\n",
       "   0.15151527523994446,\n",
       "   -0.29991960525512695,\n",
       "   -0.013457447290420532,\n",
       "   -0.1186637207865715,\n",
       "   0.2297435849905014,\n",
       "   0.3306187689304352,\n",
       "   -0.5195227265357971,\n",
       "   0.5238829255104065,\n",
       "   -0.05531316250562668,\n",
       "   -0.12541641294956207,\n",
       "   -0.1446579098701477,\n",
       "   -0.48311647772789,\n",
       "   -0.3693115711212158,\n",
       "   -0.9509905576705933,\n",
       "   0.08079423755407333,\n",
       "   -0.11169767379760742,\n",
       "   0.46738162636756897,\n",
       "   -0.04734356328845024,\n",
       "   -0.20623497664928436,\n",
       "   0.06429453194141388,\n",
       "   0.16789186000823975,\n",
       "   -0.5912736654281616,\n",
       "   -0.8214202523231506,\n",
       "   -0.3872552514076233,\n",
       "   0.658966064453125,\n",
       "   0.0367639921605587,\n",
       "   0.3356190025806427,\n",
       "   -0.3125498294830322,\n",
       "   0.2828831076622009,\n",
       "   -0.4104161560535431,\n",
       "   -0.7497537136077881,\n",
       "   -0.003899228759109974,\n",
       "   0.14202970266342163,\n",
       "   0.11027469485998154,\n",
       "   0.1617678999900818,\n",
       "   0.2506231367588043,\n",
       "   -0.0500500313937664,\n",
       "   -0.15158098936080933,\n",
       "   0.10352344065904617,\n",
       "   0.0014174049720168114,\n",
       "   -0.1292150616645813,\n",
       "   -0.21233108639717102,\n",
       "   -0.47230908274650574,\n",
       "   0.2650700509548187,\n",
       "   -0.1692068725824356,\n",
       "   -0.2161271572113037,\n",
       "   0.11385989189147949,\n",
       "   -0.10639384388923645,\n",
       "   -0.23700079321861267,\n",
       "   0.2339513748884201,\n",
       "   0.13658876717090607,\n",
       "   -0.5958523154258728,\n",
       "   -0.09193023294210434,\n",
       "   -1.1826308965682983,\n",
       "   -0.4507281482219696,\n",
       "   -0.2806832790374756,\n",
       "   0.3938458263874054,\n",
       "   0.6745370030403137,\n",
       "   -0.21526119112968445,\n",
       "   0.141302689909935,\n",
       "   -0.13156062364578247,\n",
       "   -0.5191885828971863,\n",
       "   0.0002760663628578186,\n",
       "   0.2990080714225769,\n",
       "   0.11295783519744873,\n",
       "   -0.16458874940872192,\n",
       "   -0.2516687512397766,\n",
       "   0.21291129291057587,\n",
       "   -0.034735068678855896,\n",
       "   -0.32404592633247375,\n",
       "   -0.03754846751689911,\n",
       "   -0.1665184497833252,\n",
       "   0.006056532263755798,\n",
       "   0.5024868845939636,\n",
       "   0.6025906205177307,\n",
       "   0.17680232226848602,\n",
       "   -0.8164893388748169,\n",
       "   -0.13730913400650024,\n",
       "   0.4232575297355652,\n",
       "   0.1715061366558075,\n",
       "   -0.24418798089027405,\n",
       "   -0.06814216077327728,\n",
       "   0.465315043926239,\n",
       "   -0.38659632205963135,\n",
       "   0.11799430847167969,\n",
       "   0.3327937126159668,\n",
       "   -0.23555688560009003,\n",
       "   0.5027728080749512,\n",
       "   0.3515699505805969,\n",
       "   0.10051726549863815,\n",
       "   -0.09703268110752106,\n",
       "   0.3153652548789978,\n",
       "   -0.028004758059978485,\n",
       "   -0.04500815272331238,\n",
       "   -0.24231833219528198,\n",
       "   0.2374185025691986,\n",
       "   -0.4947293698787689,\n",
       "   0.17074526846408844,\n",
       "   -0.5892705917358398,\n",
       "   0.3051777780056,\n",
       "   -0.4308699071407318,\n",
       "   -0.029568437486886978,\n",
       "   -0.6974024772644043,\n",
       "   0.16119886934757233,\n",
       "   0.06513386219739914,\n",
       "   0.07700657844543457,\n",
       "   -0.3220258057117462,\n",
       "   -0.5001996755599976,\n",
       "   -0.06798665225505829,\n",
       "   -0.988978385925293,\n",
       "   0.3637492060661316,\n",
       "   -0.05773204565048218,\n",
       "   0.2952459454536438,\n",
       "   0.0727556049823761,\n",
       "   0.20161953568458557,\n",
       "   -0.0050359126180410385,\n",
       "   0.15082702040672302,\n",
       "   -0.3018661141395569,\n",
       "   -0.031036440283060074,\n",
       "   -0.13230673968791962,\n",
       "   0.2146124392747879,\n",
       "   -0.7408344745635986,\n",
       "   0.4225473999977112,\n",
       "   -0.17302948236465454,\n",
       "   -0.6659635901451111,\n",
       "   -0.156611829996109,\n",
       "   -0.6492683291435242,\n",
       "   0.09099507331848145,\n",
       "   -0.012517747469246387,\n",
       "   -0.5251419544219971,\n",
       "   -0.2879221439361572,\n",
       "   -0.22010551393032074,\n",
       "   0.14300177991390228,\n",
       "   -0.04812435060739517,\n",
       "   -0.13265129923820496,\n",
       "   0.4828248620033264,\n",
       "   0.22073188424110413,\n",
       "   0.3868746757507324,\n",
       "   -0.00959104485809803,\n",
       "   -0.36828312277793884,\n",
       "   0.3337440490722656,\n",
       "   0.34181028604507446,\n",
       "   0.5150432586669922,\n",
       "   0.4715133607387543,\n",
       "   -0.17704206705093384,\n",
       "   -0.1607610434293747,\n",
       "   0.13847407698631287,\n",
       "   -0.008947700262069702,\n",
       "   -0.41638651490211487,\n",
       "   0.2652187645435333,\n",
       "   0.06732241064310074,\n",
       "   -0.06145778298377991,\n",
       "   0.11447267234325409,\n",
       "   -0.7358319759368896,\n",
       "   -0.5847708582878113,\n",
       "   -0.04286663606762886,\n",
       "   -0.2064397633075714,\n",
       "   0.2911492884159088,\n",
       "   0.4463323950767517,\n",
       "   -0.10909511893987656,\n",
       "   0.0307163018733263,\n",
       "   -0.40206706523895264,\n",
       "   0.7018203139305115,\n",
       "   0.17239846289157867,\n",
       "   0.1763114333152771,\n",
       "   -0.35870361328125,\n",
       "   -0.1786297857761383,\n",
       "   -0.25562867522239685,\n",
       "   0.19948828220367432,\n",
       "   -0.39294445514678955,\n",
       "   -0.43313395977020264,\n",
       "   -0.43034470081329346,\n",
       "   0.5622184872627258,\n",
       "   0.5689756274223328,\n",
       "   -0.7415934801101685,\n",
       "   0.4714929163455963,\n",
       "   0.0837104469537735,\n",
       "   0.5340216159820557,\n",
       "   0.278860867023468,\n",
       "   -0.09496896713972092,\n",
       "   -0.11151719093322754,\n",
       "   -0.15514227747917175,\n",
       "   -0.4374738037586212,\n",
       "   -0.866813063621521,\n",
       "   0.711315929889679,\n",
       "   0.23860430717468262,\n",
       "   0.22847706079483032,\n",
       "   0.5276831388473511,\n",
       "   0.5207663178443909,\n",
       "   -0.14396074414253235,\n",
       "   0.17294999957084656,\n",
       "   0.10046206414699554,\n",
       "   -0.21848666667938232,\n",
       "   -0.05254703015089035,\n",
       "   -0.22555415332317352,\n",
       "   0.28051111102104187,\n",
       "   0.5794097781181335,\n",
       "   0.09984740614891052,\n",
       "   0.22698962688446045,\n",
       "   -0.6186579465866089,\n",
       "   0.2805480659008026,\n",
       "   0.06699599325656891,\n",
       "   0.6560162901878357,\n",
       "   -0.10874107480049133,\n",
       "   0.2053757756948471,\n",
       "   -0.38961005210876465,\n",
       "   -0.04029648005962372,\n",
       "   -0.4295680522918701,\n",
       "   0.018299439921975136,\n",
       "   -0.015278501436114311,\n",
       "   0.06062805652618408,\n",
       "   -0.10542862117290497,\n",
       "   0.3536381423473358,\n",
       "   -0.4453751742839813,\n",
       "   -0.3260810077190399,\n",
       "   0.19720458984375,\n",
       "   0.11971243470907211,\n",
       "   0.09464429318904877,\n",
       "   -0.016126329079270363,\n",
       "   -0.1723378598690033,\n",
       "   -0.06811191141605377,\n",
       "   0.20292827486991882,\n",
       "   -0.01877504400908947,\n",
       "   -0.4276675581932068,\n",
       "   0.7741925716400146,\n",
       "   0.46006643772125244,\n",
       "   -0.07403817772865295,\n",
       "   -0.38440388441085815,\n",
       "   -0.6232853531837463,\n",
       "   0.4076051712036133,\n",
       "   -0.4934270679950714,\n",
       "   -0.2842552959918976,\n",
       "   -0.36117327213287354,\n",
       "   -0.05005578696727753,\n",
       "   -0.11414405703544617,\n",
       "   -0.24655525386333466,\n",
       "   0.11512170732021332,\n",
       "   -0.42984747886657715,\n",
       "   0.529678225517273,\n",
       "   0.04383774846792221,\n",
       "   -0.06048687547445297,\n",
       "   -0.01961245760321617,\n",
       "   0.39701080322265625,\n",
       "   0.4341692626476288,\n",
       "   -0.17765626311302185,\n",
       "   -0.43917036056518555,\n",
       "   -0.30436038970947266,\n",
       "   0.32540634274482727,\n",
       "   0.5063284039497375,\n",
       "   0.15033049881458282,\n",
       "   0.6072776913642883,\n",
       "   0.01161867193877697,\n",
       "   0.030567096546292305,\n",
       "   -0.5295037031173706,\n",
       "   0.5136765241622925,\n",
       "   0.2867601811885834,\n",
       "   0.8457043170928955,\n",
       "   0.4255841374397278,\n",
       "   -0.12779253721237183,\n",
       "   0.32173240184783936,\n",
       "   0.18892312049865723,\n",
       "   -0.1168435737490654,\n",
       "   -0.21690316498279572,\n",
       "   -0.16602899134159088,\n",
       "   -0.31570491194725037,\n",
       "   -0.47001913189888,\n",
       "   -0.47443637251853943,\n",
       "   -0.32197487354278564,\n",
       "   0.29202327132225037,\n",
       "   0.9017189741134644,\n",
       "   0.12795984745025635,\n",
       "   0.12219729274511337,\n",
       "   0.2040407359600067,\n",
       "   0.4134374260902405,\n",
       "   -0.05320645123720169,\n",
       "   0.3226366639137268,\n",
       "   0.1761268526315689,\n",
       "   -0.14216087758541107,\n",
       "   0.43504059314727783,\n",
       "   0.004195345565676689,\n",
       "   0.06537981331348419,\n",
       "   -0.11007661372423172,\n",
       "   0.13539500534534454,\n",
       "   -0.030017893761396408,\n",
       "   -0.10725203156471252,\n",
       "   0.9718596935272217,\n",
       "   -0.06090166047215462,\n",
       "   0.21223542094230652,\n",
       "   0.1830425262451172,\n",
       "   0.16434605419635773,\n",
       "   -0.15211167931556702,\n",
       "   -0.28180721402168274,\n",
       "   0.12234950065612793,\n",
       "   -0.6682173013687134,\n",
       "   -0.524705171585083,\n",
       "   -0.06638241559267044,\n",
       "   -0.11201834678649902,\n",
       "   0.38740986585617065,\n",
       "   0.21633337438106537,\n",
       "   0.3654482960700989,\n",
       "   0.008620252832770348,\n",
       "   -0.30683809518814087,\n",
       "   0.27099812030792236,\n",
       "   -0.3368167281150818,\n",
       "   -0.331899493932724,\n",
       "   -0.039687078446149826,\n",
       "   -0.0962386280298233,\n",
       "   0.4754260778427124,\n",
       "   -0.24192798137664795,\n",
       "   -0.02896847575902939,\n",
       "   -0.250399649143219,\n",
       "   -0.5193383097648621,\n",
       "   -0.09433333575725555,\n",
       "   0.08773963153362274,\n",
       "   -0.14761343598365784,\n",
       "   -0.13884548842906952,\n",
       "   0.1504020392894745,\n",
       "   -0.2054920494556427,\n",
       "   -0.15395484864711761,\n",
       "   -0.5282682776451111,\n",
       "   -0.37486571073532104,\n",
       "   -0.020211681723594666,\n",
       "   0.5675984025001526,\n",
       "   -0.20962363481521606,\n",
       "   -0.17298898100852966,\n",
       "   -0.29576876759529114,\n",
       "   -0.5375962257385254,\n",
       "   0.1096682921051979,\n",
       "   0.33787909150123596,\n",
       "   0.4128841161727905,\n",
       "   -0.38690805435180664,\n",
       "   -0.38926181197166443,\n",
       "   -0.43024858832359314,\n",
       "   0.2850137948989868,\n",
       "   -0.11109940707683563,\n",
       "   -0.18036532402038574,\n",
       "   0.14165166020393372,\n",
       "   0.13072484731674194,\n",
       "   0.00870438665151596,\n",
       "   0.14146412909030914,\n",
       "   0.34556880593299866,\n",
       "   -0.0028864555060863495,\n",
       "   0.07913031429052353,\n",
       "   -0.8071608543395996,\n",
       "   0.13351593911647797,\n",
       "   0.32628947496414185,\n",
       "   0.009851431474089622,\n",
       "   0.1400603950023651,\n",
       "   0.22691944241523743,\n",
       "   0.02689717710018158,\n",
       "   -0.07606098800897598,\n",
       "   -0.2973063290119171,\n",
       "   0.2645648121833801,\n",
       "   0.37234485149383545,\n",
       "   0.15931245684623718,\n",
       "   0.23426425457000732,\n",
       "   -0.1092720478773117,\n",
       "   -0.1791340410709381,\n",
       "   -1.0576955080032349,\n",
       "   -0.20106439292430878,\n",
       "   0.1628807932138443,\n",
       "   0.2247885763645172,\n",
       "   0.2396276742219925,\n",
       "   0.42261290550231934,\n",
       "   -0.05976879596710205,\n",
       "   0.037428874522447586,\n",
       "   -0.14040392637252808,\n",
       "   0.42268040776252747,\n",
       "   0.012030556797981262,\n",
       "   0.06350374966859818,\n",
       "   -0.6553401947021484,\n",
       "   -0.21470895409584045,\n",
       "   -0.09395326673984528,\n",
       "   0.19316428899765015,\n",
       "   -0.1602218747138977,\n",
       "   -0.952868640422821,\n",
       "   0.3430088758468628,\n",
       "   -0.10820537805557251,\n",
       "   0.19179247319698334,\n",
       "   0.28331583738327026,\n",
       "   0.14835897088050842,\n",
       "   0.38261181116104126,\n",
       "   -0.048493362963199615,\n",
       "   0.09964102506637573,\n",
       "   0.23071067035198212,\n",
       "   0.31873416900634766,\n",
       "   0.025290990248322487,\n",
       "   0.7659032344818115,\n",
       "   0.3467586040496826,\n",
       "   0.17333121597766876,\n",
       "   -0.4499679207801819,\n",
       "   0.007897360250353813,\n",
       "   0.6057376265525818,\n",
       "   -0.4392393231391907,\n",
       "   -0.6469764113426208,\n",
       "   -0.7218402028083801,\n",
       "   0.24914851784706116,\n",
       "   -0.2867041826248169,\n",
       "   -0.41735029220581055,\n",
       "   0.5088403820991516,\n",
       "   0.0995408147573471,\n",
       "   1.2018712759017944,\n",
       "   -0.05984156206250191,\n",
       "   -0.4512251913547516,\n",
       "   0.4939951002597809,\n",
       "   -0.1493212878704071,\n",
       "   0.48132267594337463,\n",
       "   0.12358320504426956,\n",
       "   0.1816800981760025,\n",
       "   -0.5983604788780212,\n",
       "   -0.14105787873268127,\n",
       "   -0.2880706489086151,\n",
       "   -0.079495869576931,\n",
       "   -0.17483210563659668,\n",
       "   0.008431286551058292,\n",
       "   -0.15261758863925934,\n",
       "   -0.357288122177124,\n",
       "   -0.13240285217761993,\n",
       "   0.3406859040260315,\n",
       "   -0.3324263095855713,\n",
       "   0.524083137512207,\n",
       "   -0.49184563755989075,\n",
       "   0.24798423051834106,\n",
       "   0.13559791445732117,\n",
       "   -0.338329941034317,\n",
       "   0.24623936414718628,\n",
       "   -0.6132384538650513,\n",
       "   0.2957674562931061,\n",
       "   0.2041061520576477,\n",
       "   0.46706193685531616,\n",
       "   -0.19984883069992065,\n",
       "   0.5938975811004639,\n",
       "   0.31613680720329285,\n",
       "   -0.11597704142332077,\n",
       "   -0.06220342218875885,\n",
       "   0.5259623527526855,\n",
       "   0.04496227949857712,\n",
       "   0.11060701310634613,\n",
       "   -0.4093523323535919,\n",
       "   0.10711926221847534,\n",
       "   0.39742210507392883,\n",
       "   0.8131210207939148,\n",
       "   0.06368788331747055,\n",
       "   0.014794528484344482,\n",
       "   0.14655686914920807,\n",
       "   -0.20495331287384033,\n",
       "   0.5492188930511475,\n",
       "   0.16612830758094788,\n",
       "   -0.37950244545936584,\n",
       "   0.9487369060516357,\n",
       "   -0.3462058901786804,\n",
       "   -0.12041103839874268,\n",
       "   0.17074424028396606,\n",
       "   -0.10987572371959686,\n",
       "   0.4347076117992401,\n",
       "   -0.3553902208805084,\n",
       "   0.35323038697242737,\n",
       "   0.16970539093017578,\n",
       "   0.23507069051265717,\n",
       "   0.03813440352678299,\n",
       "   1.064010739326477,\n",
       "   -0.3930937647819519,\n",
       "   0.25800201296806335,\n",
       "   0.08430954813957214,\n",
       "   0.20090588927268982,\n",
       "   0.6940435767173767,\n",
       "   0.20404456555843353,\n",
       "   -0.4077289402484894,\n",
       "   -0.09767672419548035,\n",
       "   0.322909951210022,\n",
       "   -0.005515929311513901,\n",
       "   -0.4482777416706085,\n",
       "   -8.640649795532227,\n",
       "   0.6765819191932678,\n",
       "   -0.10054385662078857,\n",
       "   -0.026228606700897217,\n",
       "   0.13484498858451843,\n",
       "   0.24147164821624756,\n",
       "   0.1712651550769806,\n",
       "   -0.4797505736351013,\n",
       "   -0.21327760815620422,\n",
       "   -0.3225753605365753,\n",
       "   -0.12264908850193024,\n",
       "   -0.017789538949728012,\n",
       "   0.05406741797924042,\n",
       "   -0.08259568363428116,\n",
       "   0.40824267268180847,\n",
       "   -0.3414229154586792,\n",
       "   0.13753485679626465,\n",
       "   0.09191040694713593,\n",
       "   0.6397038102149963,\n",
       "   -0.36878061294555664,\n",
       "   -0.16832688450813293,\n",
       "   0.029922930523753166,\n",
       "   -0.059796661138534546,\n",
       "   0.7439448833465576,\n",
       "   -0.20347003638744354,\n",
       "   0.8705413937568665,\n",
       "   0.35516980290412903,\n",
       "   0.14089396595954895,\n",
       "   0.4472811818122864,\n",
       "   -0.12661133706569672,\n",
       "   -0.2678869366645813,\n",
       "   0.3026532232761383,\n",
       "   -0.07628756761550903,\n",
       "   -0.171426460146904,\n",
       "   -0.013351203873753548,\n",
       "   0.05043773353099823,\n",
       "   0.08545917272567749,\n",
       "   -0.21878409385681152,\n",
       "   -0.3192090690135956,\n",
       "   -0.8778052926063538,\n",
       "   0.23840437829494476,\n",
       "   -0.39747005701065063,\n",
       "   -0.09175175428390503,\n",
       "   0.15258724987506866,\n",
       "   0.5591398477554321,\n",
       "   0.3125271499156952,\n",
       "   -0.030518632382154465,\n",
       "   0.6418731808662415,\n",
       "   0.1253783255815506,\n",
       "   0.2531825304031372,\n",
       "   0.4925342798233032,\n",
       "   0.4097660481929779,\n",
       "   0.20669721066951752,\n",
       "   0.3484809398651123,\n",
       "   -0.12764900922775269,\n",
       "   0.15885601937770844,\n",
       "   -0.39796513319015503,\n",
       "   0.6651549339294434,\n",
       "   0.08404430747032166,\n",
       "   -0.8051371574401855,\n",
       "   -0.15659818053245544,\n",
       "   -0.17412714660167694,\n",
       "   0.6929264664649963,\n",
       "   0.07764751464128494,\n",
       "   -0.4472368359565735,\n",
       "   -0.11799946427345276,\n",
       "   -0.42847347259521484,\n",
       "   -0.07523122429847717,\n",
       "   0.04148444905877113,\n",
       "   -0.5630863904953003,\n",
       "   -0.547252357006073,\n",
       "   -0.26462286710739136,\n",
       "   0.0455714613199234,\n",
       "   -0.35606998205184937,\n",
       "   0.2925111651420593,\n",
       "   -0.6638209819793701,\n",
       "   -0.13172736763954163,\n",
       "   0.10895028710365295,\n",
       "   -0.3436434864997864,\n",
       "   0.3819120228290558,\n",
       "   -0.18354427814483643,\n",
       "   0.340742826461792,\n",
       "   0.4370786249637604,\n",
       "   -0.6676291823387146,\n",
       "   -0.10450470447540283,\n",
       "   0.010519318282604218,\n",
       "   -0.44917309284210205,\n",
       "   0.43528053164482117,\n",
       "   0.256680428981781,\n",
       "   -0.09171672910451889,\n",
       "   -0.26746609807014465,\n",
       "   0.09788913279771805,\n",
       "   -0.05004032328724861,\n",
       "   -0.03508438542485237,\n",
       "   0.07424043118953705,\n",
       "   -0.04703879356384277,\n",
       "   -0.14557474851608276,\n",
       "   0.19091582298278809,\n",
       "   -0.004856843501329422,\n",
       "   0.9004971385002136,\n",
       "   -0.3904283940792084,\n",
       "   -0.20269936323165894,\n",
       "   -0.42893314361572266,\n",
       "   0.09756723791360855,\n",
       "   -0.08464034646749496,\n",
       "   -0.12718740105628967,\n",
       "   0.11188612878322601,\n",
       "   -0.24016311764717102,\n",
       "   -0.011290531605482101,\n",
       "   -0.030287055298686028,\n",
       "   -0.171675905585289,\n",
       "   0.43239498138427734,\n",
       "   0.7464095950126648,\n",
       "   -0.029980648308992386,\n",
       "   -0.0944301187992096,\n",
       "   0.4587255120277405,\n",
       "   0.3875080347061157,\n",
       "   -0.21265193819999695,\n",
       "   -0.07020825147628784,\n",
       "   -0.25009652972221375,\n",
       "   -0.3435051441192627,\n",
       "   -0.6404613852500916,\n",
       "   0.43854349851608276,\n",
       "   -0.3225851356983185,\n",
       "   0.3257301151752472,\n",
       "   0.4143334925174713,\n",
       "   -0.0765669196844101,\n",
       "   0.17829585075378418,\n",
       "   0.707331120967865,\n",
       "   -0.040042977780103683,\n",
       "   0.6580309867858887,\n",
       "   0.1721462458372116,\n",
       "   -0.42926788330078125,\n",
       "   0.13446952402591705,\n",
       "   0.17528893053531647,\n",
       "   -0.21409036219120026,\n",
       "   0.0015285802073776722,\n",
       "   -0.19579240679740906,\n",
       "   -0.35962244868278503,\n",
       "   -0.12266474962234497,\n",
       "   -0.6709003448486328,\n",
       "   -0.013882901519536972,\n",
       "   0.01859479956328869,\n",
       "   0.3536869287490845,\n",
       "   0.6433117985725403,\n",
       "   -0.25192347168922424,\n",
       "   0.06898950040340424,\n",
       "   -0.14182935655117035,\n",
       "   -0.5137295722961426,\n",
       "   -0.4543918967247009,\n",
       "   -0.16873812675476074,\n",
       "   0.18941234052181244,\n",
       "   0.659424901008606,\n",
       "   0.24381333589553833,\n",
       "   0.6103267669677734,\n",
       "   -0.2937886714935303,\n",
       "   0.3371627926826477,\n",
       "   1.0620769262313843,\n",
       "   -0.33909687399864197,\n",
       "   -0.07685013860464096,\n",
       "   -0.20470882952213287,\n",
       "   -0.09979221224784851,\n",
       "   0.28058522939682007,\n",
       "   0.24900402128696442,\n",
       "   -0.14818884432315826,\n",
       "   0.27832865715026855,\n",
       "   -0.150852233171463,\n",
       "   0.1657451093196869,\n",
       "   0.10170666873455048,\n",
       "   0.25096428394317627,\n",
       "   0.05835045501589775,\n",
       "   -0.24396762251853943,\n",
       "   0.3004998564720154,\n",
       "   0.17076005041599274,\n",
       "   0.160955011844635,\n",
       "   -0.2560756504535675,\n",
       "   0.46377456188201904,\n",
       "   0.05866368114948273,\n",
       "   0.20186053216457367,\n",
       "   0.0979144424200058,\n",
       "   0.9664281010627747,\n",
       "   0.5513372421264648,\n",
       "   -0.07509921491146088,\n",
       "   0.029835239052772522,\n",
       "   -0.4979151487350464,\n",
       "   0.03459125757217407,\n",
       "   0.4288221597671509,\n",
       "   0.2616991102695465,\n",
       "   -0.04570060223340988,\n",
       "   -0.10850870609283447,\n",
       "   0.241525799036026,\n",
       "   0.20134226977825165,\n",
       "   -0.3603684902191162,\n",
       "   0.48495909571647644,\n",
       "   -0.12864552438259125,\n",
       "   0.2094232439994812,\n",
       "   -0.22924645245075226,\n",
       "   -0.3565734922885895,\n",
       "   0.14567632973194122,\n",
       "   -0.01447344571352005,\n",
       "   -0.08789684623479843,\n",
       "   -0.1493714153766632,\n",
       "   0.3680245280265808,\n",
       "   -0.32441002130508423,\n",
       "   -0.32569923996925354,\n",
       "   0.41314923763275146,\n",
       "   0.24584859609603882,\n",
       "   -0.16232863068580627,\n",
       "   0.4430101215839386,\n",
       "   0.21592503786087036,\n",
       "   0.28708651661872864],\n",
       "  [0.27048882842063904,\n",
       "   0.15668238699436188,\n",
       "   -0.21538880467414856,\n",
       "   -0.16829907894134521,\n",
       "   0.6598244905471802,\n",
       "   -0.2477933168411255,\n",
       "   0.3411738872528076,\n",
       "   0.01521247997879982,\n",
       "   -0.15180647373199463,\n",
       "   0.2622115910053253,\n",
       "   0.005026179365813732,\n",
       "   0.32607579231262207,\n",
       "   -0.17043796181678772,\n",
       "   0.13932687044143677,\n",
       "   -0.8387951850891113,\n",
       "   0.03736191242933273,\n",
       "   0.28406989574432373,\n",
       "   0.0056013744324445724,\n",
       "   -0.02556324005126953,\n",
       "   0.15819953382015228,\n",
       "   0.070686936378479,\n",
       "   -0.15886613726615906,\n",
       "   0.24196256697177887,\n",
       "   0.05266603082418442,\n",
       "   0.18224576115608215,\n",
       "   0.28754258155822754,\n",
       "   0.32672008872032166,\n",
       "   0.8491672277450562,\n",
       "   -0.17913717031478882,\n",
       "   0.5304561853408813,\n",
       "   0.3548373281955719,\n",
       "   0.2133607417345047,\n",
       "   0.31977444887161255,\n",
       "   -0.024873007088899612,\n",
       "   -0.11033976078033447,\n",
       "   0.038182348012924194,\n",
       "   -0.19125640392303467,\n",
       "   0.2971962094306946,\n",
       "   -0.23124343156814575,\n",
       "   0.12048646807670593,\n",
       "   -0.04031941667199135,\n",
       "   -0.11449278891086578,\n",
       "   -0.02399582415819168,\n",
       "   0.0030201245099306107,\n",
       "   -0.017957545816898346,\n",
       "   -0.13359928131103516,\n",
       "   -0.3251345753669739,\n",
       "   -0.17043496668338776,\n",
       "   0.15771672129631042,\n",
       "   0.326756089925766,\n",
       "   -0.008189426735043526,\n",
       "   -0.5808810591697693,\n",
       "   -0.2482212781906128,\n",
       "   -0.40274733304977417,\n",
       "   -0.06865235418081284,\n",
       "   -0.16874311864376068,\n",
       "   -0.04123423621058464,\n",
       "   -0.1797887533903122,\n",
       "   -0.5146222114562988,\n",
       "   0.3718053698539734,\n",
       "   0.12456692010164261,\n",
       "   0.16852346062660217,\n",
       "   0.6325781941413879,\n",
       "   0.11719648540019989,\n",
       "   -0.18686489760875702,\n",
       "   0.2513582706451416,\n",
       "   0.0014547687023878098,\n",
       "   -0.12507787346839905,\n",
       "   0.1994073987007141,\n",
       "   -0.12600520253181458,\n",
       "   0.009528465569019318,\n",
       "   0.021739423274993896,\n",
       "   -0.03098278120160103,\n",
       "   -0.3972286581993103,\n",
       "   0.4722898304462433,\n",
       "   0.000599004328250885,\n",
       "   0.2881808280944824,\n",
       "   -0.34668928384780884,\n",
       "   -0.3532629907131195,\n",
       "   -0.03210338205099106,\n",
       "   0.406296968460083,\n",
       "   0.2704189121723175,\n",
       "   0.09890913963317871,\n",
       "   0.31415265798568726,\n",
       "   0.2502538561820984,\n",
       "   -0.01030820608139038,\n",
       "   0.1610373556613922,\n",
       "   0.19846490025520325,\n",
       "   0.15697553753852844,\n",
       "   0.34240198135375977,\n",
       "   -0.09883354604244232,\n",
       "   -0.2441992163658142,\n",
       "   0.06396038830280304,\n",
       "   0.16316257417201996,\n",
       "   -0.41740161180496216,\n",
       "   0.24066618084907532,\n",
       "   -0.42625176906585693,\n",
       "   0.22274018824100494,\n",
       "   -0.037530567497015,\n",
       "   0.04393409192562103,\n",
       "   -0.1346745789051056,\n",
       "   -0.19136428833007812,\n",
       "   0.277754545211792,\n",
       "   -0.1386459916830063,\n",
       "   0.1524629443883896,\n",
       "   -0.27373984456062317,\n",
       "   -0.5061299204826355,\n",
       "   -0.7519993185997009,\n",
       "   -0.5665060877799988,\n",
       "   0.35712674260139465,\n",
       "   0.0539153590798378,\n",
       "   0.6686205863952637,\n",
       "   0.4017816185951233,\n",
       "   -0.03640757128596306,\n",
       "   -0.4991784691810608,\n",
       "   -0.12251244485378265,\n",
       "   0.15181922912597656,\n",
       "   0.3484560251235962,\n",
       "   -0.040236491709947586,\n",
       "   -0.39091286063194275,\n",
       "   0.4183999300003052,\n",
       "   0.3448886573314667,\n",
       "   -0.025989633053541183,\n",
       "   -0.2264731526374817,\n",
       "   0.027883518487215042,\n",
       "   -0.002520313486456871,\n",
       "   -0.1804353892803192,\n",
       "   -0.14192964136600494,\n",
       "   0.23908860981464386,\n",
       "   0.13966470956802368,\n",
       "   -0.2786043584346771,\n",
       "   -0.41809338331222534,\n",
       "   -0.005834266543388367,\n",
       "   0.4547346830368042,\n",
       "   0.048731472343206406,\n",
       "   0.12085488438606262,\n",
       "   -0.3297218084335327,\n",
       "   0.03415285050868988,\n",
       "   -0.9670751690864563,\n",
       "   0.07584885507822037,\n",
       "   -0.4497148394584656,\n",
       "   0.10873787850141525,\n",
       "   0.46784070134162903,\n",
       "   -0.16284002363681793,\n",
       "   -0.040602173656225204,\n",
       "   0.08491206169128418,\n",
       "   -0.20084317028522491,\n",
       "   -0.09557748585939407,\n",
       "   0.39163699746131897,\n",
       "   0.29938554763793945,\n",
       "   -0.3853532075881958,\n",
       "   0.18919922411441803,\n",
       "   0.04875348135828972,\n",
       "   0.06813344359397888,\n",
       "   -0.2973157465457916,\n",
       "   -0.11096100509166718,\n",
       "   0.27128055691719055,\n",
       "   -0.7036793828010559,\n",
       "   0.8165761232376099,\n",
       "   0.24340420961380005,\n",
       "   -0.2943015694618225,\n",
       "   -0.4595538079738617,\n",
       "   -0.4243869483470917,\n",
       "   0.3639110326766968,\n",
       "   0.2537534236907959,\n",
       "   -0.6121051907539368,\n",
       "   -0.18234893679618835,\n",
       "   0.2944418787956238,\n",
       "   -0.11774703860282898,\n",
       "   0.6756159067153931,\n",
       "   0.4874635636806488,\n",
       "   0.23089948296546936,\n",
       "   -0.23773224651813507,\n",
       "   -0.06251433491706848,\n",
       "   0.16241158545017242,\n",
       "   0.33460384607315063,\n",
       "   0.2555679976940155,\n",
       "   -0.24654430150985718,\n",
       "   0.30701249837875366,\n",
       "   0.049567561596632004,\n",
       "   0.05037695914506912,\n",
       "   -0.044868871569633484,\n",
       "   -0.27035531401634216,\n",
       "   -0.21052199602127075,\n",
       "   -0.13134129345417023,\n",
       "   -0.2342880368232727,\n",
       "   0.14279894530773163,\n",
       "   -0.7091218829154968,\n",
       "   0.6887481808662415,\n",
       "   -0.07793465256690979,\n",
       "   0.07293061912059784,\n",
       "   -0.3115408420562744,\n",
       "   -0.717670202255249,\n",
       "   -0.10344752669334412,\n",
       "   -0.8400514125823975,\n",
       "   0.17481443285942078,\n",
       "   0.3153996169567108,\n",
       "   0.20919594168663025,\n",
       "   0.07392898947000504,\n",
       "   0.7409330606460571,\n",
       "   -0.16798067092895508,\n",
       "   0.11457131803035736,\n",
       "   -0.06842207908630371,\n",
       "   0.07183709740638733,\n",
       "   0.056102901697158813,\n",
       "   0.021549005061388016,\n",
       "   -0.36041802167892456,\n",
       "   0.5176116228103638,\n",
       "   -0.18345880508422852,\n",
       "   -0.41684260964393616,\n",
       "   -0.1785300225019455,\n",
       "   -0.34320133924484253,\n",
       "   0.10425800085067749,\n",
       "   0.0008237133733928204,\n",
       "   0.3267163038253784,\n",
       "   -0.38263264298439026,\n",
       "   0.17936156690120697,\n",
       "   -0.3918709456920624,\n",
       "   0.1737876832485199,\n",
       "   0.08455314487218857,\n",
       "   -0.38394588232040405,\n",
       "   -0.022221092134714127,\n",
       "   0.6806154251098633,\n",
       "   -0.13604913651943207,\n",
       "   -0.035000093281269073,\n",
       "   0.7893763780593872,\n",
       "   -0.4820171594619751,\n",
       "   0.44465166330337524,\n",
       "   0.4689665138721466,\n",
       "   -0.015163782984018326,\n",
       "   0.19094829261302948,\n",
       "   0.280339777469635,\n",
       "   -0.026473768055438995,\n",
       "   -0.3884854316711426,\n",
       "   -0.2805354595184326,\n",
       "   0.5652726292610168,\n",
       "   -0.1907440721988678,\n",
       "   -0.18350937962532043,\n",
       "   -0.2625311017036438,\n",
       "   -0.5773026347160339,\n",
       "   0.1741524338722229,\n",
       "   -0.0051117222756147385,\n",
       "   0.6079856753349304,\n",
       "   -0.05640938878059387,\n",
       "   -0.17008359730243683,\n",
       "   -0.11082781851291656,\n",
       "   -0.111411914229393,\n",
       "   0.40516868233680725,\n",
       "   -0.11774927377700806,\n",
       "   0.2437855750322342,\n",
       "   -0.3240917921066284,\n",
       "   -0.07126999646425247,\n",
       "   -0.38785943388938904,\n",
       "   -0.05410373955965042,\n",
       "   0.23277148604393005,\n",
       "   -0.21814347803592682,\n",
       "   0.5615978240966797,\n",
       "   -0.111088827252388,\n",
       "   0.05532706528902054,\n",
       "   -0.5107293725013733,\n",
       "   0.5725882649421692,\n",
       "   0.013936586678028107,\n",
       "   0.2869584858417511,\n",
       "   -0.16210483014583588,\n",
       "   -0.07882015407085419,\n",
       "   -0.11265036463737488,\n",
       "   -0.4859987497329712,\n",
       "   -0.04930415749549866,\n",
       "   -0.5126750469207764,\n",
       "   0.4313805103302002,\n",
       "   0.0945458710193634,\n",
       "   -0.09012643992900848,\n",
       "   0.3865426778793335,\n",
       "   0.14490292966365814,\n",
       "   -0.4029698669910431,\n",
       "   0.547650158405304,\n",
       "   0.31057676672935486,\n",
       "   -0.4348991811275482,\n",
       "   0.13938415050506592,\n",
       "   -0.10260429978370667,\n",
       "   0.3583798408508301,\n",
       "   0.6478510499000549,\n",
       "   -0.3321206867694855,\n",
       "   0.5317215919494629,\n",
       "   -0.5623785257339478,\n",
       "   -0.5293629169464111,\n",
       "   0.16154280304908752,\n",
       "   0.7328402400016785,\n",
       "   0.141525000333786,\n",
       "   -0.11595301330089569,\n",
       "   0.04775012657046318,\n",
       "   -0.3396628797054291,\n",
       "   -0.277638703584671,\n",
       "   0.5422871112823486,\n",
       "   0.3527224063873291,\n",
       "   0.06423938274383545,\n",
       "   -0.3293793499469757,\n",
       "   0.5074755549430847,\n",
       "   -0.4263398349285126,\n",
       "   -0.27202558517456055,\n",
       "   0.2031649500131607,\n",
       "   -0.025758568197488785,\n",
       "   0.3303121328353882,\n",
       "   0.04340135678648949,\n",
       "   -0.6946112513542175,\n",
       "   0.6312726736068726,\n",
       "   0.10478416085243225,\n",
       "   0.2238139510154724,\n",
       "   0.24453742802143097,\n",
       "   0.4900628626346588,\n",
       "   0.4695613384246826,\n",
       "   0.22519263625144958,\n",
       "   -0.10173351317644119,\n",
       "   -0.03345026820898056,\n",
       "   -0.14090293645858765,\n",
       "   -0.3817378878593445,\n",
       "   -0.049033477902412415,\n",
       "   0.01266530156135559,\n",
       "   0.0009606695966795087,\n",
       "   0.03062661737203598,\n",
       "   -0.17692913115024567,\n",
       "   0.4990820586681366,\n",
       "   -0.4158218801021576,\n",
       "   0.22434157133102417,\n",
       "   -0.056173764169216156,\n",
       "   0.13888545334339142,\n",
       "   -0.0936974436044693,\n",
       "   0.48757636547088623,\n",
       "   -0.10114537924528122,\n",
       "   -0.2252216935157776,\n",
       "   -0.10824259370565414,\n",
       "   -0.10733204334974289,\n",
       "   -0.10540477931499481,\n",
       "   0.3956373333930969,\n",
       "   -0.11213946342468262,\n",
       "   0.06164179742336273,\n",
       "   0.024327268823981285,\n",
       "   0.4701434075832367,\n",
       "   -0.5488744378089905,\n",
       "   0.25769585371017456,\n",
       "   0.015597090125083923,\n",
       "   -0.32097187638282776,\n",
       "   0.11687380820512772,\n",
       "   -0.3522021770477295,\n",
       "   -0.11630921065807343,\n",
       "   0.3112383186817169,\n",
       "   0.39252224564552307,\n",
       "   0.036316901445388794,\n",
       "   0.3067031502723694,\n",
       "   -0.23669517040252686,\n",
       "   -0.3549073040485382,\n",
       "   0.08126820623874664,\n",
       "   -0.1726457178592682,\n",
       "   -0.2919338345527649,\n",
       "   0.37779000401496887,\n",
       "   0.4306500554084778,\n",
       "   0.0679822787642479,\n",
       "   0.09067767858505249,\n",
       "   0.1806594580411911,\n",
       "   -0.12497542798519135,\n",
       "   -0.1802050769329071,\n",
       "   -0.2707486152648926,\n",
       "   0.05156542733311653,\n",
       "   -0.09962435066699982,\n",
       "   -0.20803344249725342,\n",
       "   -0.05709008872509003,\n",
       "   -0.5023210048675537,\n",
       "   -0.2920582592487335,\n",
       "   0.06015939265489578,\n",
       "   0.234599307179451,\n",
       "   0.5431784391403198,\n",
       "   -0.36660635471343994,\n",
       "   0.5095207691192627,\n",
       "   0.326203316450119,\n",
       "   0.3090534210205078,\n",
       "   -0.036784905940294266,\n",
       "   -0.02421312779188156,\n",
       "   0.2630685567855835,\n",
       "   0.1829444169998169,\n",
       "   -0.8472188711166382,\n",
       "   0.18613335490226746,\n",
       "   0.010790080763399601,\n",
       "   -0.19137951731681824,\n",
       "   0.04862593859434128,\n",
       "   0.2203270047903061,\n",
       "   -0.07183398306369781,\n",
       "   0.058266494423151016,\n",
       "   -0.30758628249168396,\n",
       "   0.5217296481132507,\n",
       "   -0.29008084535598755,\n",
       "   0.28204184770584106,\n",
       "   -0.05290248617529869,\n",
       "   0.29706698656082153,\n",
       "   -0.36826813220977783,\n",
       "   0.1739921122789383,\n",
       "   0.03266344964504242,\n",
       "   -0.6018846035003662,\n",
       "   0.5473195910453796,\n",
       "   0.256028950214386,\n",
       "   -0.34285035729408264,\n",
       "   0.16273319721221924,\n",
       "   0.25029048323631287,\n",
       "   -0.21448498964309692,\n",
       "   0.2665656805038452,\n",
       "   -0.6228356957435608,\n",
       "   -0.32255417108535767,\n",
       "   -0.01614363305270672,\n",
       "   0.42706790566444397,\n",
       "   0.013884201645851135,\n",
       "   -0.9283338785171509,\n",
       "   0.05445309728384018,\n",
       "   -0.44055861234664917,\n",
       "   0.2837667167186737,\n",
       "   0.8677456378936768,\n",
       "   0.18354594707489014,\n",
       "   -0.1533302515745163,\n",
       "   0.1767890453338623,\n",
       "   -0.2131185084581375,\n",
       "   0.0284561850130558,\n",
       "   0.22995416820049286,\n",
       "   -0.1682247817516327,\n",
       "   -0.4777423143386841,\n",
       "   -0.25052109360694885,\n",
       "   -0.24041514098644257,\n",
       "   0.4411170780658722,\n",
       "   0.20260190963745117,\n",
       "   -0.06036429852247238,\n",
       "   -0.24747240543365479,\n",
       "   -0.21246159076690674,\n",
       "   -0.003535378724336624,\n",
       "   -0.062243953347206116,\n",
       "   -0.07603625953197479,\n",
       "   0.06815024465322495,\n",
       "   0.3610630929470062,\n",
       "   -0.019025705754756927,\n",
       "   0.23208864033222198,\n",
       "   -0.05716833472251892,\n",
       "   0.18874669075012207,\n",
       "   0.0522199422121048,\n",
       "   -0.2804722189903259,\n",
       "   0.1886340230703354,\n",
       "   -0.5492249727249146,\n",
       "   0.05304449051618576,\n",
       "   -0.8842456340789795,\n",
       "   0.2214246690273285,\n",
       "   -0.26619696617126465,\n",
       "   -0.17663222551345825,\n",
       "   0.24325688183307648,\n",
       "   -0.1293053776025772,\n",
       "   -0.1192958801984787,\n",
       "   0.43411147594451904,\n",
       "   0.09164785593748093,\n",
       "   0.3453882932662964,\n",
       "   0.20361290872097015,\n",
       "   0.34344208240509033,\n",
       "   0.08104631304740906,\n",
       "   -0.362141489982605,\n",
       "   -0.13482366502285004,\n",
       "   -0.6124111413955688,\n",
       "   0.16778507828712463,\n",
       "   -0.24922002851963043,\n",
       "   0.45567256212234497,\n",
       "   -2.1051615476608276e-05,\n",
       "   0.2646808624267578,\n",
       "   0.21099495887756348,\n",
       "   0.3289880156517029,\n",
       "   -0.29379817843437195,\n",
       "   0.785909116268158,\n",
       "   -0.06815672665834427,\n",
       "   0.26211491227149963,\n",
       "   0.09641854465007782,\n",
       "   0.3679993450641632,\n",
       "   0.13283678889274597,\n",
       "   0.9933955669403076,\n",
       "   -0.0377853587269783,\n",
       "   0.3191249370574951,\n",
       "   0.1793186515569687,\n",
       "   -0.1938074380159378,\n",
       "   -0.3432970941066742,\n",
       "   0.22006309032440186,\n",
       "   -0.3294242322444916,\n",
       "   0.16755278408527374,\n",
       "   -0.3149670660495758,\n",
       "   0.03364831581711769,\n",
       "   0.09576763212680817,\n",
       "   0.2709932327270508,\n",
       "   0.9067676067352295,\n",
       "   0.24268914759159088,\n",
       "   0.11451928317546844,\n",
       "   0.4106091558933258,\n",
       "   0.27445411682128906,\n",
       "   0.1070580929517746,\n",
       "   -0.036566317081451416,\n",
       "   -0.22545552253723145,\n",
       "   0.20897866785526276,\n",
       "   0.4915682375431061,\n",
       "   0.08622294664382935,\n",
       "   0.22100022435188293,\n",
       "   0.4244522154331207,\n",
       "   0.1381082534790039,\n",
       "   -0.2959927022457123,\n",
       "   0.04071458429098129,\n",
       "   -0.1375792920589447,\n",
       "   0.16460271179676056,\n",
       "   -0.6933707594871521,\n",
       "   0.46627792716026306,\n",
       "   -0.02890990674495697,\n",
       "   0.06575145572423935,\n",
       "   -0.03242678940296173,\n",
       "   0.0625639259815216,\n",
       "   -0.13140760362148285,\n",
       "   -0.5594700574874878,\n",
       "   -0.3002561330795288,\n",
       "   -0.1959657371044159,\n",
       "   -0.13473694026470184,\n",
       "   0.18794313073158264,\n",
       "   0.4960445463657379,\n",
       "   0.1064373254776001,\n",
       "   0.12519939243793488,\n",
       "   -0.039610132575035095,\n",
       "   0.3903627395629883,\n",
       "   0.10821374505758286,\n",
       "   -0.17056848108768463,\n",
       "   0.08031827211380005,\n",
       "   0.6037554144859314,\n",
       "   -0.2539169192314148,\n",
       "   0.4437233805656433,\n",
       "   0.26769334077835083,\n",
       "   -0.16261065006256104,\n",
       "   -0.01800675503909588,\n",
       "   -0.3149082064628601,\n",
       "   -0.09894723445177078,\n",
       "   0.13392452895641327,\n",
       "   -0.37458521127700806,\n",
       "   1.1765952110290527,\n",
       "   0.2271745502948761,\n",
       "   0.1342248022556305,\n",
       "   -0.14145687222480774,\n",
       "   -0.052575185894966125,\n",
       "   -0.12496055662631989,\n",
       "   0.0035109780728816986,\n",
       "   0.7553287744522095,\n",
       "   0.10820417106151581,\n",
       "   0.311308354139328,\n",
       "   0.007499021477997303,\n",
       "   0.4806961715221405,\n",
       "   0.10182812809944153,\n",
       "   0.5311688780784607,\n",
       "   0.06917556375265121,\n",
       "   0.1388351023197174,\n",
       "   0.3566824197769165,\n",
       "   0.4060057997703552,\n",
       "   -0.7701284885406494,\n",
       "   0.43623116612434387,\n",
       "   0.6099734902381897,\n",
       "   0.5601496696472168,\n",
       "   -0.5176160335540771,\n",
       "   -8.882248878479004,\n",
       "   0.08591821044683456,\n",
       "   0.1900283843278885,\n",
       "   0.0005192682147026062,\n",
       "   0.06014145165681839,\n",
       "   0.15132874250411987,\n",
       "   0.33512240648269653,\n",
       "   -0.21233180165290833,\n",
       "   0.3100105822086334,\n",
       "   0.169849693775177,\n",
       "   -0.0912327915430069,\n",
       "   -0.16460028290748596,\n",
       "   0.4975756108760834,\n",
       "   -0.5497260689735413,\n",
       "   0.03979521244764328,\n",
       "   0.0012767873704433441,\n",
       "   -0.14203625917434692,\n",
       "   -0.06944110244512558,\n",
       "   0.16114647686481476,\n",
       "   -0.2232375144958496,\n",
       "   -0.619208574295044,\n",
       "   -0.029661839827895164,\n",
       "   0.1993045061826706,\n",
       "   0.2560461759567261,\n",
       "   0.0004926864057779312,\n",
       "   0.23917806148529053,\n",
       "   -0.25220322608947754,\n",
       "   -0.19692444801330566,\n",
       "   -0.06634706258773804,\n",
       "   -0.37312960624694824,\n",
       "   -0.24087361991405487,\n",
       "   -0.21920917928218842,\n",
       "   -0.19716203212738037,\n",
       "   -0.015186378732323647,\n",
       "   0.048405490815639496,\n",
       "   -0.25037455558776855,\n",
       "   0.10146407783031464,\n",
       "   0.17023959755897522,\n",
       "   -0.14209255576133728,\n",
       "   -0.37562909722328186,\n",
       "   -0.23042599856853485,\n",
       "   0.16234354674816132,\n",
       "   0.43024885654449463,\n",
       "   -0.10344865918159485,\n",
       "   0.19775836169719696,\n",
       "   -0.11231047660112381,\n",
       "   0.42236605286598206,\n",
       "   0.23827402293682098,\n",
       "   0.0877867043018341,\n",
       "   0.4836825430393219,\n",
       "   0.5206185579299927,\n",
       "   0.30554211139678955,\n",
       "   -0.06565070152282715,\n",
       "   -0.16468879580497742,\n",
       "   -0.5692193508148193,\n",
       "   -0.11385369300842285,\n",
       "   -0.13268157839775085,\n",
       "   0.3871883153915405,\n",
       "   -0.3004986047744751,\n",
       "   0.0009195189923048019,\n",
       "   -0.18314877152442932,\n",
       "   -0.4069945216178894,\n",
       "   0.4006212651729584,\n",
       "   -0.32803577184677124,\n",
       "   -0.3620864152908325,\n",
       "   0.06971746683120728,\n",
       "   -0.6421961784362793,\n",
       "   0.14855174720287323,\n",
       "   0.06987293064594269,\n",
       "   -0.08107934147119522,\n",
       "   -0.5309675335884094,\n",
       "   -0.5628448128700256,\n",
       "   -0.17616555094718933,\n",
       "   0.2976260185241699,\n",
       "   -0.04058816283941269,\n",
       "   -0.4840904176235199,\n",
       "   -0.6053866744041443,\n",
       "   0.1538715362548828,\n",
       "   -0.31440311670303345,\n",
       "   0.019195783883333206,\n",
       "   -0.8628641963005066,\n",
       "   0.8346047401428223,\n",
       "   0.5519159436225891,\n",
       "   0.005833368748426437,\n",
       "   -0.08096825331449509,\n",
       "   -0.32482463121414185,\n",
       "   -0.15598826110363007,\n",
       "   0.15913935005664825,\n",
       "   -0.0631980448961258,\n",
       "   0.34587883949279785,\n",
       "   0.0007768739014863968,\n",
       "   -0.34493711590766907,\n",
       "   -0.1980772316455841,\n",
       "   -0.05000728368759155,\n",
       "   -0.11227402836084366,\n",
       "   0.05673525854945183,\n",
       "   -0.3678133487701416,\n",
       "   -0.3305041491985321,\n",
       "   0.3967896103858948,\n",
       "   -0.360233336687088,\n",
       "   -0.637210488319397,\n",
       "   -0.12956315279006958,\n",
       "   -0.2628238797187805,\n",
       "   0.10332442075014114,\n",
       "   -0.20214857161045074,\n",
       "   -0.3992138206958771,\n",
       "   0.3106656074523926,\n",
       "   -0.47039955854415894,\n",
       "   0.005399979650974274,\n",
       "   -0.16022615134716034,\n",
       "   0.12886953353881836,\n",
       "   0.2643311321735382,\n",
       "   0.40629592537879944,\n",
       "   -0.5570684671401978,\n",
       "   -0.2315569967031479,\n",
       "   0.042844921350479126,\n",
       "   0.9930726289749146,\n",
       "   0.010312788188457489,\n",
       "   0.05796203762292862,\n",
       "   -0.052084580063819885,\n",
       "   -0.07781298458576202,\n",
       "   -0.5032777786254883,\n",
       "   0.24828630685806274,\n",
       "   0.3395995497703552,\n",
       "   0.16209396719932556,\n",
       "   0.0548451729118824,\n",
       "   0.04987574741244316,\n",
       "   0.050221171230077744,\n",
       "   0.09724533557891846,\n",
       "   -0.142594113945961,\n",
       "   0.5513696670532227,\n",
       "   0.011150011792778969,\n",
       "   -0.011863231658935547,\n",
       "   0.3522876501083374,\n",
       "   0.23027706146240234,\n",
       "   -0.2735130190849304,\n",
       "   0.04976127669215202,\n",
       "   -0.39139819145202637,\n",
       "   -0.3082929849624634,\n",
       "   -0.10738881677389145,\n",
       "   -0.5475694537162781,\n",
       "   -0.04023740068078041,\n",
       "   0.4157305061817169,\n",
       "   0.06600753962993622,\n",
       "   0.2638888955116272,\n",
       "   -0.4750365912914276,\n",
       "   -0.053101446479558945,\n",
       "   0.13399064540863037,\n",
       "   -0.14963509142398834,\n",
       "   0.12503564357757568,\n",
       "   -0.44539982080459595,\n",
       "   -0.020008299499750137,\n",
       "   0.10009926557540894,\n",
       "   -0.0896279364824295,\n",
       "   0.28157687187194824,\n",
       "   -0.14359794557094574,\n",
       "   0.1987595409154892,\n",
       "   0.06333211064338684,\n",
       "   -0.23100945353507996,\n",
       "   0.05210784822702408,\n",
       "   -0.30702123045921326,\n",
       "   0.12845632433891296,\n",
       "   -0.019365135580301285,\n",
       "   0.1084027886390686,\n",
       "   -0.28543156385421753,\n",
       "   0.09791971743106842,\n",
       "   -0.3957374095916748,\n",
       "   0.32370325922966003,\n",
       "   0.4067331552505493,\n",
       "   -0.30319055914878845,\n",
       "   0.26429131627082825,\n",
       "   0.18984995782375336,\n",
       "   -0.14861294627189636,\n",
       "   0.22332756221294403,\n",
       "   0.02932230569422245,\n",
       "   -0.314860999584198,\n",
       "   0.1290040910243988,\n",
       "   -0.5674054622650146,\n",
       "   -0.3473992943763733,\n",
       "   -0.16314825415611267,\n",
       "   0.10660892724990845,\n",
       "   -0.20729099214076996,\n",
       "   -0.45603474974632263,\n",
       "   -0.1481599509716034,\n",
       "   -0.2616686522960663,\n",
       "   0.14073702692985535,\n",
       "   0.027410905808210373,\n",
       "   0.15168026089668274,\n",
       "   0.15766370296478271,\n",
       "   0.17745590209960938,\n",
       "   -0.04731924831867218,\n",
       "   0.2201441377401352,\n",
       "   -0.6324775218963623,\n",
       "   0.07844553887844086,\n",
       "   -0.05542369186878204,\n",
       "   -0.06431832909584045,\n",
       "   -0.14774125814437866,\n",
       "   0.2943076193332672,\n",
       "   -0.2886044979095459,\n",
       "   0.04944427311420441,\n",
       "   -0.008628014475107193,\n",
       "   -0.16418009996414185,\n",
       "   -0.3013007342815399,\n",
       "   -0.41882896423339844,\n",
       "   0.10776576399803162,\n",
       "   -0.4128091335296631,\n",
       "   0.10787126421928406,\n",
       "   -0.05177198350429535,\n",
       "   -0.08718061447143555,\n",
       "   0.3165351152420044,\n",
       "   -0.023058395832777023],\n",
       "  [0.9966162443161011,\n",
       "   0.5051445960998535,\n",
       "   0.07963529229164124,\n",
       "   1.06462562084198,\n",
       "   -0.08048916608095169,\n",
       "   -0.6290382146835327,\n",
       "   0.4297933578491211,\n",
       "   0.47899535298347473,\n",
       "   0.11691686511039734,\n",
       "   0.04389828443527222,\n",
       "   -0.25387927889823914,\n",
       "   0.21233008801937103,\n",
       "   -1.1743441820144653,\n",
       "   -0.17467398941516876,\n",
       "   -1.2236237525939941,\n",
       "   -0.5252819061279297,\n",
       "   -0.2666274905204773,\n",
       "   0.008942266926169395,\n",
       "   0.4210275411605835,\n",
       "   0.49823611974716187,\n",
       "   -0.08733522891998291,\n",
       "   0.22606495022773743,\n",
       "   -0.47304123640060425,\n",
       "   -0.037924766540527344,\n",
       "   0.22226616740226746,\n",
       "   -0.34191587567329407,\n",
       "   0.6340200901031494,\n",
       "   2.0982697010040283,\n",
       "   0.02851906418800354,\n",
       "   1.1393636465072632,\n",
       "   0.21930690109729767,\n",
       "   -0.004972273483872414,\n",
       "   0.48801857233047485,\n",
       "   0.5020011067390442,\n",
       "   -0.5341134071350098,\n",
       "   0.4986744523048401,\n",
       "   -0.32173752784729004,\n",
       "   0.7203016877174377,\n",
       "   -0.7215859889984131,\n",
       "   0.5728512406349182,\n",
       "   -0.1572558879852295,\n",
       "   0.41489529609680176,\n",
       "   0.7037241458892822,\n",
       "   -0.8287929892539978,\n",
       "   0.719713568687439,\n",
       "   -0.5470277667045593,\n",
       "   -0.6034747362136841,\n",
       "   0.5604385137557983,\n",
       "   -1.074593424797058,\n",
       "   1.184788703918457,\n",
       "   -0.10053831338882446,\n",
       "   -0.7975971102714539,\n",
       "   0.21009503304958344,\n",
       "   0.42202675342559814,\n",
       "   0.7596243619918823,\n",
       "   -0.6827675104141235,\n",
       "   -0.1840956062078476,\n",
       "   0.28066638112068176,\n",
       "   -0.4203248918056488,\n",
       "   0.28198689222335815,\n",
       "   -0.3584112823009491,\n",
       "   0.11121618747711182,\n",
       "   1.0599952936172485,\n",
       "   0.26284781098365784,\n",
       "   -1.1338155269622803,\n",
       "   0.2574000358581543,\n",
       "   0.7782740592956543,\n",
       "   -0.5635043978691101,\n",
       "   -0.1303001046180725,\n",
       "   -0.1581408977508545,\n",
       "   1.077977180480957,\n",
       "   0.08295099437236786,\n",
       "   -0.10979306697845459,\n",
       "   -0.7740588188171387,\n",
       "   0.28531190752983093,\n",
       "   -0.8226649761199951,\n",
       "   0.23389557003974915,\n",
       "   -0.17730695009231567,\n",
       "   -0.6278398633003235,\n",
       "   0.5158253908157349,\n",
       "   0.6460168361663818,\n",
       "   0.5094989538192749,\n",
       "   -0.6028514504432678,\n",
       "   -0.25484856963157654,\n",
       "   0.9487494826316833,\n",
       "   1.5683306455612183,\n",
       "   0.061843760311603546,\n",
       "   -0.8710119128227234,\n",
       "   -0.06904290616512299,\n",
       "   -0.25585487484931946,\n",
       "   0.43586036562919617,\n",
       "   -0.3820737302303314,\n",
       "   0.1390354186296463,\n",
       "   -0.056464795023202896,\n",
       "   -0.1614113748073578,\n",
       "   -0.07046891003847122,\n",
       "   -0.8051484227180481,\n",
       "   -0.0670960545539856,\n",
       "   -0.6068912744522095,\n",
       "   0.43885019421577454,\n",
       "   -0.7309326529502869,\n",
       "   0.36923304200172424,\n",
       "   0.127871572971344,\n",
       "   0.6440837979316711,\n",
       "   0.35313916206359863,\n",
       "   -0.23661693930625916,\n",
       "   -0.1198841780424118,\n",
       "   -1.7992644309997559,\n",
       "   -0.14372342824935913,\n",
       "   1.2027790546417236,\n",
       "   -0.5309547781944275,\n",
       "   0.32936832308769226,\n",
       "   -0.02316509187221527,\n",
       "   -0.8793085217475891,\n",
       "   -0.2826245427131653,\n",
       "   0.5906851291656494,\n",
       "   0.16178828477859497,\n",
       "   0.24590596556663513,\n",
       "   0.15462487936019897,\n",
       "   0.23513983190059662,\n",
       "   0.31806305050849915,\n",
       "   0.6830692291259766,\n",
       "   -0.4423272907733917,\n",
       "   0.2431076467037201,\n",
       "   0.22383832931518555,\n",
       "   0.6882485747337341,\n",
       "   -0.7403786778450012,\n",
       "   -0.7198994159698486,\n",
       "   0.6861613988876343,\n",
       "   0.5845159888267517,\n",
       "   -0.359189510345459,\n",
       "   -0.34829309582710266,\n",
       "   0.854824960231781,\n",
       "   0.6806472539901733,\n",
       "   0.5622522830963135,\n",
       "   -0.3217059075832367,\n",
       "   0.021657079458236694,\n",
       "   0.14444074034690857,\n",
       "   -5.382903575897217,\n",
       "   -0.7796133756637573,\n",
       "   -0.8941161036491394,\n",
       "   -0.005599889904260635,\n",
       "   1.3077343702316284,\n",
       "   -0.6629658937454224,\n",
       "   0.08606022596359253,\n",
       "   0.20400282740592957,\n",
       "   -0.38359642028808594,\n",
       "   0.4012024998664856,\n",
       "   -0.4312450885772705,\n",
       "   0.3963763415813446,\n",
       "   -0.6387618780136108,\n",
       "   -0.24249863624572754,\n",
       "   0.039793405681848526,\n",
       "   -0.8439098000526428,\n",
       "   -0.43808513879776,\n",
       "   -0.5735242366790771,\n",
       "   0.2764878273010254,\n",
       "   0.055801864713430405,\n",
       "   0.5104990601539612,\n",
       "   0.05119258165359497,\n",
       "   -0.628990113735199,\n",
       "   -0.7949182987213135,\n",
       "   -0.40857815742492676,\n",
       "   -0.5195648074150085,\n",
       "   0.5105634927749634,\n",
       "   -0.6848462224006653,\n",
       "   0.36681535840034485,\n",
       "   0.20423364639282227,\n",
       "   -1.0965383052825928,\n",
       "   1.0480024814605713,\n",
       "   0.4561798870563507,\n",
       "   -0.8430494070053101,\n",
       "   -0.817780613899231,\n",
       "   0.08612050116062164,\n",
       "   0.596510648727417,\n",
       "   0.88656085729599,\n",
       "   0.34649065136909485,\n",
       "   0.001387283205986023,\n",
       "   0.1656961590051651,\n",
       "   -0.5799510478973389,\n",
       "   0.21883124113082886,\n",
       "   0.40203702449798584,\n",
       "   -0.2286028265953064,\n",
       "   -0.42904579639434814,\n",
       "   -0.363252729177475,\n",
       "   -0.7682754397392273,\n",
       "   0.1522885411977768,\n",
       "   0.37369173765182495,\n",
       "   1.640663504600525,\n",
       "   0.11247216165065765,\n",
       "   0.24283479154109955,\n",
       "   -0.6825174689292908,\n",
       "   -1.2728897333145142,\n",
       "   0.2696461081504822,\n",
       "   -0.5900673270225525,\n",
       "   -0.7516137361526489,\n",
       "   -0.9755032658576965,\n",
       "   -0.15209755301475525,\n",
       "   0.38236892223358154,\n",
       "   1.0102920532226562,\n",
       "   -0.5913684964179993,\n",
       "   -0.01317688450217247,\n",
       "   -0.4437865614891052,\n",
       "   0.06705759465694427,\n",
       "   -0.07608523964881897,\n",
       "   -0.13592170178890228,\n",
       "   -0.30840882658958435,\n",
       "   0.05930577591061592,\n",
       "   -0.30219873785972595,\n",
       "   -1.9560679197311401,\n",
       "   -0.27834486961364746,\n",
       "   0.28361114859580994,\n",
       "   0.46100151538848877,\n",
       "   -0.20101259648799896,\n",
       "   0.26015937328338623,\n",
       "   -0.766426146030426,\n",
       "   -0.599359393119812,\n",
       "   -0.05550771951675415,\n",
       "   0.02436717599630356,\n",
       "   -0.24750059843063354,\n",
       "   0.42929598689079285,\n",
       "   0.021416254341602325,\n",
       "   0.2458343803882599,\n",
       "   -0.8333982229232788,\n",
       "   0.2704000771045685,\n",
       "   0.8741835951805115,\n",
       "   1.235162377357483,\n",
       "   0.4104245901107788,\n",
       "   0.9193057417869568,\n",
       "   0.29443150758743286,\n",
       "   0.9814878106117249,\n",
       "   0.3362222909927368,\n",
       "   -0.33557310700416565,\n",
       "   -1.087821125984192,\n",
       "   -0.2534085512161255,\n",
       "   0.5105876922607422,\n",
       "   0.0939609482884407,\n",
       "   0.4136310815811157,\n",
       "   -0.7450944185256958,\n",
       "   -0.16089186072349548,\n",
       "   -0.33828556537628174,\n",
       "   -0.030273279175162315,\n",
       "   0.7167390584945679,\n",
       "   -0.5249872207641602,\n",
       "   -0.01639607734978199,\n",
       "   0.33758509159088135,\n",
       "   0.03130863606929779,\n",
       "   0.7587047815322876,\n",
       "   0.1140705794095993,\n",
       "   0.37892064452171326,\n",
       "   -0.059159714728593826,\n",
       "   0.009812399744987488,\n",
       "   -0.43507400155067444,\n",
       "   -0.14671236276626587,\n",
       "   0.7179781794548035,\n",
       "   -0.19640085101127625,\n",
       "   0.6415848135948181,\n",
       "   0.3163129687309265,\n",
       "   -0.06818824261426926,\n",
       "   -0.8220934867858887,\n",
       "   0.11204198002815247,\n",
       "   -0.14812231063842773,\n",
       "   0.5145487785339355,\n",
       "   -0.19566583633422852,\n",
       "   -0.16417156159877777,\n",
       "   0.12814447283744812,\n",
       "   -0.27841275930404663,\n",
       "   0.36980319023132324,\n",
       "   -0.4235658347606659,\n",
       "   0.47679486870765686,\n",
       "   0.3625199496746063,\n",
       "   -0.5520097613334656,\n",
       "   -0.044203951954841614,\n",
       "   0.4539165198802948,\n",
       "   -0.07958529144525528,\n",
       "   1.0332523584365845,\n",
       "   0.5768615007400513,\n",
       "   -0.1428653746843338,\n",
       "   -0.3316052556037903,\n",
       "   -1.18876051902771,\n",
       "   -0.275004506111145,\n",
       "   0.5394750833511353,\n",
       "   -0.3295271694660187,\n",
       "   1.3745572566986084,\n",
       "   -0.9100775122642517,\n",
       "   0.6111759543418884,\n",
       "   0.059577763080596924,\n",
       "   0.7276814579963684,\n",
       "   -0.25699788331985474,\n",
       "   -0.09530520439147949,\n",
       "   -0.07372836768627167,\n",
       "   -0.7207301259040833,\n",
       "   -2.1627888679504395,\n",
       "   -0.6804243326187134,\n",
       "   0.33078882098197937,\n",
       "   -0.6422055959701538,\n",
       "   -0.5625901222229004,\n",
       "   0.1534261256456375,\n",
       "   -0.7942054867744446,\n",
       "   -0.2707170844078064,\n",
       "   0.41012972593307495,\n",
       "   0.4917699098587036,\n",
       "   0.10126237571239471,\n",
       "   -0.17397291958332062,\n",
       "   -6.409373760223389,\n",
       "   -0.046590473502874374,\n",
       "   0.36371612548828125,\n",
       "   0.41334861516952515,\n",
       "   -0.5569247603416443,\n",
       "   0.10336073487997055,\n",
       "   1.4915556907653809,\n",
       "   0.3220340311527252,\n",
       "   -0.3418820798397064,\n",
       "   0.5405740141868591,\n",
       "   0.09602446109056473,\n",
       "   -1.0446795225143433,\n",
       "   0.05905492603778839,\n",
       "   -0.29885292053222656,\n",
       "   -0.7720311880111694,\n",
       "   0.2632550895214081,\n",
       "   0.44042202830314636,\n",
       "   1.3648991584777832,\n",
       "   -0.5464106798171997,\n",
       "   0.6322758793830872,\n",
       "   -0.9537038803100586,\n",
       "   -0.06843548268079758,\n",
       "   -0.7914628982543945,\n",
       "   0.9899353384971619,\n",
       "   0.08796007931232452,\n",
       "   -1.2708138227462769,\n",
       "   0.8117467164993286,\n",
       "   -0.5911766290664673,\n",
       "   -0.16479742527008057,\n",
       "   -0.34194058179855347,\n",
       "   -0.8356947898864746,\n",
       "   -0.2520713210105896,\n",
       "   -0.2521525025367737,\n",
       "   0.33939576148986816,\n",
       "   -0.8077856302261353,\n",
       "   0.36378154158592224,\n",
       "   0.9284148216247559,\n",
       "   0.2421356439590454,\n",
       "   -0.0346907377243042,\n",
       "   -0.7579529285430908,\n",
       "   -0.052956294268369675,\n",
       "   -0.05127852037549019,\n",
       "   -0.12210497260093689,\n",
       "   -0.14752301573753357,\n",
       "   -0.3573991358280182,\n",
       "   0.14026370644569397,\n",
       "   -0.47300150990486145,\n",
       "   -0.1428396850824356,\n",
       "   -0.29832229018211365,\n",
       "   -1.038834571838379,\n",
       "   0.2177363485097885,\n",
       "   -0.058680545538663864,\n",
       "   0.3419225513935089,\n",
       "   0.2445373237133026,\n",
       "   -0.4015955924987793,\n",
       "   1.1249370574951172,\n",
       "   0.2955514192581177,\n",
       "   0.23193161189556122,\n",
       "   0.8642780780792236,\n",
       "   -0.26469606161117554,\n",
       "   -1.0354655981063843,\n",
       "   0.07946795225143433,\n",
       "   -0.5151578187942505,\n",
       "   0.009705722332000732,\n",
       "   0.0044503360986709595,\n",
       "   0.7084028124809265,\n",
       "   1.580002784729004,\n",
       "   -0.12735985219478607,\n",
       "   0.12497885525226593,\n",
       "   -0.2114928662776947,\n",
       "   0.42236998677253723,\n",
       "   0.443771094083786,\n",
       "   -0.12323890626430511,\n",
       "   -0.1702251136302948,\n",
       "   -0.37636300921440125,\n",
       "   -2.525871515274048,\n",
       "   0.12171792984008789,\n",
       "   0.20285722613334656,\n",
       "   0.46400758624076843,\n",
       "   -0.3288493752479553,\n",
       "   0.3366893529891968,\n",
       "   0.07762804627418518,\n",
       "   -0.10144290328025818,\n",
       "   -0.07687711715698242,\n",
       "   0.6196882724761963,\n",
       "   0.6219303607940674,\n",
       "   0.7304766178131104,\n",
       "   -0.17463256418704987,\n",
       "   0.4130699932575226,\n",
       "   -0.8491223454475403,\n",
       "   -1.5872979164123535,\n",
       "   -0.26810258626937866,\n",
       "   -0.9491366744041443,\n",
       "   0.4100206196308136,\n",
       "   -0.5752217769622803,\n",
       "   -0.36705780029296875,\n",
       "   -0.7260398864746094,\n",
       "   -0.15508413314819336,\n",
       "   -0.6914818286895752,\n",
       "   0.05365864932537079,\n",
       "   -0.6965555548667908,\n",
       "   -0.4468705654144287,\n",
       "   -0.06434617191553116,\n",
       "   0.5237184166908264,\n",
       "   -0.0462641716003418,\n",
       "   0.2958783507347107,\n",
       "   -0.7005383372306824,\n",
       "   -0.7283102869987488,\n",
       "   0.29247748851776123,\n",
       "   1.0486490726470947,\n",
       "   0.7302676439285278,\n",
       "   -1.5917119979858398,\n",
       "   0.1862296462059021,\n",
       "   0.12528261542320251,\n",
       "   0.5190648436546326,\n",
       "   0.6232797503471375,\n",
       "   0.7696925401687622,\n",
       "   -0.25548845529556274,\n",
       "   0.06644046306610107,\n",
       "   -0.4419972896575928,\n",
       "   1.3128167390823364,\n",
       "   0.49452126026153564,\n",
       "   0.536468505859375,\n",
       "   -0.24784566462039948,\n",
       "   0.029971525073051453,\n",
       "   0.4333288371562958,\n",
       "   -0.009823359549045563,\n",
       "   0.3251959979534149,\n",
       "   -0.6948186159133911,\n",
       "   0.6346617341041565,\n",
       "   -0.79951012134552,\n",
       "   -0.09101232141256332,\n",
       "   -0.6642326712608337,\n",
       "   0.652635395526886,\n",
       "   -0.7232744097709656,\n",
       "   -0.5824548006057739,\n",
       "   -0.022725414484739304,\n",
       "   -0.9558692574501038,\n",
       "   -0.3048463761806488,\n",
       "   -1.6428204774856567,\n",
       "   -0.16110682487487793,\n",
       "   -0.5148823857307434,\n",
       "   -0.11817090958356857,\n",
       "   0.45777809619903564,\n",
       "   0.8849974274635315,\n",
       "   -1.0239152908325195,\n",
       "   -0.1493050456047058,\n",
       "   0.13701850175857544,\n",
       "   -0.19286981225013733,\n",
       "   -0.44693291187286377,\n",
       "   0.5568046569824219,\n",
       "   0.22600245475769043,\n",
       "   0.9051109552383423,\n",
       "   0.09258120507001877,\n",
       "   -0.10196714848279953,\n",
       "   1.275286316871643,\n",
       "   -1.374114990234375,\n",
       "   1.2203530073165894,\n",
       "   -1.0049395561218262,\n",
       "   1.2225936651229858,\n",
       "   5.112767219543457,\n",
       "   0.430319607257843,\n",
       "   0.055199265480041504,\n",
       "   1.0985203981399536,\n",
       "   -0.6836615800857544,\n",
       "   -0.05134095996618271,\n",
       "   -0.47957006096839905,\n",
       "   0.29901936650276184,\n",
       "   1.5985430479049683,\n",
       "   1.5668598413467407,\n",
       "   -0.40182414650917053,\n",
       "   -0.07427611947059631,\n",
       "   -0.543464183807373,\n",
       "   0.5273314714431763,\n",
       "   -0.12276087701320648,\n",
       "   0.6446514129638672,\n",
       "   0.3823085427284241,\n",
       "   0.24053844809532166,\n",
       "   -0.6423047184944153,\n",
       "   -0.4800140857696533,\n",
       "   1.1436489820480347,\n",
       "   0.10598593950271606,\n",
       "   0.776612401008606,\n",
       "   -0.029250819236040115,\n",
       "   -0.016469761729240417,\n",
       "   0.5901609659194946,\n",
       "   0.266898512840271,\n",
       "   0.15016528964042664,\n",
       "   -0.7249331474304199,\n",
       "   0.3941906988620758,\n",
       "   0.4448662996292114,\n",
       "   0.8646466135978699,\n",
       "   -0.058793067932128906,\n",
       "   0.296902060508728,\n",
       "   -0.18510538339614868,\n",
       "   0.3629436790943146,\n",
       "   0.1307165026664734,\n",
       "   -0.3848949074745178,\n",
       "   -0.13766980171203613,\n",
       "   0.458537220954895,\n",
       "   0.09381771832704544,\n",
       "   1.063178300857544,\n",
       "   0.0668175220489502,\n",
       "   -0.5895475745201111,\n",
       "   -0.14019131660461426,\n",
       "   -1.2560514211654663,\n",
       "   -0.4526907205581665,\n",
       "   -1.6102303266525269,\n",
       "   -0.157863050699234,\n",
       "   -0.03284400328993797,\n",
       "   -0.16954617202281952,\n",
       "   0.09797966480255127,\n",
       "   0.9523451924324036,\n",
       "   0.11758297681808472,\n",
       "   0.45506107807159424,\n",
       "   0.1905880719423294,\n",
       "   -0.42796868085861206,\n",
       "   -0.46087563037872314,\n",
       "   -1.6068921089172363,\n",
       "   -0.011250561103224754,\n",
       "   0.10770010948181152,\n",
       "   0.08881397545337677,\n",
       "   0.8807696104049683,\n",
       "   0.6028741598129272,\n",
       "   0.4714268445968628,\n",
       "   0.11569845676422119,\n",
       "   -1.092703104019165,\n",
       "   -0.5552639961242676,\n",
       "   -0.18707239627838135,\n",
       "   -0.18943825364112854,\n",
       "   1.259253978729248,\n",
       "   -0.08302118629217148,\n",
       "   -0.4243992865085602,\n",
       "   0.00464726984500885,\n",
       "   0.12677569687366486,\n",
       "   0.28445279598236084,\n",
       "   0.17360234260559082,\n",
       "   0.9756999611854553,\n",
       "   0.014935825020074844,\n",
       "   -0.3695814609527588,\n",
       "   -0.10539428889751434,\n",
       "   0.4223751425743103,\n",
       "   -0.22875633835792542,\n",
       "   1.0328478813171387,\n",
       "   0.2999250888824463,\n",
       "   0.49591556191444397,\n",
       "   0.41979163885116577,\n",
       "   0.8442884683609009,\n",
       "   -1.703405737876892,\n",
       "   0.5264267921447754,\n",
       "   1.2397983074188232,\n",
       "   0.08535729348659515,\n",
       "   -0.9382040500640869,\n",
       "   -1.5194209814071655,\n",
       "   0.4530239999294281,\n",
       "   0.14635144174098969,\n",
       "   0.18718260526657104,\n",
       "   -0.2072928547859192,\n",
       "   0.3681420385837555,\n",
       "   0.7737175822257996,\n",
       "   0.04379675164818764,\n",
       "   0.4584100544452667,\n",
       "   0.42265379428863525,\n",
       "   -0.7509170770645142,\n",
       "   0.5669096112251282,\n",
       "   0.2346978485584259,\n",
       "   -0.046616241335868835,\n",
       "   -0.33819296956062317,\n",
       "   0.18430349230766296,\n",
       "   -0.032630838453769684,\n",
       "   -0.7767743468284607,\n",
       "   0.8298144340515137,\n",
       "   0.7776114344596863,\n",
       "   0.536407470703125,\n",
       "   0.48094040155410767,\n",
       "   -0.07273314893245697,\n",
       "   0.25226637721061707,\n",
       "   0.516394317150116,\n",
       "   0.7864491939544678,\n",
       "   -0.30754590034484863,\n",
       "   -0.33328989148139954,\n",
       "   0.6611070036888123,\n",
       "   -0.758349597454071,\n",
       "   -0.42717689275741577,\n",
       "   0.4360654056072235,\n",
       "   -0.5050904750823975,\n",
       "   -0.0008138907141983509,\n",
       "   0.2606765329837799,\n",
       "   -1.3681902885437012,\n",
       "   0.16441217064857483,\n",
       "   -0.047917500138282776,\n",
       "   0.04641435667872429,\n",
       "   -1.6265339851379395,\n",
       "   -0.6035729050636292,\n",
       "   0.1639939695596695,\n",
       "   -0.3901689648628235,\n",
       "   0.9699581861495972,\n",
       "   0.5005284547805786,\n",
       "   0.034055471420288086,\n",
       "   -0.8668609857559204,\n",
       "   0.24439869821071625,\n",
       "   0.1815873235464096,\n",
       "   0.49089640378952026,\n",
       "   5.3771796226501465,\n",
       "   0.2553400993347168,\n",
       "   -0.5807623267173767,\n",
       "   -0.8583124279975891,\n",
       "   -0.4070497155189514,\n",
       "   0.4460919499397278,\n",
       "   -0.3021390736103058,\n",
       "   0.7278066873550415,\n",
       "   0.46428653597831726,\n",
       "   -0.6722910404205322,\n",
       "   -0.4841652512550354,\n",
       "   -0.5266547203063965,\n",
       "   1.8777134418487549,\n",
       "   -0.15462026000022888,\n",
       "   -1.0588328838348389,\n",
       "   0.9768964052200317,\n",
       "   -0.9349135160446167,\n",
       "   0.2663831412792206,\n",
       "   0.6076731085777283,\n",
       "   -0.6981254816055298,\n",
       "   -1.0044276714324951,\n",
       "   -0.2843928337097168,\n",
       "   0.3644416928291321,\n",
       "   -0.1085451990365982,\n",
       "   -0.1074434220790863,\n",
       "   -0.16358941793441772,\n",
       "   -1.2864922285079956,\n",
       "   0.21947765350341797,\n",
       "   -1.0823618173599243,\n",
       "   -0.3427114188671112,\n",
       "   -0.6361173987388611,\n",
       "   0.38542959094047546,\n",
       "   1.1239666938781738,\n",
       "   -0.333648681640625,\n",
       "   0.20754052698612213,\n",
       "   -1.0489847660064697,\n",
       "   -0.006863787770271301,\n",
       "   0.5844832062721252,\n",
       "   0.3371308147907257,\n",
       "   0.7886577248573303,\n",
       "   -0.16389819979667664,\n",
       "   -0.41674160957336426,\n",
       "   -0.5695079565048218,\n",
       "   0.22339078783988953,\n",
       "   -1.2867096662521362,\n",
       "   -0.26526662707328796,\n",
       "   -0.20364031195640564,\n",
       "   0.3655511140823364,\n",
       "   0.3810231685638428,\n",
       "   0.8225753903388977,\n",
       "   -1.0550483465194702,\n",
       "   -0.302307665348053,\n",
       "   -0.8101998567581177,\n",
       "   -0.34596726298332214,\n",
       "   0.40305063128471375,\n",
       "   -0.5622059106826782,\n",
       "   -0.009597478434443474,\n",
       "   -0.2411862015724182,\n",
       "   -0.43737781047821045,\n",
       "   0.1373283416032791,\n",
       "   -0.08907999098300934,\n",
       "   -0.22050020098686218,\n",
       "   0.8654420971870422,\n",
       "   0.278245747089386,\n",
       "   -0.053494200110435486,\n",
       "   0.7777311205863953,\n",
       "   0.7855013608932495,\n",
       "   -0.4795171916484833,\n",
       "   0.14980734884738922,\n",
       "   -0.190331369638443,\n",
       "   0.4283079206943512,\n",
       "   -1.2399768829345703,\n",
       "   0.32876238226890564,\n",
       "   -0.3028460443019867,\n",
       "   0.09259067475795746,\n",
       "   -0.3208226263523102,\n",
       "   0.24108831584453583,\n",
       "   -0.5625868439674377,\n",
       "   -0.0696600154042244,\n",
       "   -0.3374219834804535,\n",
       "   1.1417278051376343,\n",
       "   0.24445420503616333,\n",
       "   -0.7591786980628967,\n",
       "   -0.01609942689538002,\n",
       "   0.5024920701980591,\n",
       "   -0.40348079800605774,\n",
       "   0.007404777221381664,\n",
       "   -0.8329918384552002,\n",
       "   -0.9079856872558594,\n",
       "   -0.14958199858665466,\n",
       "   -0.8839247226715088,\n",
       "   0.08374518156051636,\n",
       "   0.6902536749839783,\n",
       "   0.26954877376556396,\n",
       "   1.0885686874389648,\n",
       "   -0.4727911651134491,\n",
       "   -0.17973579466342926,\n",
       "   -0.4755564332008362,\n",
       "   -0.3599908649921417,\n",
       "   0.7644369602203369,\n",
       "   -0.07562986761331558,\n",
       "   0.237338125705719,\n",
       "   0.15481577813625336,\n",
       "   -0.840255618095398,\n",
       "   0.16034673154354095,\n",
       "   -0.5869197249412537,\n",
       "   0.443844735622406,\n",
       "   0.3376539349555969,\n",
       "   -0.7756619453430176,\n",
       "   -0.34474608302116394,\n",
       "   -1.3999539613723755,\n",
       "   -0.3533538281917572,\n",
       "   0.16453228890895844,\n",
       "   -0.644514799118042,\n",
       "   -0.39252933859825134,\n",
       "   0.4821474552154541,\n",
       "   -0.6707284450531006,\n",
       "   -0.3503221869468689,\n",
       "   0.5845309495925903,\n",
       "   0.49622470140457153,\n",
       "   -0.05925392359495163,\n",
       "   -0.32472777366638184,\n",
       "   0.38811105489730835,\n",
       "   0.8882663249969482,\n",
       "   -0.019736241549253464,\n",
       "   0.010697738267481327,\n",
       "   0.19899997115135193,\n",
       "   -0.058501917868852615,\n",
       "   0.6227814555168152,\n",
       "   -0.3181810677051544,\n",
       "   0.13129010796546936,\n",
       "   -0.1555285006761551,\n",
       "   0.023864910006523132,\n",
       "   -0.5985145568847656,\n",
       "   0.3148359954357147,\n",
       "   -0.09440918266773224,\n",
       "   0.46019673347473145,\n",
       "   0.6278021931648254,\n",
       "   -0.54412841796875,\n",
       "   0.3171495795249939,\n",
       "   -0.9902046918869019,\n",
       "   -0.44530630111694336,\n",
       "   -0.775679349899292,\n",
       "   1.1484856605529785,\n",
       "   0.5179707407951355,\n",
       "   0.3967016339302063,\n",
       "   -0.23297862708568573,\n",
       "   -1.24800705909729,\n",
       "   -0.3433879017829895,\n",
       "   0.9208346605300903,\n",
       "   0.4773138165473938,\n",
       "   -0.5381406545639038,\n",
       "   0.653114914894104,\n",
       "   -0.3053353726863861,\n",
       "   -0.27540257573127747,\n",
       "   0.32471948862075806,\n",
       "   0.42980054020881653,\n",
       "   0.36469027400016785,\n",
       "   -0.16158244013786316,\n",
       "   0.6605656743049622,\n",
       "   -0.167363241314888]]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('model_save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Our analysis has shown, that the form or expression of the value of a commodity originates in the nature of value, and not that value and its magnitude originate in the mode of their expression as exchange value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "our           2,256\n",
      "analysis      4,106\n",
      "has           2,038\n",
      "shown         3,491\n",
      ",             1,010\n",
      "that          2,008\n",
      "the           1,996\n",
      "form          2,433\n",
      "or            2,030\n",
      "expression    3,670\n",
      "of            1,997\n",
      "the           1,996\n",
      "value         3,643\n",
      "of            1,997\n",
      "a             1,037\n",
      "commodity    19,502\n",
      "originates   16,896\n",
      "in            1,999\n",
      "the           1,996\n",
      "nature        3,267\n",
      "of            1,997\n",
      "value         3,643\n",
      ",             1,010\n",
      "and           1,998\n",
      "not           2,025\n",
      "that          2,008\n",
      "value         3,643\n",
      "and           1,998\n",
      "its           2,049\n",
      "magnitude    10,194\n",
      "originate    21,754\n",
      "in            1,999\n",
      "the           1,996\n",
      "mode          5,549\n",
      "of            1,997\n",
      "their         2,037\n",
      "expression    3,670\n",
      "as            2,004\n",
      "exchange      3,863\n",
      "value         3,643\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indices.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model_embedding = BertModel.from_pretrained('model_save')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model_embedding.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model_embedding(tokens_tensor)\n",
    "len(output[0][0][0]), len(output[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 our\n",
      "2 analysis\n",
      "3 has\n",
      "4 shown\n",
      "5 ,\n",
      "6 that\n",
      "7 the\n",
      "8 form\n",
      "9 or\n",
      "10 expression\n",
      "11 of\n",
      "12 the\n",
      "13 value\n",
      "14 of\n",
      "15 a\n",
      "16 commodity\n",
      "17 originates\n",
      "18 in\n",
      "19 the\n",
      "20 nature\n",
      "21 of\n",
      "22 value\n",
      "23 ,\n",
      "24 and\n",
      "25 not\n",
      "26 that\n",
      "27 value\n",
      "28 and\n",
      "29 its\n",
      "30 magnitude\n",
      "31 originate\n",
      "32 in\n",
      "33 the\n",
      "34 mode\n",
      "35 of\n",
      "36 their\n",
      "37 expression\n",
      "38 as\n",
      "39 exchange\n",
      "40 value\n",
      "41 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = word_embeddings[0][0]\n",
    "vec = vec.detach().numpy()\n",
    "token_vecs = []\n",
    "# For each token in the sentence...\n",
    "for embedding in word_embeddings[0]:\n",
    "    cat_vec = embedding.detach().numpy()\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs.append(cat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"value\".\n",
      "\n",
      "the value of a commodity   [-0.5239726  -0.65937173  0.93190265 -0.30739987  0.6103005 ]\n",
      "(the nature of value       [-0.24452752 -0.4482684   1.096798   -0.46360025  0.4351163 ]\n",
      "that value                 [-0.36624748 -0.52834743  1.1856767  -0.27496788  0.66663253]\n",
      "exchange value             [-0.54252017 -0.6123511   0.7570514  -0.31329563  0.9065098 ]\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"value\".')\n",
    "print('')\n",
    "print(\"the value of a commodity  \", str(token_vecs[13][:5]))\n",
    "print(\"(the nature of value      \", str(token_vecs[22][:5]))\n",
    "print(\"that value                \", str(token_vecs[27][:5]))\n",
    "print(\"exchange value            \", str(token_vecs[40][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarities:\n",
      "\n",
      "val1 to val2 0.92 val1 to val3 0.93 val1 to val4 0.89\n",
      "val2 to val3 0.92 val2 to val4 0.87\n",
      "val3 to val4 0.90\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diffa1 = 1 - cosine(token_vecs[13], token_vecs[22])\n",
    "diffa2 = 1 - cosine(token_vecs[13], token_vecs[27])\n",
    "diffa3 = 1 - cosine(token_vecs[13], token_vecs[40])\n",
    "\n",
    "diffb1 = 1 - cosine(token_vecs[22], token_vecs[27])\n",
    "diffb2 = 1 - cosine(token_vecs[22], token_vecs[40])\n",
    "\n",
    "diffc1 = 1 - cosine(token_vecs[27], token_vecs[40])\n",
    "\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "#same_bank = 1 - cosine(token_vecs[10], token_vecs[6])\n",
    "\n",
    "heatdf = pd.DataFrame({'key': ['val1','val2','val3','val4'],\n",
    "                           'val1': [None, diffa1, diffa2, diffa3],\n",
    "                           'val2': [diffa1, None, diffb1, diffb2],\n",
    "                           'val3': [diffa2, diffb1, None, diffc1],\n",
    "                           'val4': [diffa3, diffb2, diffc1, None]}).set_index('key')\n",
    "import seaborn\n",
    "seaborn.heatmap(heatdf)\n",
    "\n",
    "print('Vector similarities:')\n",
    "print('')\n",
    "print('val1 to val2 %.2f' % diffa1,'val1 to val3 %.2f' % diffa2,'val1 to val4 %.2f' % diffa3)\n",
    "print('val2 to val3 %.2f' % diffb1,'val2 to val4 %.2f' % diffb2)\n",
    "print('val3 to val4 %.2f' % diffc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = output[1]\n",
    "sentence_embedding_0 = sentence_embedding.detach().numpy()[0]\n",
    "sentence_embedding_1 = np.mean(token_vecs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04262512922286987"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sentence_embedding_0, sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation using BERT\n",
    "\n",
    "The last method which we will explore is text generation. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, such as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n",
    "\n",
    "These models can be quite impressive, even uncanny in how human like they sound. Check out this [cool website](https://transformer.huggingface.co), which allows you to write with a transformer. The website is built by the folks who wrote the package we are using. The code underneath the website can be found in their examples: [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py).\n",
    "\n",
    "We will be using the built in generate function, but the example file has more detailed code which allows you to set the seed differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing that we like to do more than analyse data all day long and then try to figure out what's going on.\n",
      "\n",
      "\"We're not going to be able to do that. We're not going to be able to do that.!\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Nothing that we like to do more than analyse data all day long and\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. A little creepy, and as we can see, far from perfect: GPT doesn't alwats work out flawlessly, but it sometimes can, and we will try and see if fine-tuning helps. We are going to tune the model on a complete dataset of Trump tweets, as they have a set of distinctive, highly identifiable qualities.\n",
    "\n",
    "### Creating a domain-specific language model\n",
    "\n",
    "One of the most exciting things about BERT and GPT is being able to retune them the way we want to. We will be training models to perform two tasks - one is to create a BERT with an \"accent\", by traning a model with english news data from the UK, from the US, and from India. We will also train a language generation model with a bunch of Trump tweets. \n",
    "\n",
    "We can train models specifically over a certain domain to make its language generation similar to that domain. \n",
    "[run_language modelling.py](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py), followed by [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py). I've downloaded these files and added them to this directory so we can run them through the notebook. You are encouraged to look at these files to get a rough idea of what is going on.\n",
    "\n",
    "### Loading Data \n",
    "\n",
    "We want to now get our Trump tweets and our English news datasets ready. The data the scripts expect is just a text file with relevant data. We load the Trump tweets and then write them to disk as train and test files with only data. I leave the original dataframes in case you would like to use it for your own purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in os.listdir(\"../data/trump_tweets\"):\n",
    "    dfs.append(pd.read_json(\"../data/trump_tweets/\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id_str</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 13:37:52</td>\n",
       "      <td>51473</td>\n",
       "      <td>947824196909961216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>8237</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Will be leaving Florida for Washington (D.C.) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 12:44:40</td>\n",
       "      <td>53557</td>\n",
       "      <td>947810806430826496</td>\n",
       "      <td>25073877.0</td>\n",
       "      <td>False</td>\n",
       "      <td>14595</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Iran is failing at every level despite the ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 12:12:00</td>\n",
       "      <td>138808</td>\n",
       "      <td>947802588174577664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>49566</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>The United States has foolishly given Pakistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-31 23:43:04</td>\n",
       "      <td>154769</td>\n",
       "      <td>947614110082043904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>35164</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-31 22:18:20</td>\n",
       "      <td>157655</td>\n",
       "      <td>947592785519173632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>39428</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>As our Country rapidly grows stronger and smar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           created_at  favorite_count              id_str  \\\n",
       "0 2018-01-01 13:37:52           51473  947824196909961216   \n",
       "1 2018-01-01 12:44:40           53557  947810806430826496   \n",
       "2 2018-01-01 12:12:00          138808  947802588174577664   \n",
       "3 2017-12-31 23:43:04          154769  947614110082043904   \n",
       "4 2017-12-31 22:18:20          157655  947592785519173632   \n",
       "\n",
       "   in_reply_to_user_id_str  is_retweet  retweet_count              source  \\\n",
       "0                      NaN       False           8237  Twitter for iPhone   \n",
       "1               25073877.0       False          14595  Twitter for iPhone   \n",
       "2                      NaN       False          49566  Twitter for iPhone   \n",
       "3                      NaN       False          35164  Twitter for iPhone   \n",
       "4                      NaN       False          39428  Twitter for iPhone   \n",
       "\n",
       "                                                text  \n",
       "0  Will be leaving Florida for Washington (D.C.) ...  \n",
       "1  Iran is failing at every level despite the ter...  \n",
       "2  The United States has foolishly given Pakistan...  \n",
       "3  HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...  \n",
       "4  As our Country rapidly grows stronger and smar...  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text = train_test_split(df['text'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1239    The @WashingtonPost quickly put together a hit...\n",
       "4335    WOW, SO NICE AND SO TRUE. THANK YOU! \"@not_tha...\n",
       "2894    It is time to send someone from the outside to...\n",
       "4179    \"@jkapper15: @realDonaldTrump please deeply co...\n",
       "676     \"@twins44: @realDonaldTrump I think you are re...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text.to_frame().to_csv(r'train_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_text.to_frame().to_csv(r'test_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now used the Google Colab GPUs to train the Trump tweet models. We'll be doing the same for our blog posts too.\n",
    "\n",
    "### GloWBe dataset\n",
    "\n",
    "We'll now load up the GloWbe (Corpus of Global Web-Based English) dataset which have different texts from different countries. We'll try and draw out texts from only the US, UK and India. We'll then save these to disk. Note that this is a Davies Corpora dataset: the full download can be done with the Dropbox link I sent in an announcement a few weeks ago. The whole download is about 3.5 GB but we only need two files, which are anout 250 MB each. The other files might be useful for your research purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lucem_illud_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "address = \"/Users/bhargavvader/Downloads/Academics_tech/corpora/GloWbE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these are the exact name of the files\n",
    "us = \"/text_us_blog_jfy.zip\"\n",
    "gb = \"/text_gb_blog_akq.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_us_blog_jfy.zip\n"
     ]
    }
   ],
   "source": [
    "us_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"us_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_gb_blog_akq.zip\n"
     ]
    }
   ],
   "source": [
    "gb_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"gb_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dictionary with document ids mapping to text. Since we don't need any information but the text, we can just save these to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< p > Many workers within the UK are required to wear an uniform for work but very few are aware they could really claim back some money from the tax man to help with the price of washing or repairing the uniform HMRC will actually pay money back once again to those individuals who are eligible even dating back four years worth of washing < p > In order to find out about claiming a tax rebate on uniform it is worth having a look online to find out whether or not you will be eligible The conditions are fairly straightforward you just have to wear a recognisable work uniform which might contain a T shirt which displays a logo design or possibly some specialist protective clothing The type of the occupation is unrelated anyone from nurses to cops to electricians are able to claim As long as the uniform is worn at work is washed on your own and you are an UK tax payer then your chances are you will be eligible to claim are eligible how do you actually go about claiming your tax refund for washing uniform You can access the HMRC web site directly and complete the important forms to help you to claim your hard earned money back Or you can use an agent that is registered with the HMRC who are able to deal with the whole tax rebate on your behalf This may save you a great deal of time and effort and they may also have the ability to advise you on any other areas where you could claim a rebate for instance if you are required to supply your own tools within your job < p > The amount of money you can claim as a tax rebate on uniform will depend on your profession and how long you have already been wearing a work uniform for You might be in a position to claim a rebate for the past four years which could add up to a fine sum of money < p > In those times of economic decline most people are seeking to save lots of washing uniform can be an effective way to start saving a couple of pounds every month which over the course of a'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(list(us_texts.values())[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_to_texts(texts, file_name):\n",
    "    text = []\n",
    "    for doc in list(texts.values()):\n",
    "        text.append(' '.join(doc).replace(\"< h >\", \"\").replace(\"< p >\", \"\"))\n",
    "    train_text, test_text = train_test_split(text, test_size=0.2)\n",
    "    with open(file_name + \"_train\", 'w') as f:\n",
    "        for item in train_text:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open(file_name + \"_test\", 'w') as f:\n",
    "        for item in test_text:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_to_texts(us_texts, \"us_blog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_to_texts(gb_texts, \"gb_blog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the training and testing files for both US and GB blogs in English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING - SHIFT TO GOOGLE COLAB OR GPU ENABLED MACHINE\n",
    "\n",
    "The [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) walks you through the process of fine-tuning models, as we did before for the classification task. Move now to the colab file to fine tune your models. Once you downloaded all the models and their information, place those files in the directory of the HW to use them as demonstrated below. \n",
    "\n",
    "\n",
    "\n",
    "### Running Scripts\n",
    "\n",
    "We use the scripts to do language modelling and text generation. The following cells run the code as if you would have run it in a terminal. I trained all of these models using the Googlr Colab file, and then saved the models to disk.\n",
    "\n",
    "#### Trump GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_language_modelling.py --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=train_text_trump --do_eval --eval_data_file=test_text_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_language_modeling.py --output_dir=output_roberta_US --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_language_modeling.py --output_dir=output_roberta_UK --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COME BACK TO THIS NOTEBOOK to load and work with your trained model\n",
    "\n",
    "### Loading and using models\n",
    "\n",
    "Let us now load the four models we have and see how we can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now - let us see what our Trump Tweet Bot looks like!\n",
    "You can generate text via command line using the command below. You can also load a model once it is saved - I trained my model using Google Colab, downloaded the model, and am loading it again via the command below. Note that you have to download all the files in your folder of the fine-tuned model to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_generation.py --model_type=gpt2 --model_name_or_path=output_trump_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer_trump = AutoTokenizer.from_pretrained(\"output_trump_gpt\")\n",
    "model_trump = AutoModelWithLMHead.from_pretrained(\"output_trump_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is going to be a disaster for the United States. He is a total loser. He is a total loser!\"\n",
      "\"\"\"@jeff_mcclaren: @realDonaldTrump @realDonaldTrump @foxandfriends @megynkelly @\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_trump.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_trump.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_trump.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - our Trump bot is nasty, so we know our model trained well. What happens if we try the same sentence for our non-fine tuned model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is going to be a very good president,\" said Sen. John McCain (R-Ariz.). \"He's going to be a very good president. He's going to be a very good president. He's going to be a very\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite the contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that generate a BERT-powered chatbot tuned on text related to your final project. What is interesting about this model, and how to does it compare to an untrained model? What does it reveal about the social game involved with your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer_marx = AutoTokenizer.from_pretrained(\"gpt_marx_save\")\n",
    "model_marx = AutoModelWithLMHead.from_pretrained(\"gpt_marx_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function of value is to determine the relative value of a given quantity of labour power\"\n",
      "\"The value of a commodity is determined by its relative value\"\n",
      "\"The value of a commodity is determined by its relative value\"\n",
      "\"The value\n"
     ]
    }
   ],
   "source": [
    "sequence = \"The function of value is\"\n",
    "\n",
    "input = tokenizer_marx.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_marx.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_marx.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hegel argued that the French had been carrying on trade with a view to increasing the export of coal iron c. and that the French wanted to make use of the English coal iron c. and that the English wanted to make use of the French\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Hegel argued that\"\n",
    "\n",
    "input = tokenizer_marx.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_marx.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_marx.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite country is England\"\n",
      "\"The happiness of the English agricultural labourer is in their labour\"\n",
      "\"Their powers are always upon the stretch they can not live cheaper than they do nor work harder\"\n",
      "\"They therefore work harder and get\n"
     ]
    }
   ],
   "source": [
    "sequence = \"My favorite country is\"\n",
    "\n",
    "input = tokenizer_marx.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_marx.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_marx.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really hate the idea of a free market\"\n",
      "\"I think that the only thing that can be done is to make the system work for the people\"\n",
      "\"I think that the only thing that can be done is to make the system work\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I really hate\"\n",
    "\n",
    "input = tokenizer_marx.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_marx.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_marx.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite professor is Mr. W. Newmarch collaborator and editor of Tooke ’s History of Prices\"\n",
      "\"The formation of monopolies by monopoly is a feature of the capitalist mode of production\"\n",
      "\"The formation of monopoly by\n"
     ]
    }
   ],
   "source": [
    "sequence = \"My favorite professor is\"\n",
    "\n",
    "input = tokenizer_marx.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_marx.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_marx.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My pet dog is named after him\"\n",
      "\"The dog is a symbol of the state of the English agricultural labourer\"\n",
      "\"The English agricultural labourer is the embodiment of the English ideal of liberty\"\n",
      "\"The English ideal of liberty is\n"
     ]
    }
   ],
   "source": [
    "sequence = \"My pet dog is named\"\n",
    "\n",
    "input = tokenizer_marx.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_marx.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_marx.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The science of history is the only true science which can be applied to the history of mankind\"\n",
      "\"The history of mankind is a history of mankind\"\n",
      "\"History of England vol\"\n",
      "\"It is a history of mankind\"\n",
      "\"History\n"
     ]
    }
   ],
   "source": [
    "sequence = \"The science of history\"\n",
    "\n",
    "input = tokenizer_marx.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_marx.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_marx.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check out our UK and GB embeddings - how do you think the two models will differ? Maybe in the way different words relate to each other in the same sentence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roberta_us_model_embedding = RobertaModel.from_pretrained('roberta_us')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roberta_us_tokenizer = RobertaTokenizer.from_pretrained('roberta_us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to visualise how words in a sentence or different or similar to each other. We will try to construct sentences where words might mean different things in different countries - in the US, people might eat chips with salsa, but in the UK, chips are what Americans call french fries, and might eat it fried fish instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Do you have your chips with fish or with salsa?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1 = \"He went out in just his undershirt and pants.\" #pants are underwear in Britain; maybe closer to an undershirt\n",
    "text2 = \"His braces completed the outfit.\" #braces are suspenders (in Britain); maybe closer to an outfit\n",
    "text3 = \"Does your pencil have a rubber on it?\" #rubber is an eraser in Britain); maybe closer to a pencil\n",
    "text4 = \"Was the bog closer to the forest or the house?\" #bog is a toilen in Britain); maybe closer to a house\n",
    "text5 = \"Are you taking the trolley or the train to the grocery market\" #trolley is a food carriage; possibly closer to a market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_us_model_embedding, roberta_us_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roberta_gb_model_embedding = RobertaModel.from_pretrained('roberta_gb')\n",
    "roberta_gb_tokenizer = RobertaTokenizer.from_pretrained('roberta_gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_gb_model_embedding, roberta_gb_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see regarding the relations with chips and sala/fish? What about the other sentences? How about comparing sentence embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that tune BERT to at least two different textual samples. These could be from different corpora, distinct time periods, separate authors, alternative publishing outlets, etc. Then compare the meaning of words, phrases and sentences to each other across the separate models. What do they reveal about the social worlds inscribed by the distinctive samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer_earlymarx = AutoTokenizer.from_pretrained(\"gpt_vol3\")\n",
    "model_earlymarx = AutoModelWithLMHead.from_pretrained(\"gpt_vol3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"I do not value linen, for its exchange value is low, in both science and history.\"\n",
    "text2 = \"I am laboring on this book about labor in order to liberate the laborer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = seaborn.heatmap(div)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
    "\n",
    "def word_vector(text, word_id, model, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings = model(tokens_tensor)[0]   \n",
    "    vector = word_embeddings[0][word_id].detach().numpy()\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATE MARX, TEXT 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"LATE MARX, TEXT 1\")\n",
    "visualise_diffs(text1, model_marx, tokenizer_marx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY MARX, TEXT 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEjCAYAAAD+PUxuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5yElEQVR4nO3de7zUVb3/8debDchFBUTEK4KKF9QkQcxS0uxiWplZqd3USo6mqfXLjl3OyTzpybJTahaRmpdMM9EkMy+RgooeUUEuKkmCR8RLpiIIAnvvz++PtQa+DLP3fGd/v8zMnvk8fXwfe+b7/a41aw/bWbNunyUzwznnnMtTj1oXwDnnXOPxysU551zuvHJxzjmXO69cnHPO5c4rF+ecc7nzysU551zuvHJxzrluRtJVkl6RNK+D65J0qaSFkuZI2j9x7QhJC+K1cxPnt5J0j6Rn4s9BiWvfivcvkPShNGX0ysU557qfq4EjOrn+YWBkPCYAvwSQ1AJcHq+PAk6QNCqmOReYamYjganxOfH68cDe8TV/EfPplFcuzjnXzZjZdOC1Tm45GrjWgoeBgZK2A8YBC83sWTNbA9wY7y2kuSY+vgb4eOL8jWa22swWAQtjPp3yysU55xrPDsDziedL4rmOzgMMNbMXAeLPbcrk1ameXSp2g1j76rOZYt/86xNfzFyGpxcMyZzHmE+9lSm9+m6WuQzLH3ojcx55uO75sn/znTqsNdt7CfBce7/MeYzb4aVM6XeZ83TmMvx428My53FU/1cz53H3iq0zpf/M/s+XvymFrW6bpizpK/m86T1k138jdGcVTDKzSRW8XKmyWifnu5JXp5q6cnHOuappb0t9a6xIKqlMii0Bdko83xFYCvTu4DzAy5K2M7MXYxfaK2Xy6pR3iznnXDVYe/ojuynAF+KssXcBy2JX10xgpKQRknoTBuqnJNKcGB+fCNyWOH+8pM0kjSBMEnikXAEatuUiaYWZbV7rcjjnHADtuVQaAEi6ATgU2FrSEuB7QC8AM5sI3AEcSRh8XwmcHK+1SjoDuAtoAa4ys/kx2x8CN0n6EvB/wKdimvmSbgKeBFqB082sbDOsYSsX55yrJ9bWml9eZieUuW7A6R1cu4NQ+RSf/xdweAdpLgAuqKSMXrk451w15NPd1W145eKcc9VQwYB+I2i6AX1JEyQ9KunRK669odbFcc41i+oO6Ndc07VcklP8sq5zcc651HIc0O8Omq5ycc65WrAGaZGk5ZWLc85VQ46zxbqDhq1cfI2Lc66uNNmAfsNWLs45V1e8W6x5ZA08OfiWqzKX4cA7r8ych61YkS2DFdmDNfbfK3se1pr9f77P80Km9L36Zy/DsDWdRUJPp8+wbP9rnv3q+Mxl6JvDF+3tDs8+IfWjM7IF8ey5de/MZciFD+g755zLnbdcnHPO5c5bLs455/Jm7WtrXYSq6vaVi6TzgBVmdnGty+Kccx3ylotzzrncNdmYS7eMLSbpO5IWSPorsEc8N1rSw5LmSLpV0qAaF9M559Zrb0t/NIBuV7lIGkPYPe2dwCeAA+Kla4F/N7N3AHMJm+eUSr8ucOV1L5bdqdM55/LhgSvr3iHArWa2EkDSFKA/MNDMpsV7rgH+UCpxMnDlS+MP9cCVzrnq8PAv3YJXCs657qXJBvS7XbcYMB04RlJfSVsAHwXeAl6XdEi85/PAtI4ycM65qmtvT380gG7XcjGzxyX9HpgNPAfcHy+dCEyU1A94Fji5NiV0zrmNmTXGQH1a3a5yATCzC4ALSlx6V7XL4pxzqTRIiyStblm55OXpBUMypc8j6GTPI76UOY/W236RKX37sjczl2H14tWZ82hfq8x5vPXaZpnSt77SkrkM6pF9SLD/ymzv59lDX+acl7bMlEdLS99M6QHWvpA9oOmy17L9Hn0XZ//7BshWChpmFlhaTV25ONeoslYsbhPw2WLOOedy591izjnnctdk3WLdcSpypySdJGn7WpfDOec20GRTkRuucgFOArxycc7VF69c6ouk4ZKekvRrSfMl3R0XUG4UqFLSJ4GxwPWSZkvKPt3FOefy0GSxxeq+colGApeb2d7AG8CxlAhUaWY3A48CnzWz0Wa2qlYFds65DbS1pj8aQHepXBaZ2ez4+DFgVzYOVDk+TUbJqMh/WvVs/iV1zrlSvFusLiVXlLUBA7uakZlNMrOxZjb2o313yVww55xLJeduMUlHxH2tFko6t8T1QXHIYI6kRyTtk7h2lqR5cajh7MT5/SQ9JGmupD9J2jKeHy5pVRxumC1pYrnydZfKpdgyOg5UuRzYoialcs65juTYcpHUAlwOfBgYBZwgaVTRbd8GZsehgy8Al8S0+wCnAOOA/YCPSBoZ01wBnGtm+wK3Auck8vtHHG4YbWanlitjd61cIASq/LGkOcBo4Px4/mpCAEsf0HfO1Y98u8XGAQvN7FkzWwPcCBxddM8oYCqAmT0NDJc0FNgLeNjMVppZK+GL+TExzR6EyPMA9xDGt7uk7hdRmtliYJ/E84sTlzcKVGlmk4HJm75kzjlXgbb0UZElTQAmJE5NihsdFuwAPJ94vgQ4sCibJwi79T4gaRywM7AjMA+4QNJgYBVwJGEiFPHax4DbgE8BOyXyGyFpFvAm8F0zu59O1H3l4pxzDaGCgfrkjrkdKBXltThi6g+BSyTNJsyonQW0mtlTki4itExWECqhwhS1LwKXSvpPYAqwJp5/ERhmZv+KW83/UdLeZtZhVNCmrlzGfCpbxFZbsSJzGbJGNAboefRXspXhz539DafTZ9nKzHnYW9kjK28496MLlD2isbVlj+7cf59sPbqtf87+ewy07BGiNxu1TeY8Nl+4LFP6fmMHZy5DLvJdv7KEDVsVOwJLN3i58MF/MoAkAYvigZldCVwZr10Y8yt0n30wnt8dOCqeX038n8vMHpP0D2B31rd4NtKdx1ycc677yHfMZSYwUtIISb2B4wktjXUkDYzXAL4MTC+0NCRtE38OI3Sd3VB0vgfwXWBifD4kTiJA0i6EtYedruVo6paLc85VjWVvTa7PylolnQHcBbQAV5nZfEmnxusTCQP310pqA54EkptHTY5jLmuB083s9Xj+BEmnx8e3AL+Jj8cD50tqJSwHOdXMXuusjF65OOdcNeS8ONLM7gDuKDo3MfH4IUILo1TaQzo4fwlxynLR+YonSnWLykXSCjPbvNblcM65LmuQsC5pdYvKxTnnujtrz69brDuoyYC+pIskfSXx/DxJ35M0VdLjMfRA8YIgJB0q6fbE859LOik+HiNpmqTHJN0labuq/DLOOZeGxxarihuB4xLPP00YODrGzPYHDgN+EqfPlSWpF3AZ8EkzGwNcBVzQwb3rAlf+Zv7/ZfkdnHMuvSYLuV+TbjEzmyVpm7hj5BDgdcIinZ9KGg+0E1agDgVeSpHlHoRV/PfE+qgl5lfqtdctTlp+xpHN1U51ztVOk3WL1XLM5Wbgk8C2hJbMZwkVzRgzWytpMdCnKE0rG7a2CtcFzDezgzZpiZ1zrqtam2tAv5aLKG8kLPz5JKGiGQC8EiuWwwhxcIo9B4yStJmkAcDh8fwCYIikgyB0k0nae5P/Bs45l5ZZ+qMB1KzlEhf8bAG8YGYvSroe+JOkR4HZwNMl0jwv6SZgDvAMIVYOZrYmbnF8aax0egI/A+ZX5ZdxzrlyGmSgPq2aTkWOewYUHr8KlOzWSq5xMbNvAt8scc9sUu5G6ZxzVedjLs4553LXILPA0mrqykV9N8uWwYpsUZUB2pd1GLE6taxRjXseNaH8TWVot4cy55GHHrffkil9y+67Zi5D+9I0Exw712O3bOWYMPnJzGXYbduXM+fRY69xmfMYyt+zlWHUnpnLkAtvuTjnnMubtabfLKwReOXinHPV0GTdYrUK/7Ii/txe0s21KINzzlVVu6U/GkCtZ4stJaxzcc65xtZkU5FruhOlpOGS5sXHJ0m6RdKdkp6R9KPEfR+U9FAMavkHSZvH84slfT8R7LJORu6cc65Ik7Vc6m2b49GEgJb7AsdJ2knS1oTtNt8fg1o+Cnw9kebVeP6XwDeqXF7nnEunyQJX1lvlMtXMlpnZ24RtOXcG3gWMAh6UNBs4kQ1DwxTmnj4GDC/3AsmoyFc9sTjHojvnXMestS310QjqbbbY6sTjNkL5BNxjZieUSVO4v1PJqMgrzjmmMdqfzrn61yDdXWnVW8ullIeB90jaDUBSP0m7d5ZA0jhJ11aldM45l4aPudQXM/sncBJwg6Q5hMqm3MD9MGDVJi6ac86l12RjLrXaLGzz+HMxYZMvzOxq4OrEPR9JPP4bcECJfIYnHj8KHBqfHghcnnOxnXOu6xqkRZJWvY255MLMzql1GZxzLslaG6NFklZDVi5pLX/ojUzp+++VPXDl6sWry99URp9lKzOlzyPoZMse2TcBbX/1+ezl2DvjUqdBQzKXoUePHHqbBw/NlHzswfdmLkKPzVsy50G//pmz0M47ZsugV6/MZchFky2ibOrKxTnnqsa7xZxzzuXOKxfnnHN5M2uuyqXupyInSTpT0lOSrq91WZxzriJNts6lu7VcvgJ82MwWlbtRUk8za61CmZxzriyfLVanJE0EdgGmSLoaOCQ+XwlMMLM5ks4DtifEGHsV+ExNCuucc8UapEWSVrfpFjOzU4GlwGGEymOWmb0D+DaQDPUyBjjazEpWLMnAlb99aekmLrVzzkXtFRwpSDpC0gJJCyWdW+L6IEm3Spoj6RFJ+ySunSVpnqT5ks5OnN8vbm8yV9KfJG2ZuPat+FoLJH2oXPm6TeVS5GDgOli3en+wpAHx2hQz6zD0i5lNMrOxZjb2c9tuX4WiOuccWLulPsqR1EKIQvJhQtT4EySNKrrt28Ds+CX8C8AlMe0+wCnAOGA/4COSRsY0VwDnmtm+wK3AOTHNKOB4YG/gCOAXsQwd6q6Vi0qcK/yLZF/Z6Jxzect3QH8csNDMnjWzNcCNwNFF94wCpgKY2dPAcElDgb2Ah81sZRyXngYcE9PsAUyPj+8Bjo2PjwZuNLPVccx7YSxDh7pr5TId+CyApEMJG4a9WcsCOedcpyroFkt238djQlFuOwDJkBZL4rmkJ4BPQIgUT9gHa0dgHjBe0mBJ/YAjgZ1imnnAx+LjTyXOp3m9DXSbAf0i5wG/iVGSVxI2EHPOubplrekH9JP7TnWgs96bgh8Cl8RNFucCs4BWM3tK0kWElskKQiVUmFn7ReBSSf8JTAHWVPB6G+hWlUsyCjIbNwExs/OqVhjnnKtAmrGUCixhfasCQotkgxlKsTfnZABJAhbFAzO7ErgyXrsw5lfoPvtgPL87cFTa1yvWXbvFnHOue8l3tthMYKSkEZJ6EwbbpyRvkDQwXgP4MjC9MHwgaZv4cxih6+yGovM9gO8CE2P6KcDxkjaTNAIYCTzSWQG7Vcul3uSxKKp9banWZoXleCt7ZOWs8oho3GPrncrfVK4cffpmSq+tskUjBrAVyzLnQc/e5e/pRMtW2dIDqCWH7569++RQjozRmesk7Eqee4CZWaukM4C7gBbgKjObL+nUeH0iYeD+WkltwJPAlxJZTJY0GFgLnG5mr8fzJ0g6PT6+BfhNzG++pJtiPq0xTVtnZfTKxTnnqiHnBfpmdgdwR9G5iYnHDxFaGKXSHtLB+UuIU5ZLXLsAuCBt+bxycc65Kmi2YFReuTjnXBXk2S3WHXTLAX1JM+LP4ZI8fphzru5Ze/qjEXTLysXM3h0fDseDUzrnugGvXLoBSSviwx8Ch0iaLelrkvaOAdpmx2BtJQeznHOu6kzpjwbQLSuXhHOB+81stJn9FDgVuMTMRgNjiQuDkjwqsnOuFpqt5dJoA/oPAd+RtCNwi5k9U3xDMqzCiwcfVh8T4J1zDa+9tTFaJGl195bLBszsd4Sga6uAuyS9r8ZFcs45AMyU+mgE3b3lshzYovBE0i7As2Z2aXz8DuBvtSqcc84VNEp3V1rdvXKZA7RKegK4GugDfE7SWuAl4Pwals0559ax9sZokaTVLSsXM9s8/lwLHF50+b+rXyLnnOtcnYQ4q5puWbnk5brnO93rpqzP80LmMrz12maZ84BsgSt73H5L5hK07L1n5jyyBp0E6HlotmVPbc/8b+Yy0DuHf9OMfSi9Dt4/cxHa5j6VOY88AlcyZNtMyW1p9qCqefCWi3POudy1t3nl4pxzLmfecnHOOZe7RplinFZV1rlIWixp62q8lnPO1SNfoe+ccy537d5y2ZikzyUCQv5K0oExMGQfSf0lzZe0j6QWSRdLmhuvfzWRzVclPR6v7RnzHSdphqRZ8ece8fxJkm6RdKekZyT9KFGWL0n6u6T7JP1a0s/j+SGSJkuaGY/35Pg+OedcJu1tPVIfjaBsy0XSXsBxwHvMbK2kXwB7AFOAHwB9gd+a2TxJpwEjgHfGPZ63SmT1qpntL+krwDeALwNPA+Pjve8HLgSOjfePBt5JmGe7QNJlQBvwH8D+hNX5fwOeiPdfAvzUzB6QNIywt/ReXXpXnHMuZ77OZWOHA2OAmZIgVCavEFa/zwTeBs6M974fmGgWNvQ0s9cS+RQWUzwGfCI+HgBcE0PjG9Arcf9UM1sGIOlJYGdga2BaIV9JfwB2T7z2qFhGgC0lbWFmy5O/jKQJwASAT2w1jgM396j8zrlNz2eLbUzANWb2rQ1OStsCmxMqhD7AW/Hejurnwkq/tsTr/hdwr5kdI2k4cF+J+5NpOvvX6QEcZGarOvtlklGRf7Tz55rsu4RzrlZ8zGVjU4FPStoGQNJWknYmfED/B3A9cFG8927gVEk9C/eWyXsArFvmflKKsjwCvFfSoPgaxyau3Q2cUXgiaXSK/Jxzrio8KnIRM3tS0neBuyX1ANYCtwGtZvY7SS3AjBje/gpCN9WcGDzy18DPO8n+R4Rusa+TInqxmb0g6ULgf4GlwJPAsnj5TOBySXPi7zWdsHmYc87VnI+5lGBmvwd+38G1NuDAxKmvxyN5z/DE40eBQ+Pjh1g/ZgKhJYSZXU2IclxI85HEPb8zs0mx5XIrocWCmb1KmHjgnHN1p629MWaBpdUdf9vzJM0G5gGLgD/WtDTOOZeCWfqjEXS7RZRm9o288jqs9a1M6Xv1z76UtvWVlsx5oGx/jS2775q9DIOGZM5CWw3NnEfWqMYtIw8sf1O5MrS2Zs5DA7bJlL792aczl6HH0MGZ86BPv+x5rFqRKbm2yxb9PC/NNqDf7SoX55zrjhploD4tr1ycc64KvOXinHMudw0ylJJatxjQl5St09U552qsrb1H6iMNSUdIWiBpoaRzS1wfJOnWGOfxEUn7JK6dJWlejAt5duL8aEkPxziSj0oaF88Pl7Qqnp8taWK58nnLxTnnqiDPSPpxfeHlwAeAJYTwXFPM7MnEbd8GZscIKHvG+w+PlcwpwDhgDXCnpD+b2TOEtYffN7O/SDoyPj805vcPMxudtow1ablIuigGsCw8P0/S9yRNTUROPrpEukMl3Z54/nNJJ8XHYyRNk/SYpLskbVeVX8Y551IwlPpIYRyw0MyeNbM1wI1A8WfmKEKEFczsaWC4pKGEgL4Pm9nKGAdyGnDMumLClvHxAMJi9S6pVbfYjWy44PHTwG+AY8xsf+Aw4CdKRKHsjKRewGXAJ81sDHAVcEEH906Izb1Hb31rcYZfwTnn0mu39EcKOwDPJ54vieeSniAGCY7dWzsDOxLWCI6XNFhSP+BIYKeY5mzgx5KeBy4GkjElR8TtUaZJOqRcAWvSLWZmsyRtI2l7YAjwOvAi8FNJ4wktyB2AocBLKbLcA9gHuCfWRy0xv1KvvS5w5cwdjmm2MTbnXI20p2uRABtGb48mxc+udbeUSFb8efZD4JK46HwuMIsQtuspSRcB9wArCJVQYXHWacDXzGyypE8DVxIizr8IDDOzf0kaA/xR0t5m9mZHv0Mtx1xuBj4JbEtoyXyWUNGMifvGLCZEW05qZcPWVuG6gPlmdtAmLbFzznVRWwWVS/JLcAeWsL61AaFFskEXVvzgPxkg9gItigdmdiWh4iDGa1wSk50InBUf/4EQLxIzW02MVG9mj0n6ByF016MdFbCWs8VuBI4nVDA3E/r3XokVy2GEJlyx5wh7tmwmaQBhrxmABcAQSQdB6CaTtPcm/w2ccy6lnMdcZgIjJY2Q1JvwWToleYOkgfEahM0ZpxdaGoko98MIXWc3xPuWAu+Nj98HPBPvGxInESBpF2Ak8GxnBaxZy8XM5kvaAnjBzF6UdD3wJ0mPArMJu1QWp3le0k3AHMIvPSueXyPpk8ClsdLpCfwMmF+VX8Y558rIc7ZY3L33DMKOuy3AVfEz9dR4fSJh4P5aSW2ECPJfSmQxWdJgQpT7083s9Xj+FEJXWk/CRpCFrrnxwPmSWgn7a51atBnkRmo6FdnM9k08fhUo2a1lZpsnHn8T+GaJe2YT3gDnnKs7eVYuAGZ2B3BH0bmJiccPEVoYpdKWHJA3swcIOw8Xn58MTK6kfL7OxTnnqiBld1fDaOrK5bn2bBFbh63ptFWYinpkn7Bmbdn+aNuXppmQ17kePbIP39mKZeVvKqf3ZpmS5xHRuGWv92TOo23hzEzp21/M/m/a9twrmfPotXv2PHi7053Ly0u3omGTa6+PYlRNU1cuzjlXLZXMFmsEXrk451wV5D3mUu+6ReDKciTNqHUZnHOuM+1S6qMRNETLxczeXesyOOdcZ5otHEijtFxWxJ/bSZoeQ0LPSxP/xjnnqqG9gqMRNETLJeEzwF1mdkFcTZrDBt7OOZdda4N0d6XVEC2XhJnAyZLOA/Y1s+XFNySjIt+zcmHVC+ica05WwdEIGqpyMbPphFX6LwDXSfpCiXsmmdlYMxv7gX67Vb2Mzrnm1K70RyNoqG4xSTsTYpX9WlJ/YH/g2hoXyznnGmYsJa2GqlwI23GeI2ktYZ+CjVouzjlXC43S3ZVWQ1QuhcCWZnYNcE2Ni+OccxtplO6utBqicnHOuXqXPWpd99LUlcu4HbIF9+szLPvb13/l6ux57NM3U/oeu+2auQwMHpo9j569y99TjmXr2daAbTIXIWvQSYCW3Q7IVoY7/5i9DDsMzpyHhpba868y9srz5W/qzJZbZS5DHsxbLs455/LmA/rOOedy55WLc8653PlssTogaUVya2PnnOvufLaYc8653DXbbLG6Dv+i4McxwvFcScfF87+Q9LH4+FZJV8XHX5L0g1qW2TnnSvHYYvXlE8BoYD/g/cCPJW0HTAcK4fR3AEbFxwcD93eWYTJw5e/++cImKbRzzhVrtthi9V65HAzcYGZtZvYyMA04gFCBHCJpFPAk8HKsdA4COt2VMhm48jNDdtjExXfOucD3c6kvJetwM3tB0iDgCEIrZivg08CKUmH2nXOu1hqluyutem+5TAeOk9QiaQghnP4j8dpDwNnxnvuBb5DoEpM0VZI3TZxzdaEVS300gnpvudxK6Op6glDxf9PMCjFb7gc+aGYLJT1HaL3cDyCpB7Ab8Fr1i+yccxtrjCojvbqsXBJRjg04Jx7F91wJXBkfrwX6Jy6PAiab2apNX1rnnCuvUcZS0qrLyiUrM5sHfL3W5XDOuYJGmQWWlkLjoDn17L1Dpl/+7O3HZy7DCzk0rrL20U54u0/mMow9+OXMebRslT0qcq+D98+WQWv2pW7tL2aLth3K0ZYpee8zLshchNUX/b/Medja7O9ny47ZIm6vuveZzGUAGPznaZmqh+8O/0zq/1F/sPh33b4qasiWi3PO1Ztm+xpf77PFnHOuIeQ9W0zSEZIWSFoo6dwS1wfFCCZzJD0iaZ/EtbNi5JP5ks5OnB8t6WFJs+Ni83GJa9+Kr7VA0ofKla/bVy6SOl006Zxz9SDP8C+SWoDLgQ8TJjCdEBeVJ30bmG1m7wC+AFwS0+4DnAKMI0Q/+YikkTHNj4Dvm9lo4D/jc2LexwN7E9YX/iKWoUPdvnIxs3fXugzOOVdOziv0xwELzexZM1sD3AgcXXTPKGAqgJk9DQyXNBTYC3jYzFaaWSsh8skxMY0BW8bHA4Cl8fHRwI1mttrMFgELYxk61O0rF0kr4s9DJd0n6WZJT0u6XlK3HxRzzjWGdiz1kcIOQHL/5yXxXNIThPiMxO6tnYEdgXnAeEmDJfUDjgR2imnOJsRwfB64GPhWBa+3gW5fuRR5J+HNGQXsArynpqVxzrmokm6xZIDdeEwoyq7UF+fiWumHwCBJs4GvArOAVjN7CrgIuAe4k1AJFab1nQZ8zcx2Ar5GXEuY8vU20GizxR4xsyUA8Q0dDjyQvCH+I00AUMsAevToj3PObWqVLKI0s0nApE5uWcL61gaEFsnS5A1m9iZwMoTtS4BF8dhgEbqkC2N+ACcCZ8XHfwCuSPt6xRqt5bI68biNEpVnMiqyVyzOuWppw1IfKcwERkoaIak3YbB9SvIGSQPjNYAvA9NjhYOkbeLPYYSusxvifUuB98bH7wMKi4SmAMdL2kzSCGAk6+M8ltRoLRfnnKtLKcdSUjGzVklnAHcBLcBVZjZf0qnx+kTCwP21ktoIW5N8KZHFZEmDgbXA6Wb2ejx/CnCJpJ7A28Renpj3TTGf1pim05W+Xrk451wV5L2I0szuAO4oOjcx8fghQgujVNpDOjj/ADCmg2sXAKlDP3T7yiUR5PI+4L7E+TNqVCTnnNtIni2X7qDbVy7OOdcdeFTkJvLjbQ/LlL5vttiCALS09M2cx0DrdKFsWbttmz3oZI/Ns5UBQC3Z55e0zX0qU/oeQwdnL8Nzr2TOo2WHbOXII+jkZv/+k8x5rP39/2TOw17K9n72Gb1N5jLkIeVAfcNo6srFOeeqxbxycc45lzfvFnPOOZe79ibbO6tuFlFKGi5pXgX3nx3j4hSer9g0JXPOuezyjIrcHdRN5dIFZwP9yt3knHP1IOfAlXWv3iqXnpKuiZvb3Cypn6TDJc2SNFfSVTH8wJnA9sC9ku4tJJZ0gaQn4mY32fZGdc65HOUc/qXu1VvlsgcwKW5u8ybwdeBq4Dgz25cwRnSamV1KiIFzmJkV5hP3J+xRsB8wnRDGwDnn6oK3XGrreTN7MD7+LXA4sMjM/h7PXQOM7yDtGuD2+PgxQkTkjSRDWT+04plStzjnXO6sgv8aQb1VLlne1bVm66ZjlIyIDBtGRT5o85Jhd5xzLnc570RZ9+qtchkm6aD4+ATgr4StOXeL5z5P2JITYDmwRZXL55xzXWJmqY9GUG+Vy1PAiZLmAFsBPyVsdvMHSXMJlXoh6uck4C/JAX3nnKtXzTbmUjeLKM1sMWF74mJTCdsXF99/GXBZ4vnmicc3AzfnX0rnnOuaRpkFllbdVC7OOdfIGqVFklZTVy5H9X81U/rtDs/eq7j2hbcy57HZqGxRX3vsNS5zGeiXw5bRvfvUPo8+2dfl9to9e1RkDd05U/q1N/42cxnyiGjc67ivZ86jde7fMqVXn/rYzrxRxlLSaurKxTnnqqVRZoGl5ZWLc85VQaOsX0nLKxfnnKuCNmuutktNpiJLGivp0lq8tnPO1YJPRa4CM3sUeLQWr+2cc7XQbN1iubZcJPWX9OcYmXiepOMkHSBpRjz3iKQtJB0q6fZEmqskzYzRj4+O50+SdIukOyU9I+lHidc5QtLjMc+pneXjnHP1oN0s9dEI8m65HAEsNbOjACQNAGYRohrPlLQlsKoozXeAv5nZFyUNBB6R9Nd4bTRhAeVqYIGky4C3gV8D481skaStOsvHzDaY6ytpAjAB4PxtR3HcwJ1y/PWdc660xqgy0su7cpkLXCzpIkKE4jeAF81sJoCZvQkgKZnmg8DHJH0jPu8DDIuPp5rZspjmSWBnYBAw3cwWxTxfK5PPU8kXM7NJhNAx/H2vI5rt39s5VyONMpaSVq6Vi5n9XdIY4Ejgv4G7KV9hCzjWzBZscFI6kNBiKShEOlYHeZbMxznn6oHPFstA0vbASjP7LXAx8C5ge0kHxOtbSCqu0O4CvqrYnJG0URyxIg8B75U0It5f6BarNB/nnKsany2Wzb7AjyW1A2uB0wgtissk9SWMt7y/KM1/AT8D5sSKYTHwkY5ewMz+GcdNbpHUA3gF+ECl+TjnXDU122yxvLvF7iK0IIq9q+j5ffHAzFYB/1Yir6sJWxwXnn8k8fgvwF+K7i+Zj3PO1QOPLdZE7l6xdab0J/Iar81YkymPLXaGJbO2zJTHrqPg5SnLupx+u72g/am/l7+xEz3GvBN79Z+Z8tD2O8HLSzPlwU67wNsru56+Tz94PXvgSd4unhRZOXvl+UzpW3Ycmr0ML2V/L7IGngToue/7upy27Zn/xd7I+Lc5cEim9NB8A/r1tllYt5K1YoHsFQtkq1gge8UCZK5YgOwVC2SrWMArlmQZGqBiATJXLHnl0WbtqY804nq/BZIWSjq3xPVBkm6VNCeuMdwnce2suBZxvqSzE+d/L2l2PBZLmh3PD5e0KnFtYvHrFWvqlotzzlVLnmMuklqAywnjzUuAmZKmmNmTidu+Dcw2s2Mk7RnvPzxWMqcA44A1wJ2S/mxmz5jZcYnX+AmQ/Ob6DzMbnbaM3nJxzrkqyHmF/jhgoZk9a2ZrgBuB4qgkowg7+WJmTwPDJQ0F9gIeNrOVZtYKTAOOSSaMk6I+DdzQ1d+3YSsXSStqXQbnnCuwCv6TNEHSo4ljQlF2OwDJvtMl8VzSE8AnACSNIyxC3xGYB4yXNFhSP8K6xOJQJYcAL5vZM4lzI2JorWmSDin3+3q3mHPOVUElMcOSkUQ6oBLnil/gh8AlcdxkLiEUV6uZPRWjqNwDrCBUQq1FaU9gw1bLi8AwM/tXXCj/R0l7F6KulFLXlYukPxJq1D7AJWY2KbZILiGsYVkFHG1mL8dFlb8j/E531qjIzjlXUs7rXJawYWtjR2CDGTHxg/9kWNfNtSgemNmVwJXx2oUxP+LznoQWz5hEXquJEVPM7DFJ/wB2p5Po9vXeLfZFMxsDjAXOlDQY6E/oL9wPmE4YmIJQ4fzSzA4AXqpJaZ1zrgM5zxabCYyUNEJSb+B4YEryBkkD4zWALxNiMhbiO24Tfw4jVCTJVsr7gafNLFnhDImTCJC0CzASeLazAtZ75XKmpCeAhwm19EjC7Ibb4/XHgOHx8XtY/wZd11GGyb7MB1c809FtzjmXqzwH9ONA/BmERetPATeZ2XxJp0o6Nd62FzBf0tPAh4GzEllMjsGA/wScbmavJ64dz8YD+eMJ0U+eAG4GTk0EDS6pbrvFJB1KqEEPMrOVku4jdI+ttfVLXQvBLAvK/qsk+zJ/vtPnmmtVk3OuZvIO/2JmdwB3FJ2bmHj8EOELeam0HQ7Im9lJJc5NBiZXUr56brkMAF6PFcuebBxCptiDhBoX4LObtGTOOVchs/bURyOo58rlTqCnpDmEoJQPl7n/LOB0STMJFZNzztUNj4pcJ+LshA+XuLR54p6bCf1/xM3DDkrc98NNWkDnnKuAB650zjmXu2bbLKypK5fP7J8tOGDPrXuXv6mMvos7XIOUWr+xgzOl7zFqz8xloFev7Hnk8M3Olmb7N9V2xYucu5JJqfVtFdpyq/L3dGLVvdlnQvYZvU3mPNSnf+Y8WmfeXv6mTvQ8oD62dapkEWUjaOrKxTnnqsU3C3POOZc7H3NxzjmXu0aZBZZW7lOR46Yy80qcP1/S+ztJ93FJo/Iuj3PO1YO29vbURyOoWsvFzP6zzC0fJ4R1ebLMfetI6hnDIDjnXF1rtm6xTbWIskXSr+MWmndL6ivpakmfBJD0Q0lPxu03L5b0buBjwI/jFpq7Shot6eF4z62SBsW090m6UNI04DuSFknqFa9tGbfmzGHqknPO5afZFlFuqsplJHC5me0NvAEcW7ggaSvCrmd7m9k7gB+Y2QxCRM9zzGy0mf0DuBb493jPXOB7ifwHmtl7zez7wH3AUfH88cBkM1vbUcGSgSuvWfxiTr+uc851zsxSH41gU1Uui8xsdnycjFwM8CbwNnCFpE8AK4sTSxpAqECmxVPXEKJyFvw+8fgK4p4F8edvOiuYmU0ys7FmNvbE4dul+22ccy6jnLc5rnubqnJZnXi8QeTiOEYyjhBh8+N0bWOvtxL5PUjYG/q9QIuZbTSZwDnnaq2SbY4bQdWnIkvaHOhnZndIehhYGC8tB7YAMLNlkl6XdIiZ3Q98HphWOkcgdKHdQAhw6ZxzdadRZoGlVYuoyFsAt8dox9OAr8XzNwLnSJolaVfgRMIA/xxgNHB+J3leDwwibnAj6Q5J22+i8jvnXMW85ZKRmS0G9kk8v7jEbeNKpHsQKF7nstEeLmZ2aIn8DgZuNrM34j1Hpi6wc85VQaMM1KfV7VfoS7qMEJrfKxTnXN1qtsqloulxzXYAExohj3ooQ73kUQ9lqJc86qEM9ZJHHmXwY8OjnneirAcTGiSPeihDveRRD2WolzzqoQz1kkceZXAJXrk455zLnVcuzjnncueVS+cmNUge9VCGesmjHspQL3nUQxnqJY88yuASFAeznHPOudx4y8U551zuvHJxzjmXO69cnHPO5c4rlzolabM05+qVpBZJv611OZIk9a91GVwQNxDco9blcJuOVy45kzRA0k8LG5JJ+kncn6ZSD6U811E5hkq6UtJf4vNRkr7UhXIk87xG0i8l7VPuXjNrA4ZI6p3lNePr7lqoWCUdKulMSQMrSP9uSU8CT8Xn+0n6RQXpc3kvJb2nUMFJ+pyk/5G0c6X5FOW5XaVfOuL78RlJXygcFaYfIunbkiZJuqpwVJD+o8Bs4nYbcdfZKZWUYVORtG2ty9AovHKJJC2X9GaJY7mkNyvI6irChmifjseblNnArKgc20oaA/SV9E5J+8fjUKBfBeW4GrgLKESH/jtwdgXpS/k58FfCFghpLAYelPQfkr5eOLrwupOBNkm7AVcCI4DfVZD+p8CHgH8BmNkTbLj5XDlXk897+UtgpaT9gG8CzxG2i8jiOuBpSaUCxG5E0nXAxYRgrwfEY2yFr3kbMIDwt/DnxJHWeYTgtW8AWNhYcHiahJ38f/pmhf+fIqmlxOkrK8nDdazbB67Mi5ltkVNWu5rZsYnn35c0u4L0HwJOAnYE/idxfjnw7Qry2drMbpL0LQibtElqqyD9RsxsJjCT8GGfxtJ49CDu1dNF7bH8xwA/M7PLJM2qJAMze15S8lQl70Ve72WrmZmko4FLzOxKSSd2IZ91zOz9Cr9YcUTxjowFRlm2NQj9zOzfM6RvtbBnU8UJC/+fSjofeIlQuQr4LJX/jS2UdDPwGzN7MuZ/VJk0LiWvXPK3StLBZvYAhK4QYFXaxGZ2DXCNpGPNLO2HeClvSRoMYXMISe8CllWSgaTdgXOAndlwN9H3pUlvZt+P+WwRntqKSl4/Ya2kEwh7/Hw0nutVQfrnJb0bsNhNdyaxiyylzO9ltDxWUJ8DxsdvzpX8HoUP1fuBGWb2FoQ3FpifMot5wLbAi5W8bpHbJR1pZnd0Mf08SZ8BWiSNJPx7zKgwjw+Z2YGJ57+U9L/AjyrI4x3A8YQt13sQeh1uMLPlFZbFlVLryJmNdgD7AU8QuoQWA7OAd3Qhn4GElsuj8fgJMKCC9PsDDxI+BB8kdOVUVI74e5xG6MIYUzgqSL9P/P2fi8djwN5deC9GAZcCJ8TnI4BzK0i/NWFDuZeBV4DfAoOr+V7GfLYFvg4cEp8PA75QYR5fJHwIPgU8Ev8ujq4g/b3A64RuvimFI2Xa5YRu3uVAO+FLU+H5mxWUoR9wAaEVPBP4AdCnwvdhBqG10kJoGX+WUOFW9G+SyG888AJhC/VrgN26mpcf4fAV+jkpGksQUJiZ9Bbhy+X/bJyq0/wmE75lXhNPfR7Yz8w+UUEePYE9YnkWmNnaCsvwmJmNqSRNUfoZwHfM7N74/FDgQjN7d4X5nGVml5Q7tyllfS83QXm2JYzpfQMYZCm7dSW9t9R5M+tsG/G6I2k4cAnwHkKL8kHgbAubFabNowU4CjiZMOZzHeFLyCGEv9Pdcy10k/HKJSeSvhcf7kEYJL2N8EH0UWC6mX25wvxmm9nocuc6SV9yBpCZpR5AlnQe4Zv+rcDqRB6vpUz/hJntV+5cinweN7P9i87NMrN3pkx/aYnTy4BHzey2FOkzvZeSHjCzgyUthw32sFXIxrZMk0/M6wpCS+5lQvfYA8DjZtaaNo+sJE01s8PLnesk/T3ApyzuHCtpEHCjmX0o98J2Xo5nCS25K81sRtG1S83szGqWp9H4mEtObP34wt3A/hb7beMH9B+6kGWmsRtCBVfQBzgceJzKZicVBpvPSZwzYJeU6Z+V9B+Eb4QQxhoWpX3xOM7yGWBE0VTVLYgzv1LqA+zJ+n+HYwljFF+SdJiZnV0mfab30swOjj/zmDQymNAV9AbwGvBqmooljwpOUh9Ci3zrWCEURuS3ZP1MujS2LlQshBd/XdI2FaRH0hDgFEKLIzke+MWU6VuAq83s/FLXvWLJziuX/A0D1iSeryHlNMsipxEG9gtrZF5n/Yd9WWb21eTzmM91HdzeUR4jKrk/8VrXmdnnCd+shwO3ED6IphG6INKaQRh43powtlCwHJhTQT67Ae8rfAhL+iVwN/ABYG65xHm8l3kxs2NiGfYizCy8V1KLme1YJl0eFdy/EaZgb0+oXAveBC6vIJ92ScPM7P8AFNb6VNqFchvh7+uvVDbzDwjrsCQdBpSsXFx2Xrnk7zrgEUm3Ev6HOYb14yaVeIow82VXwuD+MuDjVPahmrQSGFlJAkn9CAPQw8xsQpzZs4eZ3V4m6Zj4gXEicBjx23Eh27Svb2aFiQAHVVLuEnYgfOMuzPDqD2wfP2BWd5ysQxW/l3mR9BHCmMB4YBDwN8KH7CYXx7gukfRVM7ssQ1bfAR6QVBjnGU/lO0FmnQ4NMEPSz4HfE8ZGATCzxztO4tLyyiVnZnaBwkruQ+Kpk82sojUZ0W2Ero/HCbNYKiLpT6z/QO9B6Ke/qcJsfkOY4VUYgF9C6FoqV7lMJKy+3oUw021dsaigWy3HsYofAbMl3RfTjgcuVFgt/9cU5cjjvczLh4HphHUyS2tUhl9JOpP1C1HvA36VdpKDmd0paX/gXYR/j6+Z2asVliHrdGhY/3edbL0YkGqqveucD+jXKUnzzKxsmJVO0idnBbUCz5nZkgrzeNTMxiYHzysZkJf0SzM7rZLX3FQkbUeYUi3gkUo+mPN4L/MkaSjrx4EeMbNXqvz6VxDW5yRnMrZVMmlF0g5svH5qegXplxNaoKuBtXRhcoTbtLzlUr9mSNrXzMqOCZSS09TSNZL6sn7x4K4kZo2lKENdVCxRD+CfhL/53STtlvbDrJ6m6Ur6FCF8y32ED9TLJJ1jZjdXsRgHFH3B+JukJ9ImlnQRcBxhUkV7PG2EFlkqZraFpK0I3ZN90qYrKscA4Husb4FNA843s64skHVFvHKpXwcDJ0laRPhAL3wze0dniUp0Ia27ROXf7L5H6N7aSdL1hDUFJ1WQvi509cMs5/cyL98lfLi/AutmTf0VqGbl0iZpVzP7RyzDLlQ2qP5xwthdV8a7iK/5ZeAsQpik2YQuthmEmXxpXUVYS/bp+PzzhK7g1GvJXMe8cqlfH+5KopymuxbyukfS46zvGz+rC33j9eDjdOHDLM/3Mkc9irrB/kX1A9CeQ5il9mx8PpzKZgE+S+hW63LlQqhYDgAeNrPDJO0JfL/CPLLGAXSd8MqlTsWZUpnF9QPrug0K0z8r0IcwDbonMEpSRX3jdSKPD7M83ss83CnpLuCG+Pw4IMugdlc8CPyK9a2EX1HBdhCE2XazJU1lw8W5lawtedvM3paEpM3M7GlVvj9M1rVkrhNeuTQoSR8jrA3ZnrDKfmfC9Oa9K8gjc994ncj0YZbHe5kXMztH0rGELkoBk8zs1ioX41rC2pb/is9PIEzB/1TK9IWYZlksUdjT54/APZJeJ0TgrsSpwLVdXUvmOuezxRpUHGB9H/BXM3tnXDB2gpmlXk8gaQEhQGOmb/y1pg7C2luIQJ0mfeb3spHkEdYnThQZZmYLcijPewn7y9xpZmvK3Z9IN8LMFknaEsDM3iycy1om5y2XRrbWzP4lqYekHmZ2b2yJVCKX7qRaS1uJdCKP9zKTOptcMEvSu8zs4Vi2AwldZako7ER5MdCbENpnNGGW1se6UpgMs/kmE0I1JTcZu5kQ/dtl5JVL43pD0uaE1dvXS3qFsEajEnn0jddcjCzw34TFj8kxk7Qx0vJ4LzOph8kFkuYSKrhewBck/V98vjPwZAVZnUdYc3QfhJ0oJXUp1FBXxMH/vYEBkpIzw7aki9Oa3ca8cmlc0wlhY84iBIwcQOVxlPLoG68HvyFMq/4pIRzNyVQQhoZ83stG8JGc8im1E2U1++f3IPwuA1m/+RyEmHWnVLEcDc0rl8YlwoZQrwE3Ar83s0oiCefRnVQv+prZVEmKs/DOk3Q/ocJJI/N72QjymsFIPjtRdpmFbRZuk3SQmVUyy81VoNrz412VmNn3zWxv4HTCLKdpksrG0QKQdFP8OVfSnOJjExZ7U3lbYRvbZySdIekYIHWI9yzvpSvpq4RuqdWEKdVvEqItV9sxkraU1EvSVEmvSvpcDcrRkHy2WINT2LHwU4S9wrcot8I/ptnOzF5UiGy8kRy/wVaFpAMIU4cHEqbPDgB+VBiQriCfit9LV78UN9+LXzY+DnwNuLeSWW+uY94t1qAknUZYozKEMAPmFDNLNehqZi/Gn92qEumImc2MD1dQ2UpyINt76daT9DMzO1sbRplep6uzxTLoFX8eCdxgZq8VjQO5DLxyaVw7E/YUn11pwjqb9pqZpN0JIUuKo/CmDa3e5ffSbaCwwdrFNS3Fen+S9DRhVf5XYpy2t2tcpobh3WKu4cVFkBMJe9OsC7BoZo/VrFBNTGEfnVVm1h6ftwCbmdnKGpRlEPCmhY3j+gFbmtlL1S5HI/KWi2sGrWb2y1oXwq0zFXg/oZsSoC9h2+l3d5giR5LeZ2Z/S65xKeoOu6Ua5Wh0Xrm4hhX3+4DQ/fEV4FY2XAz6Wk0K5vqYWaFiwcxWxFZDtbyXsD10YY1LcgtuwyuXXHjl4hrZY4QPi8LX0nMS11Jvt+xy95ak/S3uVS9pLFWMRmxmhfVNpwHHErYMKHwW+jhBTrxycQ3LzKoWUsRV5CzgD5KWEj7MtyfMxqu2PwJvAI+zfiDfK5eceOXiGp6k04HrzeyN+HwQIarxL2pasOY1AngnMAw4hrAZXS0+1Hc0syNq8LpNwVfou2ZwSqFiATCz1/EYUrX0HzES8UDgA8AkoBYTLmZI2rcGr9sUvOXimkGPGFfMYN3U1941LlMzK0wHPwqYaGa3STqvWi+eiO7cEzhZYbvm1axfx+WRF3LglYtrBncBN0maSPhQORW4s7ZFamovSPoVYTryRZI2o7q9KHlFd3ad8EWUruHFoJUTCB9mIqypuMLM2jpN6DaJOO34CGCumT0jaTtgXzO7u8ZFcznyysU1PEmjimOBSTrUzO6rUZGca3g+oO+awU2Svqmgr6TLCDtTOuc2Ea9cXDM4kDDtdQYwE1gKvKemJXKuwXnl4prBWsIK8L6EPdIXFYImOuc2Da9cXDOYSahcxgIHAydIurm2RXKusXnl4prBKcAzwLdjOPWvArNrWiLnGpxXLq4ZnEwIMXJCfL4cOLp2xXGu8fkiStcMDjSz/SXNghD+RVKvcomcc13nLRfXDNbGkC+F8C9D8Oi3zm1SXrm4ZnApYaOwbSRdADwAXFjbIjnX2HyFvmsKkvYEDieEf5lqZk/VuEjONTSvXJxzzuXOu8Wcc87lzisX55xzufPKxTnnXO68cnHOOZc7r1ycc87l7v8Di0U8JZVMsgcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"EARLY MARX, TEXT 1\")\n",
    "visualise_diffs(text1, model_earlymarx, tokenizer_earlymarx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATE MARX, TEXT 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"LATE MARX, TEXT 2\")\n",
    "visualise_diffs(text2, model_marx, tokenizer_marx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY MARX, TEXT 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"EARLY MARX, TEXT 2\")\n",
    "visualise_diffs(text2, model_earlymarx, tokenizer_earlymarx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
